arch: dgcnn
batch_size: 32
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 1024
pt_norm: False
test_batch_size: 16
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=40, bias=True)
)
Train 0, loss: 3.305671, train acc: 0.248371, 
Test 0, loss: 2.711574, test acc: 0.360616,
Max Acc:0.360616
Train 1, loss: 2.775434, train acc: 0.394137, 
Test 1, loss: 2.390674, test acc: 0.518233,
Max Acc:0.518233
arch: dgcnn
batch_size: 32
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 16
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=40, bias=True)
)
Train 0, loss: 3.322012, train acc: 0.236258, 
Test 0, loss: 2.742167, test acc: 0.375608,
Max Acc:0.375608
Train 1, loss: 2.813791, train acc: 0.380599, 
Test 1, loss: 2.441289, test acc: 0.510940,
Max Acc:0.510940
Train 2, loss: 2.556326, train acc: 0.484731, 
Test 2, loss: 2.180509, test acc: 0.629254,
Max Acc:0.629254
Train 3, loss: 2.421971, train acc: 0.535831, 
Test 3, loss: 2.120869, test acc: 0.647083,
Max Acc:0.647083
Train 4, loss: 2.298403, train acc: 0.585607, 
Test 4, loss: 1.984169, test acc: 0.700162,
Max Acc:0.700162
Train 5, loss: 2.231792, train acc: 0.621539, 
Test 5, loss: 2.011463, test acc: 0.692464,
Train 6, loss: 2.179713, train acc: 0.641083, 
Test 6, loss: 1.932635, test acc: 0.741896,
Max Acc:0.741896
Train 7, loss: 2.137540, train acc: 0.656759, 
Test 7, loss: 1.882964, test acc: 0.749595,
Max Acc:0.749595
Train 8, loss: 2.103623, train acc: 0.677830, 
Test 8, loss: 1.832704, test acc: 0.773906,
Max Acc:0.773906
Train 9, loss: 2.068367, train acc: 0.688823, 
Test 9, loss: 1.812273, test acc: 0.788088,
Max Acc:0.788088
Train 10, loss: 2.048938, train acc: 0.705721, 
Test 10, loss: 1.833934, test acc: 0.768233,
Train 11, loss: 2.032982, train acc: 0.708164, 
Test 11, loss: 1.751136, test acc: 0.816045,
Max Acc:0.816045
Train 12, loss: 2.012216, train acc: 0.716002, 
Test 12, loss: 1.794028, test acc: 0.800648,
Train 13, loss: 1.992563, train acc: 0.723432, 
Test 13, loss: 1.763461, test acc: 0.817666,
Max Acc:0.817666
Train 14, loss: 1.980301, train acc: 0.729743, 
Test 14, loss: 1.771361, test acc: 0.794571,
Train 15, loss: 1.966586, train acc: 0.734731, 
Test 15, loss: 1.787692, test acc: 0.794165,
Train 16, loss: 1.954635, train acc: 0.745623, 
Test 16, loss: 1.733597, test acc: 0.815640,
Train 17, loss: 1.950074, train acc: 0.740737, 
Test 17, loss: 1.694883, test acc: 0.836710,
Max Acc:0.836710
Train 18, loss: 1.942448, train acc: 0.749796, 
Test 18, loss: 1.711054, test acc: 0.821718,
Train 19, loss: 1.920621, train acc: 0.754581, 
Test 19, loss: 1.721262, test acc: 0.826985,
Train 20, loss: 1.921340, train acc: 0.754988, 
Test 20, loss: 1.715490, test acc: 0.823339,
Train 21, loss: 1.910821, train acc: 0.759772, 
Test 21, loss: 1.710149, test acc: 0.835494,
Train 22, loss: 1.899161, train acc: 0.764149, 
Test 22, loss: 1.709288, test acc: 0.831848,
Train 23, loss: 1.896460, train acc: 0.767203, 
Test 23, loss: 1.702208, test acc: 0.833874,
Train 24, loss: 1.888042, train acc: 0.771274, 
Test 24, loss: 1.725506, test acc: 0.826175,
Train 25, loss: 1.887365, train acc: 0.769137, 
Test 25, loss: 1.722870, test acc: 0.831442,
Train 26, loss: 1.886487, train acc: 0.773107, 
Test 26, loss: 1.697438, test acc: 0.846434,
Max Acc:0.846434
Train 27, loss: 1.864070, train acc: 0.779316, 
Test 27, loss: 1.705529, test acc: 0.832658,
Train 28, loss: 1.863852, train acc: 0.784609, 
Test 28, loss: 1.687576, test acc: 0.835900,
Train 29, loss: 1.861866, train acc: 0.783998, 
Test 29, loss: 1.707058, test acc: 0.839546,
Train 30, loss: 1.858614, train acc: 0.786340, 
Test 30, loss: 1.709457, test acc: 0.833468,
Train 31, loss: 1.850340, train acc: 0.785118, 
Test 31, loss: 1.667059, test acc: 0.842788,
Train 32, loss: 1.850529, train acc: 0.785525, 
Test 32, loss: 1.692287, test acc: 0.844003,
Train 33, loss: 1.842832, train acc: 0.787561, 
Test 33, loss: 1.679840, test acc: 0.845219,
Train 34, loss: 1.838034, train acc: 0.793974, 
Test 34, loss: 1.654982, test acc: 0.860211,
Max Acc:0.860211
Train 35, loss: 1.825916, train acc: 0.798453, 
Test 35, loss: 1.689976, test acc: 0.829011,
Train 36, loss: 1.848630, train acc: 0.784304, 
Test 36, loss: 1.674605, test acc: 0.835089,
Train 37, loss: 1.841665, train acc: 0.792752, 
Test 37, loss: 1.667346, test acc: 0.853728,
Train 38, loss: 1.821653, train acc: 0.796824, 
Test 38, loss: 1.663266, test acc: 0.854943,
Train 39, loss: 1.811228, train acc: 0.806698, 
Test 39, loss: 1.675894, test acc: 0.846434,
Train 40, loss: 1.812625, train acc: 0.802932, 
Test 40, loss: 1.657014, test acc: 0.851297,
Train 41, loss: 1.819802, train acc: 0.798860, 
Test 41, loss: 1.685048, test acc: 0.842788,
Train 42, loss: 1.812796, train acc: 0.804866, 
Test 42, loss: 1.642050, test acc: 0.852917,
Train 43, loss: 1.822953, train acc: 0.802015, 
Test 43, loss: 1.699685, test acc: 0.832658,
Train 44, loss: 1.810501, train acc: 0.805069, 
Test 44, loss: 1.675435, test acc: 0.842788,
Train 45, loss: 1.810499, train acc: 0.806291, 
Test 45, loss: 1.647797, test acc: 0.866694,
Max Acc:0.866694
Train 46, loss: 1.802795, train acc: 0.807410, 
Test 46, loss: 1.679802, test acc: 0.844003,
Train 47, loss: 1.812172, train acc: 0.806189, 
Test 47, loss: 1.653896, test acc: 0.847245,
Train 48, loss: 1.807382, train acc: 0.808327, 
Test 48, loss: 1.686085, test acc: 0.838736,
Train 49, loss: 1.799524, train acc: 0.811787, 
Test 49, loss: 1.633246, test acc: 0.865883,
Train 50, loss: 1.787892, train acc: 0.815859, 
Test 50, loss: 1.632108, test acc: 0.869935,
Max Acc:0.869935
Train 51, loss: 1.787043, train acc: 0.813823, 
Test 51, loss: 1.690202, test acc: 0.847245,
Train 52, loss: 1.776707, train acc: 0.822170, 
Test 52, loss: 1.633284, test acc: 0.861426,
Train 53, loss: 1.777339, train acc: 0.820745, 
Test 53, loss: 1.665180, test acc: 0.843193,
Train 54, loss: 1.792334, train acc: 0.815045, 
Test 54, loss: 1.636504, test acc: 0.856969,
Train 55, loss: 1.784650, train acc: 0.813009, 
Test 55, loss: 1.640674, test acc: 0.858995,
Train 56, loss: 1.786100, train acc: 0.815859, 
Test 56, loss: 1.639848, test acc: 0.855754,
Train 57, loss: 1.789945, train acc: 0.812398, 
Test 57, loss: 1.645411, test acc: 0.850081,
Train 58, loss: 1.777343, train acc: 0.823493, 
Test 58, loss: 1.669693, test acc: 0.852512,
Train 59, loss: 1.768910, train acc: 0.823086, 
Test 59, loss: 1.635962, test acc: 0.862642,
Train 60, loss: 1.773084, train acc: 0.824308, 
Test 60, loss: 1.638225, test acc: 0.868314,
Train 61, loss: 1.771058, train acc: 0.828481, 
Test 61, loss: 1.619944, test acc: 0.867099,
Train 62, loss: 1.759101, train acc: 0.827972, 
Test 62, loss: 1.621265, test acc: 0.862237,
Train 63, loss: 1.761940, train acc: 0.823595, 
Test 63, loss: 1.685489, test acc: 0.841572,
Train 64, loss: 1.754519, train acc: 0.827565, 
Test 64, loss: 1.670409, test acc: 0.862642,
Train 65, loss: 1.754773, train acc: 0.831331, 
Test 65, loss: 1.619088, test acc: 0.869935,
Max Acc:0.869935
Train 66, loss: 1.761656, train acc: 0.830314, 
Test 66, loss: 1.629379, test acc: 0.861831,
Train 67, loss: 1.757492, train acc: 0.829601, 
Test 67, loss: 1.651663, test acc: 0.854538,
Train 68, loss: 1.757996, train acc: 0.830008, 
Test 68, loss: 1.678523, test acc: 0.840762,
Train 69, loss: 1.750906, train acc: 0.830110, 
Test 69, loss: 1.649885, test acc: 0.857374,
Train 70, loss: 1.747639, train acc: 0.832349, 
Test 70, loss: 1.634161, test acc: 0.870340,
Max Acc:0.870340
Train 71, loss: 1.748691, train acc: 0.833265, 
Test 71, loss: 1.640496, test acc: 0.857780,
Train 72, loss: 1.734414, train acc: 0.840798, 
Test 72, loss: 1.621604, test acc: 0.868720,
Train 73, loss: 1.734998, train acc: 0.841002, 
Test 73, loss: 1.609454, test acc: 0.871556,
Max Acc:0.871556
Train 74, loss: 1.743160, train acc: 0.837846, 
Test 74, loss: 1.628657, test acc: 0.865478,
Train 75, loss: 1.729596, train acc: 0.840493, 
Test 75, loss: 1.611976, test acc: 0.864263,
Train 76, loss: 1.727991, train acc: 0.842936, 
Test 76, loss: 1.630875, test acc: 0.871151,
Train 77, loss: 1.728614, train acc: 0.843445, 
Test 77, loss: 1.621934, test acc: 0.861426,
Train 78, loss: 1.736160, train acc: 0.841103, 
Test 78, loss: 1.643836, test acc: 0.849676,
Train 79, loss: 1.737955, train acc: 0.837337, 
Test 79, loss: 1.630035, test acc: 0.863857,
Train 80, loss: 1.737196, train acc: 0.841205, 
Test 80, loss: 1.610562, test acc: 0.870340,
Train 81, loss: 1.721981, train acc: 0.841002, 
Test 81, loss: 1.623644, test acc: 0.869935,
Train 82, loss: 1.730031, train acc: 0.842732, 
Test 82, loss: 1.640558, test acc: 0.858185,
Train 83, loss: 1.723208, train acc: 0.845582, 
Test 83, loss: 1.635264, test acc: 0.863857,
Train 84, loss: 1.724241, train acc: 0.842529, 
Test 84, loss: 1.632155, test acc: 0.861021,
Train 85, loss: 1.721716, train acc: 0.845379, 
Test 85, loss: 1.615761, test acc: 0.875608,
Max Acc:0.875608
Train 86, loss: 1.708463, train acc: 0.848738, 
Test 86, loss: 1.591617, test acc: 0.884927,
Max Acc:0.884927
Train 87, loss: 1.720048, train acc: 0.846906, 
Test 87, loss: 1.611842, test acc: 0.876823,
Train 88, loss: 1.712421, train acc: 0.850875, 
Test 88, loss: 1.639397, test acc: 0.872366,
Train 89, loss: 1.713005, train acc: 0.852809, 
Test 89, loss: 1.601146, test acc: 0.876823,
Train 90, loss: 1.709694, train acc: 0.850570, 
Test 90, loss: 1.628025, test acc: 0.859806,
Train 91, loss: 1.703888, train acc: 0.853827, 
Test 91, loss: 1.613303, test acc: 0.871961,
Train 92, loss: 1.711946, train acc: 0.850061, 
Test 92, loss: 1.601009, test acc: 0.877634,
Train 93, loss: 1.701650, train acc: 0.855863, 
Test 93, loss: 1.629520, test acc: 0.869125,
Train 94, loss: 1.723079, train acc: 0.844971, 
Test 94, loss: 1.593428, test acc: 0.882901,
Train 95, loss: 1.694779, train acc: 0.857390, 
Test 95, loss: 1.618643, test acc: 0.869125,
Train 96, loss: 1.706731, train acc: 0.852097, 
Test 96, loss: 1.623819, test acc: 0.871151,
Train 97, loss: 1.698314, train acc: 0.853115, 
Test 97, loss: 1.599812, test acc: 0.882496,
Train 98, loss: 1.699169, train acc: 0.856678, 
Test 98, loss: 1.612905, test acc: 0.866694,
Train 99, loss: 1.697836, train acc: 0.855660, 
Test 99, loss: 1.644683, test acc: 0.852917,
Train 100, loss: 1.687916, train acc: 0.864617, 
Test 100, loss: 1.618945, test acc: 0.873987,
Train 101, loss: 1.683475, train acc: 0.864515, 
Test 101, loss: 1.579862, test acc: 0.884522,
Train 102, loss: 1.689837, train acc: 0.859121, 
Test 102, loss: 1.594082, test acc: 0.887358,
Max Acc:0.887358
Train 103, loss: 1.691843, train acc: 0.858815, 
Test 103, loss: 1.599273, test acc: 0.871556,
Train 104, loss: 1.680625, train acc: 0.866653, 
Test 104, loss: 1.600099, test acc: 0.870340,
Train 105, loss: 1.691813, train acc: 0.859629, 
Test 105, loss: 1.584647, test acc: 0.876418,
Train 106, loss: 1.681765, train acc: 0.865432, 
Test 106, loss: 1.624637, test acc: 0.867099,
Train 107, loss: 1.678439, train acc: 0.866755, 
Test 107, loss: 1.598045, test acc: 0.875203,
Train 108, loss: 1.678480, train acc: 0.865839, 
Test 108, loss: 1.619882, test acc: 0.871556,
Train 109, loss: 1.672307, train acc: 0.868180, 
Test 109, loss: 1.610925, test acc: 0.861831,
Train 110, loss: 1.672279, train acc: 0.868282, 
Test 110, loss: 1.571752, test acc: 0.886143,
Train 111, loss: 1.681483, train acc: 0.863498, 
Test 111, loss: 1.602233, test acc: 0.878039,
Train 112, loss: 1.668699, train acc: 0.867569, 
Test 112, loss: 1.580296, test acc: 0.881686,
Train 113, loss: 1.671429, train acc: 0.868893, 
Test 113, loss: 1.606550, test acc: 0.873582,
Train 114, loss: 1.680082, train acc: 0.860953, 
Test 114, loss: 1.613112, test acc: 0.876823,
Train 115, loss: 1.666371, train acc: 0.870114, 
Test 115, loss: 1.600670, test acc: 0.869935,
Train 116, loss: 1.659817, train acc: 0.872862, 
Test 116, loss: 1.590997, test acc: 0.875608,
Train 117, loss: 1.665994, train acc: 0.870114, 
Test 117, loss: 1.596336, test acc: 0.874797,
Train 118, loss: 1.665359, train acc: 0.870623, 
Test 118, loss: 1.601061, test acc: 0.871151,
Train 119, loss: 1.657258, train acc: 0.874898, 
Test 119, loss: 1.619899, test acc: 0.873582,
Train 120, loss: 1.666878, train acc: 0.872455, 
Test 120, loss: 1.580376, test acc: 0.879660,
Train 121, loss: 1.659066, train acc: 0.872353, 
Test 121, loss: 1.580085, test acc: 0.887358,
Max Acc:0.887358
Train 122, loss: 1.654384, train acc: 0.876527, 
Test 122, loss: 1.594881, test acc: 0.877634,
Train 123, loss: 1.656022, train acc: 0.874796, 
Test 123, loss: 1.588882, test acc: 0.878444,
Train 124, loss: 1.657073, train acc: 0.875814, 
Test 124, loss: 1.592452, test acc: 0.885332,
Train 125, loss: 1.653039, train acc: 0.875204, 
Test 125, loss: 1.570273, test acc: 0.882091,
Train 126, loss: 1.646370, train acc: 0.876120, 
Test 126, loss: 1.593363, test acc: 0.875608,
Train 127, loss: 1.642654, train acc: 0.878563, 
Test 127, loss: 1.591097, test acc: 0.872771,
Train 128, loss: 1.656746, train acc: 0.872353, 
Test 128, loss: 1.589019, test acc: 0.880470,
Train 129, loss: 1.644577, train acc: 0.878563, 
Test 129, loss: 1.598769, test acc: 0.874392,
Train 130, loss: 1.639836, train acc: 0.883042, 
Test 130, loss: 1.616782, test acc: 0.868314,
Train 131, loss: 1.645216, train acc: 0.881107, 
Test 131, loss: 1.576635, test acc: 0.881686,
Train 132, loss: 1.648515, train acc: 0.880599, 
Test 132, loss: 1.606600, test acc: 0.858590,
Train 133, loss: 1.644029, train acc: 0.879173, 
Test 133, loss: 1.596229, test acc: 0.867504,
Train 134, loss: 1.647812, train acc: 0.876018, 
Test 134, loss: 1.596044, test acc: 0.877229,
Train 135, loss: 1.636164, train acc: 0.882838, 
Test 135, loss: 1.592275, test acc: 0.876418,
Train 136, loss: 1.642943, train acc: 0.882431, 
Test 136, loss: 1.604533, test acc: 0.882901,
Train 137, loss: 1.639223, train acc: 0.885485, 
Test 137, loss: 1.611649, test acc: 0.856159,
Train 138, loss: 1.639425, train acc: 0.885485, 
Test 138, loss: 1.594198, test acc: 0.869935,
Train 139, loss: 1.627616, train acc: 0.887113, 
Test 139, loss: 1.582128, test acc: 0.884117,
Train 140, loss: 1.623387, train acc: 0.886910, 
Test 140, loss: 1.594034, test acc: 0.876418,
Train 141, loss: 1.643786, train acc: 0.881515, 
Test 141, loss: 1.584118, test acc: 0.881686,
Train 142, loss: 1.622337, train acc: 0.892101, 
Test 142, loss: 1.595883, test acc: 0.876418,
Train 143, loss: 1.620695, train acc: 0.892305, 
Test 143, loss: 1.585188, test acc: 0.867099,
Train 144, loss: 1.629242, train acc: 0.886095, 
Test 144, loss: 1.587150, test acc: 0.878039,
Train 145, loss: 1.621058, train acc: 0.889251, 
Test 145, loss: 1.577123, test acc: 0.887358,
Max Acc:0.887358
Train 146, loss: 1.616641, train acc: 0.893628, 
Test 146, loss: 1.598158, test acc: 0.876418,
Train 147, loss: 1.611635, train acc: 0.899226, 
Test 147, loss: 1.579137, test acc: 0.894652,
Max Acc:0.894652
Train 148, loss: 1.620010, train acc: 0.894849, 
Test 148, loss: 1.570235, test acc: 0.883306,
Train 149, loss: 1.615343, train acc: 0.891185, 
Test 149, loss: 1.564948, test acc: 0.884117,
Train 150, loss: 1.609100, train acc: 0.895358, 
Test 150, loss: 1.583778, test acc: 0.880875,
Train 151, loss: 1.613059, train acc: 0.892406, 
Test 151, loss: 1.584205, test acc: 0.874392,
Train 152, loss: 1.618510, train acc: 0.891490, 
Test 152, loss: 1.557873, test acc: 0.885737,
Train 153, loss: 1.611557, train acc: 0.891796, 
Test 153, loss: 1.567743, test acc: 0.893436,
Train 154, loss: 1.609258, train acc: 0.893831, 
Test 154, loss: 1.589303, test acc: 0.873582,
Train 155, loss: 1.613331, train acc: 0.891185, 
Test 155, loss: 1.610403, test acc: 0.863452,
Train 156, loss: 1.600261, train acc: 0.902382, 
Test 156, loss: 1.583515, test acc: 0.882901,
Train 157, loss: 1.600581, train acc: 0.899532, 
Test 157, loss: 1.561803, test acc: 0.890600,
Train 158, loss: 1.600328, train acc: 0.899023, 
Test 158, loss: 1.587780, test acc: 0.877634,
Train 159, loss: 1.599868, train acc: 0.900957, 
Test 159, loss: 1.575347, test acc: 0.883306,
Train 160, loss: 1.596745, train acc: 0.899226, 
Test 160, loss: 1.569295, test acc: 0.891005,
Train 161, loss: 1.597651, train acc: 0.902484, 
Test 161, loss: 1.564588, test acc: 0.894652,
Max Acc:0.894652
Train 162, loss: 1.592798, train acc: 0.901873, 
Test 162, loss: 1.590959, test acc: 0.870340,
Train 163, loss: 1.589216, train acc: 0.907471, 
Test 163, loss: 1.565506, test acc: 0.880065,
Train 164, loss: 1.586018, train acc: 0.907573, 
Test 164, loss: 1.582256, test acc: 0.874797,
Train 165, loss: 1.601189, train acc: 0.898819, 
Test 165, loss: 1.562127, test acc: 0.889384,
Train 166, loss: 1.586415, train acc: 0.907166, 
Test 166, loss: 1.592235, test acc: 0.875608,
Train 167, loss: 1.589483, train acc: 0.903196, 
Test 167, loss: 1.571298, test acc: 0.886953,
Train 168, loss: 1.585929, train acc: 0.907064, 
Test 168, loss: 1.567569, test acc: 0.884117,
Train 169, loss: 1.579084, train acc: 0.907980, 
Test 169, loss: 1.638511, test acc: 0.858995,
Train 170, loss: 1.572158, train acc: 0.910118, 
Test 170, loss: 1.608685, test acc: 0.857780,
Train 171, loss: 1.578934, train acc: 0.911238, 
Test 171, loss: 1.562222, test acc: 0.888574,
Train 172, loss: 1.580296, train acc: 0.909304, 
Test 172, loss: 1.584803, test acc: 0.886953,
Train 173, loss: 1.576985, train acc: 0.909914, 
Test 173, loss: 1.561513, test acc: 0.889384,
Train 174, loss: 1.567119, train acc: 0.916124, 
Test 174, loss: 1.593609, test acc: 0.874797,
Train 175, loss: 1.580496, train acc: 0.908388, 
Test 175, loss: 1.571049, test acc: 0.880065,
Train 176, loss: 1.562595, train acc: 0.916633, 
Test 176, loss: 1.569391, test acc: 0.882901,
Train 177, loss: 1.577158, train acc: 0.908388, 
Test 177, loss: 1.560728, test acc: 0.893841,
Train 178, loss: 1.571439, train acc: 0.910729, 
Test 178, loss: 1.574168, test acc: 0.883306,
Train 179, loss: 1.565993, train acc: 0.917447, 
Test 179, loss: 1.581098, test acc: 0.885332,
Train 180, loss: 1.569417, train acc: 0.911849, 
Test 180, loss: 1.591243, test acc: 0.872366,
Train 181, loss: 1.563415, train acc: 0.914190, 
Test 181, loss: 1.574393, test acc: 0.889789,
Train 182, loss: 1.562904, train acc: 0.916938, 
Test 182, loss: 1.570015, test acc: 0.882496,
Train 183, loss: 1.550147, train acc: 0.923351, 
Test 183, loss: 1.570851, test acc: 0.882901,
Train 184, loss: 1.558271, train acc: 0.921010, 
Test 184, loss: 1.565264, test acc: 0.885737,
Train 185, loss: 1.556189, train acc: 0.923962, 
Test 185, loss: 1.577945, test acc: 0.884927,
Train 186, loss: 1.549765, train acc: 0.925183, 
Test 186, loss: 1.582341, test acc: 0.879254,
Train 187, loss: 1.540456, train acc: 0.929153, 
Test 187, loss: 1.569755, test acc: 0.888574,
Train 188, loss: 1.544673, train acc: 0.927219, 
Test 188, loss: 1.559262, test acc: 0.889789,
Train 189, loss: 1.552451, train acc: 0.920501, 
Test 189, loss: 1.575614, test acc: 0.883712,
Train 190, loss: 1.546099, train acc: 0.923555, 
Test 190, loss: 1.546319, test acc: 0.884927,
Train 191, loss: 1.541419, train acc: 0.925794, 
Test 191, loss: 1.573778, test acc: 0.889384,
Train 192, loss: 1.538647, train acc: 0.928033, 
Test 192, loss: 1.580759, test acc: 0.878444,
Train 193, loss: 1.549035, train acc: 0.924674, 
Test 193, loss: 1.572855, test acc: 0.882901,
Train 194, loss: 1.533363, train acc: 0.928848, 
Test 194, loss: 1.577344, test acc: 0.878849,
Train 195, loss: 1.540091, train acc: 0.927321, 
Test 195, loss: 1.554588, test acc: 0.886953,
Train 196, loss: 1.537501, train acc: 0.926303, 
Test 196, loss: 1.602061, test acc: 0.874392,
Train 197, loss: 1.537018, train acc: 0.928135, 
Test 197, loss: 1.552116, test acc: 0.894652,
Max Acc:0.894652
Train 198, loss: 1.533964, train acc: 0.927932, 
Test 198, loss: 1.556215, test acc: 0.888574,
Train 199, loss: 1.527843, train acc: 0.933225, 
Test 199, loss: 1.577113, test acc: 0.878849,
Train 200, loss: 1.522195, train acc: 0.937093, 
Test 200, loss: 1.563426, test acc: 0.880065,
Train 201, loss: 1.524361, train acc: 0.933734, 
Test 201, loss: 1.560917, test acc: 0.885332,
Train 202, loss: 1.516921, train acc: 0.937704, 
Test 202, loss: 1.565070, test acc: 0.890600,
Train 203, loss: 1.524807, train acc: 0.933327, 
Test 203, loss: 1.556963, test acc: 0.891410,
Train 204, loss: 1.519574, train acc: 0.937500, 
Test 204, loss: 1.557619, test acc: 0.895057,
Max Acc:0.895057
Train 205, loss: 1.515902, train acc: 0.938111, 
Test 205, loss: 1.559536, test acc: 0.882901,
Train 206, loss: 1.518066, train acc: 0.933428, 
Test 206, loss: 1.548280, test acc: 0.892626,
Train 207, loss: 1.515932, train acc: 0.938721, 
Test 207, loss: 1.575555, test acc: 0.876013,
Train 208, loss: 1.518573, train acc: 0.937500, 
Test 208, loss: 1.557179, test acc: 0.886143,
Train 209, loss: 1.510813, train acc: 0.941979, 
Test 209, loss: 1.577113, test acc: 0.880065,
Train 210, loss: 1.512588, train acc: 0.941164, 
Test 210, loss: 1.579530, test acc: 0.877634,
Train 211, loss: 1.507471, train acc: 0.942182, 
Test 211, loss: 1.546973, test acc: 0.885737,
Train 212, loss: 1.509067, train acc: 0.943099, 
Test 212, loss: 1.568439, test acc: 0.878039,
Train 213, loss: 1.505076, train acc: 0.945338, 
Test 213, loss: 1.551190, test acc: 0.886953,
Train 214, loss: 1.497229, train acc: 0.944829, 
Test 214, loss: 1.581114, test acc: 0.880065,
Train 215, loss: 1.503381, train acc: 0.943200, 
Test 215, loss: 1.554276, test acc: 0.882901,
Train 216, loss: 1.499437, train acc: 0.945949, 
Test 216, loss: 1.546077, test acc: 0.893436,
Train 217, loss: 1.497719, train acc: 0.944829, 
Test 217, loss: 1.535648, test acc: 0.893031,
Train 218, loss: 1.499281, train acc: 0.945033, 
Test 218, loss: 1.551294, test acc: 0.891410,
Train 219, loss: 1.496721, train acc: 0.948290, 
Test 219, loss: 1.550562, test acc: 0.892626,
Train 220, loss: 1.491319, train acc: 0.950020, 
Test 220, loss: 1.552744, test acc: 0.891005,
Train 221, loss: 1.490557, train acc: 0.948901, 
Test 221, loss: 1.561187, test acc: 0.889789,
Train 222, loss: 1.479844, train acc: 0.953583, 
Test 222, loss: 1.559015, test acc: 0.890194,
Train 223, loss: 1.489826, train acc: 0.949410, 
Test 223, loss: 1.580575, test acc: 0.880875,
Train 224, loss: 1.476280, train acc: 0.954499, 
Test 224, loss: 1.570104, test acc: 0.886953,
Train 225, loss: 1.480974, train acc: 0.954601, 
Test 225, loss: 1.554097, test acc: 0.890194,
Train 226, loss: 1.488305, train acc: 0.948595, 
Test 226, loss: 1.563439, test acc: 0.886143,
Train 227, loss: 1.482685, train acc: 0.953278, 
Test 227, loss: 1.567311, test acc: 0.879660,
Train 228, loss: 1.476990, train acc: 0.954397, 
Test 228, loss: 1.560981, test acc: 0.888169,
Train 229, loss: 1.474341, train acc: 0.958774, 
Test 229, loss: 1.543307, test acc: 0.890600,
Train 230, loss: 1.476060, train acc: 0.954906, 
Test 230, loss: 1.568254, test acc: 0.880470,
Train 231, loss: 1.472893, train acc: 0.958673, 
Test 231, loss: 1.552135, test acc: 0.887358,
Train 232, loss: 1.471280, train acc: 0.955822, 
Test 232, loss: 1.555434, test acc: 0.878849,
Train 233, loss: 1.465341, train acc: 0.959080, 
Test 233, loss: 1.547083, test acc: 0.891815,
Train 234, loss: 1.461351, train acc: 0.961014, 
Test 234, loss: 1.569997, test acc: 0.888979,
Train 235, loss: 1.463346, train acc: 0.961014, 
Test 235, loss: 1.551894, test acc: 0.889384,
Train 236, loss: 1.462755, train acc: 0.960912, 
Test 236, loss: 1.566656, test acc: 0.885332,
Train 237, loss: 1.465808, train acc: 0.958571, 
Test 237, loss: 1.555768, test acc: 0.882496,
Train 238, loss: 1.455690, train acc: 0.963559, 
Test 238, loss: 1.553461, test acc: 0.888979,
Train 239, loss: 1.463481, train acc: 0.961014, 
Test 239, loss: 1.545881, test acc: 0.894652,
Train 240, loss: 1.459037, train acc: 0.962134, 
Test 240, loss: 1.544838, test acc: 0.891410,
Train 241, loss: 1.455618, train acc: 0.965493, 
Test 241, loss: 1.542753, test acc: 0.891815,
Train 242, loss: 1.453249, train acc: 0.963660, 
Test 242, loss: 1.568068, test acc: 0.885737,
Train 243, loss: 1.452913, train acc: 0.966816, 
Test 243, loss: 1.561392, test acc: 0.883306,
Train 244, loss: 1.445597, train acc: 0.966511, 
Test 244, loss: 1.551306, test acc: 0.888574,
Train 245, loss: 1.450281, train acc: 0.966002, 
Test 245, loss: 1.574922, test acc: 0.883306,
Train 246, loss: 1.437543, train acc: 0.971091, 
Test 246, loss: 1.550338, test acc: 0.886143,
Train 247, loss: 1.447810, train acc: 0.964169, 
Test 247, loss: 1.548891, test acc: 0.890194,
Train 248, loss: 1.442576, train acc: 0.969971, 
Test 248, loss: 1.551480, test acc: 0.885737,
Train 249, loss: 1.440810, train acc: 0.970989, 
Test 249, loss: 1.557625, test acc: 0.890194,
Train 250, loss: 1.437059, train acc: 0.971498, 
Test 250, loss: 1.555133, test acc: 0.888169,
Train 251, loss: 1.445053, train acc: 0.967529, 
Test 251, loss: 1.561072, test acc: 0.883712,
Train 252, loss: 1.435616, train acc: 0.972109, 
Test 252, loss: 1.557889, test acc: 0.888169,
Train 253, loss: 1.434244, train acc: 0.971397, 
Test 253, loss: 1.562906, test acc: 0.885332,
Train 254, loss: 1.428690, train acc: 0.974349, 
Test 254, loss: 1.569433, test acc: 0.880065,
Train 255, loss: 1.434850, train acc: 0.972516, 
Test 255, loss: 1.547266, test acc: 0.892220,
Train 256, loss: 1.432035, train acc: 0.973331, 
Test 256, loss: 1.539413, test acc: 0.892626,
Train 257, loss: 1.432328, train acc: 0.972923, 
Test 257, loss: 1.546661, test acc: 0.894246,
Train 258, loss: 1.427557, train acc: 0.974043, 
Test 258, loss: 1.547276, test acc: 0.887763,
Train 259, loss: 1.426681, train acc: 0.974857, 
Test 259, loss: 1.548097, test acc: 0.894652,
Train 260, loss: 1.423539, train acc: 0.977606, 
Test 260, loss: 1.549112, test acc: 0.891410,
Train 261, loss: 1.426425, train acc: 0.974959, 
Test 261, loss: 1.548112, test acc: 0.889789,
Train 262, loss: 1.421233, train acc: 0.975977, 
Test 262, loss: 1.539148, test acc: 0.892220,
Train 263, loss: 1.415811, train acc: 0.977708, 
Test 263, loss: 1.547323, test acc: 0.890600,
Train 264, loss: 1.417471, train acc: 0.978929, 
Test 264, loss: 1.555346, test acc: 0.886143,
Train 265, loss: 1.418308, train acc: 0.979947, 
Test 265, loss: 1.546683, test acc: 0.888169,
Train 266, loss: 1.421800, train acc: 0.974959, 
Test 266, loss: 1.536445, test acc: 0.893031,
Train 267, loss: 1.411103, train acc: 0.981779, 
Test 267, loss: 1.550961, test acc: 0.890194,
Train 268, loss: 1.410805, train acc: 0.983204, 
Test 268, loss: 1.535430, test acc: 0.893031,
Train 269, loss: 1.413568, train acc: 0.981678, 
Test 269, loss: 1.537368, test acc: 0.893031,
Train 270, loss: 1.411616, train acc: 0.980049, 
Test 270, loss: 1.526110, test acc: 0.896272,
Max Acc:0.896272
Train 271, loss: 1.408426, train acc: 0.980761, 
Test 271, loss: 1.545659, test acc: 0.892626,
Train 272, loss: 1.407483, train acc: 0.982797, 
Test 272, loss: 1.544355, test acc: 0.894652,
Train 273, loss: 1.409402, train acc: 0.980558, 
Test 273, loss: 1.558093, test acc: 0.885737,
Train 274, loss: 1.407476, train acc: 0.982899, 
Test 274, loss: 1.550336, test acc: 0.888169,
Train 275, loss: 1.405025, train acc: 0.983306, 
Test 275, loss: 1.545860, test acc: 0.890600,
Train 276, loss: 1.404974, train acc: 0.983306, 
Test 276, loss: 1.542010, test acc: 0.893436,
Train 277, loss: 1.403784, train acc: 0.982797, 
Test 277, loss: 1.535137, test acc: 0.892626,
Train 278, loss: 1.402087, train acc: 0.985546, 
Test 278, loss: 1.538829, test acc: 0.891005,
Train 279, loss: 1.405374, train acc: 0.984222, 
Test 279, loss: 1.543717, test acc: 0.893031,
Train 280, loss: 1.400174, train acc: 0.986564, 
Test 280, loss: 1.546879, test acc: 0.890194,
Train 281, loss: 1.396859, train acc: 0.986665, 
Test 281, loss: 1.540588, test acc: 0.894246,
Train 282, loss: 1.393684, train acc: 0.988192, 
Test 282, loss: 1.538298, test acc: 0.896272,
Max Acc:0.896272
Train 283, loss: 1.391753, train acc: 0.988599, 
Test 283, loss: 1.543352, test acc: 0.894652,
Train 284, loss: 1.401612, train acc: 0.984833, 
Test 284, loss: 1.560367, test acc: 0.886953,
Train 285, loss: 1.395772, train acc: 0.985342, 
Test 285, loss: 1.554718, test acc: 0.888574,
Train 286, loss: 1.395924, train acc: 0.986360, 
Test 286, loss: 1.536485, test acc: 0.892626,
Train 287, loss: 1.394603, train acc: 0.985647, 
Test 287, loss: 1.537333, test acc: 0.897083,
Max Acc:0.897083
Train 288, loss: 1.389903, train acc: 0.988599, 
Test 288, loss: 1.546374, test acc: 0.888979,
Train 289, loss: 1.390518, train acc: 0.988599, 
Test 289, loss: 1.545066, test acc: 0.889789,
Train 290, loss: 1.389238, train acc: 0.987785, 
Test 290, loss: 1.530117, test acc: 0.895867,
Train 291, loss: 1.389009, train acc: 0.987480, 
Test 291, loss: 1.542239, test acc: 0.888979,
Train 292, loss: 1.385707, train acc: 0.990432, 
Test 292, loss: 1.544225, test acc: 0.891410,
Train 293, loss: 1.387572, train acc: 0.988803, 
Test 293, loss: 1.537553, test acc: 0.894652,
Train 294, loss: 1.385797, train acc: 0.990330, 
Test 294, loss: 1.534754, test acc: 0.892220,
Train 295, loss: 1.383584, train acc: 0.991042, 
Test 295, loss: 1.541629, test acc: 0.896272,
Train 296, loss: 1.386290, train acc: 0.988905, 
Test 296, loss: 1.529406, test acc: 0.896272,
Train 297, loss: 1.383908, train acc: 0.990330, 
Test 297, loss: 1.530829, test acc: 0.897488,
Max Acc:0.897488
Train 298, loss: 1.380557, train acc: 0.991144, 
Test 298, loss: 1.538482, test acc: 0.896272,
Train 299, loss: 1.376705, train acc: 0.991958, 
Test 299, loss: 1.538601, test acc: 0.896272,
Train 300, loss: 1.379608, train acc: 0.991755, 
Test 300, loss: 1.533629, test acc: 0.897083,
Train 301, loss: 1.381104, train acc: 0.992060, 
Test 301, loss: 1.532051, test acc: 0.897083,
Train 302, loss: 1.379303, train acc: 0.991551, 
Test 302, loss: 1.534301, test acc: 0.897488,
Max Acc:0.897488
Train 303, loss: 1.377769, train acc: 0.991551, 
Test 303, loss: 1.534436, test acc: 0.897893,
Max Acc:0.897893
Train 304, loss: 1.380271, train acc: 0.991246, 
Test 304, loss: 1.539295, test acc: 0.894246,
Train 305, loss: 1.378627, train acc: 0.991857, 
Test 305, loss: 1.536586, test acc: 0.892220,
Train 306, loss: 1.377393, train acc: 0.991144, 
Test 306, loss: 1.537774, test acc: 0.895867,
Train 307, loss: 1.377497, train acc: 0.989414, 
Test 307, loss: 1.533867, test acc: 0.898703,
Max Acc:0.898703
Train 308, loss: 1.374349, train acc: 0.992976, 
Test 308, loss: 1.536912, test acc: 0.893841,
Train 309, loss: 1.374806, train acc: 0.991653, 
Test 309, loss: 1.525640, test acc: 0.895867,
Train 310, loss: 1.372911, train acc: 0.993282, 
Test 310, loss: 1.534848, test acc: 0.897893,
Train 311, loss: 1.374960, train acc: 0.991551, 
Test 311, loss: 1.539079, test acc: 0.895057,
Train 312, loss: 1.371868, train acc: 0.992875, 
Test 312, loss: 1.536370, test acc: 0.893841,
Train 313, loss: 1.373733, train acc: 0.992773, 
Test 313, loss: 1.539153, test acc: 0.895057,
Train 314, loss: 1.373425, train acc: 0.993180, 
Test 314, loss: 1.525940, test acc: 0.902755,
Max Acc:0.902755
Train 315, loss: 1.369278, train acc: 0.993689, 
Test 315, loss: 1.532296, test acc: 0.896272,
Train 316, loss: 1.372575, train acc: 0.993078, 
Test 316, loss: 1.530563, test acc: 0.899919,
Train 317, loss: 1.375510, train acc: 0.992264, 
Test 317, loss: 1.532235, test acc: 0.895867,
Train 318, loss: 1.374560, train acc: 0.991551, 
Test 318, loss: 1.528931, test acc: 0.897488,
Train 319, loss: 1.370532, train acc: 0.993078, 
Test 319, loss: 1.530587, test acc: 0.897083,
Train 320, loss: 1.372953, train acc: 0.993282, 
Test 320, loss: 1.528305, test acc: 0.898703,
Train 321, loss: 1.368487, train acc: 0.993485, 
Test 321, loss: 1.544203, test acc: 0.892220,
Train 322, loss: 1.368697, train acc: 0.993893, 
Test 322, loss: 1.524042, test acc: 0.896272,
Train 323, loss: 1.369524, train acc: 0.993994, 
Test 323, loss: 1.529606, test acc: 0.897083,
Train 324, loss: 1.365805, train acc: 0.993791, 
Test 324, loss: 1.536355, test acc: 0.900729,
Train 325, loss: 1.367213, train acc: 0.993893, 
Test 325, loss: 1.521859, test acc: 0.900324,
Train 326, loss: 1.367810, train acc: 0.994503, 
Test 326, loss: 1.527112, test acc: 0.899919,
Train 327, loss: 1.370369, train acc: 0.993282, 
Test 327, loss: 1.528376, test acc: 0.900324,
Train 328, loss: 1.367901, train acc: 0.994605, 
Test 328, loss: 1.527207, test acc: 0.900324,
Train 329, loss: 1.368273, train acc: 0.993485, 
Test 329, loss: 1.530251, test acc: 0.899109,
Train 330, loss: 1.364594, train acc: 0.994809, 
Test 330, loss: 1.528286, test acc: 0.896677,
Train 331, loss: 1.368774, train acc: 0.993893, 
Test 331, loss: 1.523246, test acc: 0.897488,
Train 332, loss: 1.366094, train acc: 0.994198, 
Test 332, loss: 1.528881, test acc: 0.898298,
Train 333, loss: 1.368389, train acc: 0.992162, 
Test 333, loss: 1.521979, test acc: 0.899109,
Train 334, loss: 1.368028, train acc: 0.994503, 
Test 334, loss: 1.530419, test acc: 0.900729,
Train 335, loss: 1.369043, train acc: 0.993485, 
Test 335, loss: 1.525375, test acc: 0.896677,
Train 336, loss: 1.366523, train acc: 0.994096, 
Test 336, loss: 1.530958, test acc: 0.898703,
Train 337, loss: 1.363578, train acc: 0.994809, 
Test 337, loss: 1.523430, test acc: 0.899919,
Train 338, loss: 1.366750, train acc: 0.994300, 
Test 338, loss: 1.529579, test acc: 0.899514,
Train 339, loss: 1.366113, train acc: 0.993485, 
Test 339, loss: 1.523902, test acc: 0.898298,
Train 340, loss: 1.366803, train acc: 0.994096, 
Test 340, loss: 1.526525, test acc: 0.896677,
Train 341, loss: 1.367143, train acc: 0.994300, 
Test 341, loss: 1.535646, test acc: 0.896677,
Train 342, loss: 1.365947, train acc: 0.993791, 
Test 342, loss: 1.526916, test acc: 0.899919,
Train 343, loss: 1.368049, train acc: 0.993384, 
Test 343, loss: 1.523034, test acc: 0.899109,
Train 344, loss: 1.368256, train acc: 0.994401, 
Test 344, loss: 1.527891, test acc: 0.898298,
Train 345, loss: 1.363192, train acc: 0.994198, 
Test 345, loss: 1.532642, test acc: 0.898703,
Train 346, loss: 1.365215, train acc: 0.994809, 
Test 346, loss: 1.531539, test acc: 0.898298,
Train 347, loss: 1.366942, train acc: 0.994300, 
Test 347, loss: 1.526634, test acc: 0.902350,
Train 348, loss: 1.367549, train acc: 0.993078, 
Test 348, loss: 1.530435, test acc: 0.895867,
Train 349, loss: 1.366889, train acc: 0.994503, 
Test 349, loss: 1.530410, test acc: 0.898703,
arch: dgcnn
batch_size: 5
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 5
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=40, bias=True)
)
arch: dgcnn
batch_size: 5
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 5
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=40, bias=True)
)
arch: dgcnn
batch_size: 5
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 5
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=40, bias=True)
)
arch: dgcnn
batch_size: 5
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 5
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=40, bias=True)
)
arch: dgcnn
batch_size: 6
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 5
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 6
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=40, bias=True)
)
Train 0, loss: 3.614135, train acc: 0.081402, 
Test 0, loss: 3.663307, test acc: 0.040519,
Max Acc:0.040519
Train 1, loss: 3.559872, train acc: 0.077439, 
Test 1, loss: 3.673547, test acc: 0.040519,
Max Acc:0.040519
Train 2, loss: 3.560209, train acc: 0.080488, 
Test 2, loss: 3.678670, test acc: 0.040519,
Max Acc:0.040519
Train 3, loss: 3.561739, train acc: 0.077642, 
Test 3, loss: 3.658149, test acc: 0.040519,
Max Acc:0.040519
Train 4, loss: 3.561729, train acc: 0.076016, 
Test 4, loss: 3.650312, test acc: 0.040519,
Max Acc:0.040519
Train 5, loss: 3.560280, train acc: 0.079167, 
Test 5, loss: 3.675412, test acc: 0.040519,
Max Acc:0.040519
arch: dgcnn
batch_size: 6
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 6
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=40, bias=True)
)
Train 0, loss: 3.611981, train acc: 0.081707, 
Test 0, loss: 3.663423, test acc: 0.040519,
Max Acc:0.040519
Train 1, loss: 3.559872, train acc: 0.077439, 
Test 1, loss: 3.674283, test acc: 0.040113,
Train 2, loss: 3.560209, train acc: 0.080488, 
Test 2, loss: 3.678639, test acc: 0.040519,
Max Acc:0.040519
Train 3, loss: 3.561739, train acc: 0.077642, 
Test 3, loss: 3.658328, test acc: 0.040519,
Max Acc:0.040519
arch: dgcnn
batch_size: 9
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 9
workers: 6
Using GPU
arch: dgcnn
batch_size: 6
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 6
workers: 6
Using GPU
arch: dgcnn
batch_size: 6
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 6
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=4, bias=True)
)
arch: dgcnn
batch_size: 6
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 6
workers: 6
Using GPU
arch: dgcnn
batch_size: 6
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 6
workers: 6
Using GPU
arch: dgcnn
batch_size: 6
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 6
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=4, bias=True)
)
arch: dgcnn
batch_size: 6
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 6
workers: 6
Using GPU
arch: dgcnn
batch_size: 6
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 6
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=4, bias=True)
)
arch: dgcnn
batch_size: 6
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 6
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=4, bias=True)
)
arch: dgcnn
batch_size: 6
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 6
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=4, bias=True)
)
arch: dgcnn
batch_size: 6
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 6
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=4, bias=True)
)
arch: dgcnn
batch_size: 6
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 6
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=4, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=4, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=4, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=4, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=4, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=4, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=4, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=4, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=4, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn2): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn3): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn4): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (bn5): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=40, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=40, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 1.573732, train acc: 0.918474, 
Test 0, loss: 0.778406, test acc: 0.933433,
Max Acc:0.933433
Train 1, loss: 0.494400, train acc: 0.933433, 
Test 1, loss: 0.792637, test acc: 0.933433,
Max Acc:0.933433
Train 2, loss: 0.494400, train acc: 0.933433, 
Test 2, loss: 1.147658, test acc: 0.933433,
Max Acc:0.933433
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 1.210933, train acc: 0.918474, 
Test 0, loss: 0.604846, test acc: 0.869102,
Max Acc:0.869102
Train 1, loss: 0.491831, train acc: 0.933433, 
Test 1, loss: 0.494581, test acc: 0.925419,
Max Acc:0.925419
Train 2, loss: 0.491719, train acc: 0.933433, 
Test 2, loss: 0.509259, test acc: 0.893455,
Train 3, loss: 0.491816, train acc: 0.933433, 
Test 3, loss: 0.493720, test acc: 0.925419,
Max Acc:0.925419
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 1
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 4
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 1.445757, train acc: 0.915673, 
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 4
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 1.331561, train acc: 0.918228, 
Test 0, loss: 0.494047, test acc: 0.923913,
Max Acc:0.923913
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 4
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.421586, train acc: 0.000000, 
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 4
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 1.332031, train acc: 0.918654, 
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 4
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 1.457401, train acc: 0.919932, 
Test 0, loss: 0.582068, test acc: 0.923913,
Max Acc:0.923913
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 4
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 4
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 4
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 1.073130, train acc: 0.643364, 
Test 0, loss: 0.678381, test acc: 0.683935,
Max Acc:0.683935
Train 1, loss: 0.685316, train acc: 0.654534, 
Test 1, loss: 0.683963, test acc: 0.683935,
Max Acc:0.683935
Train 2, loss: 0.686993, train acc: 0.651511, 
Test 2, loss: 0.671515, test acc: 0.683935,
Max Acc:0.683935
Train 3, loss: 0.685973, train acc: 0.646124, 
Test 3, loss: 0.670263, test acc: 0.683935,
Max Acc:0.683935
Train 4, loss: 0.684965, train acc: 0.645466, 
Test 4, loss: 0.716987, test acc: 0.683935,
Max Acc:0.683935
Train 5, loss: 0.684492, train acc: 0.654271, 
Test 5, loss: 0.718585, test acc: 0.316065,
Train 6, loss: 0.686563, train acc: 0.645729, 
Test 6, loss: 0.719030, test acc: 0.683935,
Max Acc:0.683935
Train 7, loss: 0.686075, train acc: 0.652300, 
Test 7, loss: 0.672479, test acc: 0.683935,
Max Acc:0.683935
Train 8, loss: 0.685522, train acc: 0.647700, 
Test 8, loss: 0.668749, test acc: 0.683935,
Max Acc:0.683935
Train 9, loss: 0.685348, train acc: 0.647569, 
Test 9, loss: 0.702633, test acc: 0.683935,
Max Acc:0.683935
Train 10, loss: 0.685531, train acc: 0.644547, 
Test 10, loss: 0.669065, test acc: 0.683935,
Max Acc:0.683935
Train 11, loss: 0.686761, train acc: 0.649277, 
Test 11, loss: 0.755947, test acc: 0.316065,
Train 12, loss: 0.685126, train acc: 0.648489, 
Test 12, loss: 0.692410, test acc: 0.683935,
Max Acc:0.683935
Train 13, loss: 0.686056, train acc: 0.654534, 
Test 13, loss: 0.676952, test acc: 0.683935,
Max Acc:0.683935
Train 14, loss: 0.686595, train acc: 0.647438, 
Test 14, loss: 0.669194, test acc: 0.683935,
Max Acc:0.683935
Train 15, loss: 0.686927, train acc: 0.644941, 
Test 15, loss: 0.669959, test acc: 0.683935,
Max Acc:0.683935
Train 16, loss: 0.685600, train acc: 0.646124, 
Test 16, loss: 0.683959, test acc: 0.683935,
Max Acc:0.683935
Train 17, loss: 0.687889, train acc: 0.646912, 
Test 17, loss: 0.669978, test acc: 0.683935,
Max Acc:0.683935
Train 18, loss: 0.684177, train acc: 0.652431, 
Test 18, loss: 0.719550, test acc: 0.683935,
Max Acc:0.683935
Train 19, loss: 0.684043, train acc: 0.652300, 
Test 19, loss: 0.683457, test acc: 0.683935,
Max Acc:0.683935
Train 20, loss: 0.681689, train acc: 0.650329, 
Test 20, loss: 0.676608, test acc: 0.683935,
Max Acc:0.683935
Train 21, loss: 0.685840, train acc: 0.650854, 
Test 21, loss: 0.702054, test acc: 0.316065,
Train 22, loss: 0.685718, train acc: 0.637845, 
Test 22, loss: 0.720213, test acc: 0.316065,
Train 23, loss: 0.685935, train acc: 0.650854, 
Test 23, loss: 0.679139, test acc: 0.683935,
Max Acc:0.683935
Train 24, loss: 0.689326, train acc: 0.639816, 
Test 24, loss: 0.685198, test acc: 0.683935,
Max Acc:0.683935
Train 25, loss: 0.685830, train acc: 0.642313, 
Test 25, loss: 0.671110, test acc: 0.683935,
Max Acc:0.683935
Train 26, loss: 0.685718, train acc: 0.648357, 
Test 26, loss: 0.682908, test acc: 0.683935,
Max Acc:0.683935
Train 27, loss: 0.685803, train acc: 0.649934, 
Test 27, loss: 0.761316, test acc: 0.316065,
Train 28, loss: 0.683780, train acc: 0.646912, 
Test 28, loss: 0.692094, test acc: 0.683935,
Max Acc:0.683935
Train 29, loss: 0.688278, train acc: 0.642707, 
Test 29, loss: 0.688229, test acc: 0.683935,
Max Acc:0.683935
Train 30, loss: 0.687436, train acc: 0.642444, 
Test 30, loss: 0.670166, test acc: 0.683935,
Max Acc:0.683935
Train 31, loss: 0.685458, train acc: 0.648226, 
Test 31, loss: 0.682790, test acc: 0.683935,
Max Acc:0.683935
Train 32, loss: 0.686671, train acc: 0.646386, 
Test 32, loss: 0.680212, test acc: 0.683935,
Max Acc:0.683935
Train 33, loss: 0.683976, train acc: 0.652300, 
Test 33, loss: 0.839518, test acc: 0.316065,
Train 34, loss: 0.687101, train acc: 0.646386, 
Test 34, loss: 0.668773, test acc: 0.683935,
Max Acc:0.683935
Train 35, loss: 0.685855, train acc: 0.645204, 
Test 35, loss: 0.678566, test acc: 0.683935,
Max Acc:0.683935
Train 36, loss: 0.684061, train acc: 0.654402, 
Test 36, loss: 0.683058, test acc: 0.683935,
Max Acc:0.683935
Train 37, loss: 0.686914, train acc: 0.641393, 
Test 37, loss: 0.673792, test acc: 0.683935,
Max Acc:0.683935
Train 38, loss: 0.684640, train acc: 0.652825, 
Test 38, loss: 0.685448, test acc: 0.683935,
Max Acc:0.683935
Train 39, loss: 0.686761, train acc: 0.643627, 
Test 39, loss: 0.668901, test acc: 0.683935,
Max Acc:0.683935
Train 40, loss: 0.685913, train acc: 0.647306, 
Test 40, loss: 0.668702, test acc: 0.683935,
Max Acc:0.683935
Train 41, loss: 0.685940, train acc: 0.643233, 
Test 41, loss: 0.684157, test acc: 0.683935,
Max Acc:0.683935
Train 42, loss: 0.686511, train acc: 0.646781, 
Test 42, loss: 0.668871, test acc: 0.683935,
Max Acc:0.683935
Train 43, loss: 0.685303, train acc: 0.646912, 
Test 43, loss: 0.684972, test acc: 0.683935,
Max Acc:0.683935
Train 44, loss: 0.684592, train acc: 0.649146, 
Test 44, loss: 0.703071, test acc: 0.316065,
Train 45, loss: 0.684529, train acc: 0.650329, 
Test 45, loss: 0.669635, test acc: 0.683935,
Max Acc:0.683935
Train 46, loss: 0.684537, train acc: 0.644547, 
Test 46, loss: 0.668630, test acc: 0.683935,
Max Acc:0.683935
Train 47, loss: 0.683807, train acc: 0.655453, 
Test 47, loss: 0.680883, test acc: 0.683935,
Max Acc:0.683935
Train 48, loss: 0.684105, train acc: 0.641656, 
Test 48, loss: 0.692871, test acc: 0.683935,
Max Acc:0.683935
Train 49, loss: 0.686276, train acc: 0.643758, 
Test 49, loss: 0.696026, test acc: 0.683935,
Max Acc:0.683935
Train 50, loss: 0.682476, train acc: 0.660710, 
Test 50, loss: 0.669153, test acc: 0.683935,
Max Acc:0.683935
Train 51, loss: 0.685437, train acc: 0.653614, 
Test 51, loss: 0.700327, test acc: 0.316065,
Train 52, loss: 0.686322, train acc: 0.639159, 
Test 52, loss: 0.676077, test acc: 0.683935,
Max Acc:0.683935
Train 53, loss: 0.683964, train acc: 0.652431, 
Test 53, loss: 0.737109, test acc: 0.683935,
Max Acc:0.683935
Train 54, loss: 0.685178, train acc: 0.645335, 
Test 54, loss: 0.696965, test acc: 0.683935,
Max Acc:0.683935
Train 55, loss: 0.685010, train acc: 0.652037, 
Test 55, loss: 0.682999, test acc: 0.683935,
Max Acc:0.683935
Train 56, loss: 0.685582, train acc: 0.643758, 
Test 56, loss: 0.713846, test acc: 0.683935,
Max Acc:0.683935
Train 57, loss: 0.686113, train acc: 0.646649, 
Test 57, loss: 0.669114, test acc: 0.683935,
Max Acc:0.683935
Train 58, loss: 0.682037, train acc: 0.654796, 
Test 58, loss: 0.668589, test acc: 0.683935,
Max Acc:0.683935
Train 59, loss: 0.683897, train acc: 0.648226, 
Test 59, loss: 0.669296, test acc: 0.683935,
Max Acc:0.683935
Train 60, loss: 0.684046, train acc: 0.655716, 
Test 60, loss: 0.721251, test acc: 0.316065,
Train 61, loss: 0.685598, train acc: 0.647700, 
Test 61, loss: 0.669575, test acc: 0.683935,
Max Acc:0.683935
Train 62, loss: 0.682750, train acc: 0.654665, 
Test 62, loss: 0.686429, test acc: 0.683935,
Max Acc:0.683935
Train 63, loss: 0.685448, train acc: 0.642970, 
Test 63, loss: 0.668587, test acc: 0.683935,
Max Acc:0.683935
Train 64, loss: 0.685164, train acc: 0.648620, 
Test 64, loss: 0.677017, test acc: 0.683935,
Max Acc:0.683935
Train 65, loss: 0.682938, train acc: 0.654796, 
Test 65, loss: 0.669669, test acc: 0.683935,
Max Acc:0.683935
Train 66, loss: 0.685237, train acc: 0.646386, 
Test 66, loss: 0.668875, test acc: 0.683935,
Max Acc:0.683935
Train 67, loss: 0.684520, train acc: 0.654008, 
Test 67, loss: 0.681716, test acc: 0.683935,
Max Acc:0.683935
Train 68, loss: 0.685154, train acc: 0.646781, 
Test 68, loss: 0.701299, test acc: 0.316065,
Train 69, loss: 0.682267, train acc: 0.655191, 
Test 69, loss: 0.672583, test acc: 0.683935,
Max Acc:0.683935
Train 70, loss: 0.684687, train acc: 0.650854, 
Test 70, loss: 0.670946, test acc: 0.683935,
Max Acc:0.683935
Train 71, loss: 0.682794, train acc: 0.655585, 
Test 71, loss: 0.677246, test acc: 0.683935,
Max Acc:0.683935
Train 72, loss: 0.681146, train acc: 0.661498, 
Test 72, loss: 0.721557, test acc: 0.316065,
Train 73, loss: 0.684031, train acc: 0.657950, 
Test 73, loss: 0.676921, test acc: 0.683935,
Max Acc:0.683935
Train 74, loss: 0.685501, train acc: 0.642970, 
Test 74, loss: 0.680360, test acc: 0.683935,
Max Acc:0.683935
Train 75, loss: 0.681872, train acc: 0.656899, 
Test 75, loss: 0.680502, test acc: 0.683935,
Max Acc:0.683935
Train 76, loss: 0.681852, train acc: 0.649540, 
Test 76, loss: 0.685420, test acc: 0.683935,
Max Acc:0.683935
Train 77, loss: 0.682848, train acc: 0.658739, 
Test 77, loss: 0.697319, test acc: 0.683935,
Max Acc:0.683935
Train 78, loss: 0.685768, train acc: 0.647700, 
Test 78, loss: 0.670237, test acc: 0.683935,
Max Acc:0.683935
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 4
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 4
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 4
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 4
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 4
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 4
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 50
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 1.280202, train acc: 0.655585, 
Test 0, loss: 13.159955, test acc: 0.611983,
Max Acc:0.611983
Train 1, loss: 0.164539, train acc: 0.655848, 
Test 1, loss: 5.523862, test acc: 0.644689,
Max Acc:0.644689
Train 2, loss: 0.165035, train acc: 0.655191, 
Test 2, loss: 5.088616, test acc: 0.527211,
Train 3, loss: 0.164625, train acc: 0.659527, 
Test 3, loss: 1.173782, test acc: 0.594715,
Train 4, loss: 0.164421, train acc: 0.655453, 
Test 4, loss: 1.931161, test acc: 0.573260,
Train 5, loss: 0.164389, train acc: 0.657162, 
Test 5, loss: 0.632404, test acc: 0.382522,
Train 6, loss: 0.164664, train acc: 0.657556, 
Test 6, loss: 0.717609, test acc: 0.621141,
Train 7, loss: 0.165125, train acc: 0.653614, 
Test 7, loss: 0.284285, test acc: 0.635269,
Train 8, loss: 0.164617, train acc: 0.656899, 
Test 8, loss: 0.188705, test acc: 0.644689,
Max Acc:0.644689
Train 9, loss: 0.164345, train acc: 0.659396, 
Test 9, loss: 0.168530, test acc: 0.662219,
Max Acc:0.662219
Train 10, loss: 0.165120, train acc: 0.653219, 
Test 10, loss: 0.160099, test acc: 0.683935,
Max Acc:0.683935
Train 11, loss: 0.165277, train acc: 0.656636, 
Test 11, loss: 0.191249, test acc: 0.316065,
Train 12, loss: 0.163911, train acc: 0.657424, 
Test 12, loss: 0.161260, test acc: 0.683935,
Max Acc:0.683935
Train 13, loss: 0.164620, train acc: 0.661367, 
Test 13, loss: 0.157818, test acc: 0.683935,
Max Acc:0.683935
Train 14, loss: 0.165282, train acc: 0.651380, 
Test 14, loss: 0.161538, test acc: 0.683935,
Max Acc:0.683935
Train 15, loss: 0.164711, train acc: 0.651511, 
Test 15, loss: 0.166764, test acc: 0.683935,
Max Acc:0.683935
Train 16, loss: 0.164486, train acc: 0.650854, 
Test 16, loss: 0.160895, test acc: 0.683935,
Max Acc:0.683935
Train 17, loss: 0.165424, train acc: 0.663206, 
Test 17, loss: 0.158578, test acc: 0.683935,
Max Acc:0.683935
Train 18, loss: 0.164019, train acc: 0.656373, 
Test 18, loss: 0.160234, test acc: 0.683935,
Max Acc:0.683935
Train 19, loss: 0.163994, train acc: 0.665966, 
Test 19, loss: 0.173680, test acc: 0.316065,
Train 20, loss: 0.162325, train acc: 0.663206, 
Test 20, loss: 0.157748, test acc: 0.683935,
Max Acc:0.683935
Train 21, loss: 0.164343, train acc: 0.660315, 
Test 21, loss: 0.173315, test acc: 0.316065,
Train 22, loss: 0.164836, train acc: 0.654139, 
Test 22, loss: 0.190635, test acc: 0.316065,
Train 23, loss: 0.165126, train acc: 0.656899, 
Test 23, loss: 0.162878, test acc: 0.683935,
Max Acc:0.683935
Train 24, loss: 0.166332, train acc: 0.651643, 
Test 24, loss: 0.167837, test acc: 0.683935,
Max Acc:0.683935
Train 25, loss: 0.164536, train acc: 0.656242, 
Test 25, loss: 0.157764, test acc: 0.683935,
Max Acc:0.683935
Train 26, loss: 0.164355, train acc: 0.664126, 
Test 26, loss: 0.167106, test acc: 0.683935,
Max Acc:0.683935
Train 27, loss: 0.164913, train acc: 0.660447, 
Test 27, loss: 0.181826, test acc: 0.316065,
Train 28, loss: 0.164201, train acc: 0.656373, 
Test 28, loss: 0.166230, test acc: 0.683935,
Max Acc:0.683935
Train 29, loss: 0.165624, train acc: 0.650591, 
Test 29, loss: 0.160884, test acc: 0.683935,
Max Acc:0.683935
Train 30, loss: 0.165675, train acc: 0.653482, 
Test 30, loss: 0.159681, test acc: 0.683935,
Max Acc:0.683935
Train 31, loss: 0.164598, train acc: 0.654534, 
Test 31, loss: 0.159837, test acc: 0.683935,
Max Acc:0.683935
Train 32, loss: 0.165026, train acc: 0.654271, 
Test 32, loss: 0.197133, test acc: 0.683935,
Max Acc:0.683935
Train 33, loss: 0.163642, train acc: 0.659921, 
Test 33, loss: 0.239626, test acc: 0.316065,
Train 34, loss: 0.165330, train acc: 0.657950, 
Test 34, loss: 0.169417, test acc: 0.683935,
Max Acc:0.683935
Train 35, loss: 0.165035, train acc: 0.655716, 
Test 35, loss: 0.165148, test acc: 0.683935,
Max Acc:0.683935
Train 36, loss: 0.163745, train acc: 0.660184, 
Test 36, loss: 0.163440, test acc: 0.683935,
Max Acc:0.683935
Train 37, loss: 0.164958, train acc: 0.649277, 
Test 37, loss: 0.160539, test acc: 0.683935,
Max Acc:0.683935
Train 38, loss: 0.164426, train acc: 0.659658, 
Test 38, loss: 0.177440, test acc: 0.683935,
Max Acc:0.683935
Train 39, loss: 0.164754, train acc: 0.651511, 
Test 39, loss: 0.159360, test acc: 0.683935,
Max Acc:0.683935
Train 40, loss: 0.164599, train acc: 0.657556, 
Test 40, loss: 0.157950, test acc: 0.683935,
Max Acc:0.683935
Train 41, loss: 0.165132, train acc: 0.659264, 
Test 41, loss: 0.160814, test acc: 0.683935,
Max Acc:0.683935
Train 42, loss: 0.164565, train acc: 0.659921, 
Test 42, loss: 0.157889, test acc: 0.683935,
Max Acc:0.683935
Train 43, loss: 0.164303, train acc: 0.654534, 
Test 43, loss: 0.183822, test acc: 0.316065,
Train 44, loss: 0.164210, train acc: 0.656505, 
Test 44, loss: 0.160579, test acc: 0.683935,
Max Acc:0.683935
Train 45, loss: 0.164197, train acc: 0.658213, 
Test 45, loss: 0.159078, test acc: 0.683935,
Max Acc:0.683935
Train 46, loss: 0.164213, train acc: 0.658739, 
Test 46, loss: 0.159770, test acc: 0.683935,
Max Acc:0.683935
Train 47, loss: 0.164268, train acc: 0.660972, 
Test 47, loss: 0.167586, test acc: 0.683935,
Max Acc:0.683935
Train 48, loss: 0.164391, train acc: 0.647175, 
Test 48, loss: 0.157847, test acc: 0.683935,
Max Acc:0.683935
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.619033, train acc: 0.659509, 
Test 0, loss: 0.313780, test acc: 0.675439,
Max Acc:0.675439
Train 1, loss: 0.315437, train acc: 0.677768, 
Test 1, loss: 0.314440, test acc: 0.675439,
Max Acc:0.675439
Train 2, loss: 0.312280, train acc: 0.685226, 
Test 2, loss: 0.305908, test acc: 0.695562,
Max Acc:0.695562
Train 3, loss: 0.312042, train acc: 0.686897, 
Test 3, loss: 0.303668, test acc: 0.686791,
Train 4, loss: 0.309338, train acc: 0.691398, 
Test 4, loss: 0.301250, test acc: 0.696852,
Max Acc:0.696852
Train 5, loss: 0.308815, train acc: 0.693841, 
Test 5, loss: 0.327623, test acc: 0.694014,
Train 6, loss: 0.307994, train acc: 0.697698, 
Test 6, loss: 0.299763, test acc: 0.694014,
Train 7, loss: 0.307544, train acc: 0.693198, 
Test 7, loss: 0.302886, test acc: 0.699690,
Max Acc:0.699690
Train 8, loss: 0.307333, train acc: 0.690626, 
Test 8, loss: 0.312365, test acc: 0.675439,
Train 9, loss: 0.305201, train acc: 0.695255, 
Test 9, loss: 0.303276, test acc: 0.675439,
Train 10, loss: 0.305571, train acc: 0.696412, 
Test 10, loss: 0.294194, test acc: 0.694788,
Train 11, loss: 0.303054, train acc: 0.704899, 
Test 11, loss: 0.304996, test acc: 0.675439,
Train 12, loss: 0.300887, train acc: 0.705285, 
Test 12, loss: 0.316781, test acc: 0.675439,
Train 13, loss: 0.302482, train acc: 0.702199, 
Test 13, loss: 0.288380, test acc: 0.697884,
Train 14, loss: 0.300828, train acc: 0.704770, 
Test 14, loss: 0.287299, test acc: 0.733746,
Max Acc:0.733746
Train 15, loss: 0.292005, train acc: 0.714286, 
Test 15, loss: 0.267030, test acc: 0.750258,
Max Acc:0.750258
Train 16, loss: 0.276661, train acc: 0.745660, 
Test 16, loss: 0.245922, test acc: 0.767802,
Max Acc:0.767802
Train 17, loss: 0.233967, train acc: 0.806738, 
Test 17, loss: 0.214698, test acc: 0.802632,
Max Acc:0.802632
Train 18, loss: 0.203436, train acc: 0.843513, 
Test 18, loss: 0.183945, test acc: 0.837461,
Max Acc:0.837461
Train 19, loss: 0.187129, train acc: 0.859200, 
Test 19, loss: 0.225459, test acc: 0.769608,
Train 20, loss: 0.166910, train acc: 0.874759, 
Test 20, loss: 0.149161, test acc: 0.885707,
Max Acc:0.885707
Train 21, loss: 0.176982, train acc: 0.871416, 
Test 21, loss: 0.145565, test acc: 0.881837,
Train 22, loss: 0.180075, train acc: 0.865887, 
Test 22, loss: 0.124778, test acc: 0.902735,
Max Acc:0.902735
Train 23, loss: 0.167483, train acc: 0.885174, 
Test 23, loss: 0.120929, test acc: 0.917699,
Max Acc:0.917699
Train 24, loss: 0.172455, train acc: 0.872573, 
Test 24, loss: 0.121056, test acc: 0.940660,
Max Acc:0.940660
Train 25, loss: 0.171482, train acc: 0.878102, 
Test 25, loss: 0.126323, test acc: 0.916409,
Train 26, loss: 0.170967, train acc: 0.882217, 
Test 26, loss: 0.175978, test acc: 0.878741,
Train 27, loss: 0.159606, train acc: 0.893018, 
Test 27, loss: 0.108318, test acc: 0.942466,
Max Acc:0.942466
Train 28, loss: 0.169030, train acc: 0.881574, 
Test 28, loss: 0.136369, test acc: 0.902993,
Train 29, loss: 0.155716, train acc: 0.893018, 
Test 29, loss: 0.124735, test acc: 0.914603,
Train 30, loss: 0.170999, train acc: 0.878745, 
Test 30, loss: 0.110607, test acc: 0.932663,
Train 31, loss: 0.164917, train acc: 0.885174, 
Test 31, loss: 0.104662, test acc: 0.933437,
Train 32, loss: 0.167022, train acc: 0.880674, 
Test 32, loss: 0.112733, test acc: 0.937049,
Train 33, loss: 0.162533, train acc: 0.887489, 
Test 33, loss: 0.136176, test acc: 0.906347,
Train 34, loss: 0.164906, train acc: 0.884788, 
Test 34, loss: 0.144744, test acc: 0.889835,
Train 35, loss: 0.166491, train acc: 0.884531, 
Test 35, loss: 0.128059, test acc: 0.934727,
Train 36, loss: 0.166304, train acc: 0.881702, 
Test 36, loss: 0.219604, test acc: 0.809598,
Train 37, loss: 0.160074, train acc: 0.888903, 
Test 37, loss: 0.140627, test acc: 0.888545,
Train 38, loss: 0.165381, train acc: 0.887232, 
Test 38, loss: 0.106063, test acc: 0.949690,
Max Acc:0.949690
Train 39, loss: 0.169674, train acc: 0.879259, 
Test 39, loss: 0.146182, test acc: 0.902735,
Train 40, loss: 0.170566, train acc: 0.876045, 
Test 40, loss: 0.132123, test acc: 0.907895,
Train 41, loss: 0.169938, train acc: 0.880031, 
Test 41, loss: 0.296673, test acc: 0.683695,
Train 42, loss: 0.182688, train acc: 0.866787, 
Test 42, loss: 0.168023, test acc: 0.860423,
Train 43, loss: 0.169570, train acc: 0.880802, 
Test 43, loss: 0.240014, test acc: 0.729360,
Train 44, loss: 0.164728, train acc: 0.883631, 
Test 44, loss: 0.126424, test acc: 0.919763,
Train 45, loss: 0.173626, train acc: 0.876559, 
Test 45, loss: 0.129783, test acc: 0.928535,
Train 46, loss: 0.172261, train acc: 0.875145, 
Test 46, loss: 0.149143, test acc: 0.889835,
Train 47, loss: 0.169965, train acc: 0.876045, 
Test 47, loss: 0.124771, test acc: 0.916925,
Train 48, loss: 0.170625, train acc: 0.877974, 
Test 48, loss: 0.203235, test acc: 0.801084,
Train 49, loss: 0.167372, train acc: 0.878616, 
Test 49, loss: 0.235067, test acc: 0.738390,
Train 50, loss: 0.180227, train acc: 0.864344, 
Test 50, loss: 0.135906, test acc: 0.923633,
Train 51, loss: 0.167224, train acc: 0.878874, 
Test 51, loss: 0.145111, test acc: 0.884417,
Train 52, loss: 0.164383, train acc: 0.883631, 
Test 52, loss: 0.120700, test acc: 0.907895,
Train 53, loss: 0.175867, train acc: 0.870773, 
Test 53, loss: 0.180915, test acc: 0.885707,
Train 54, loss: 0.176567, train acc: 0.846342, 
Test 54, loss: 0.187636, test acc: 0.816821,
Train 55, loss: 0.185232, train acc: 0.850199, 
Test 55, loss: 0.115224, test acc: 0.932663,
Train 56, loss: 0.186697, train acc: 0.843642, 
Test 56, loss: 0.422433, test acc: 0.676987,
Train 57, loss: 0.186991, train acc: 0.848399, 
Test 57, loss: 0.124599, test acc: 0.915893,
Train 58, loss: 0.177372, train acc: 0.851485, 
Test 58, loss: 0.123357, test acc: 0.941950,
Train 59, loss: 0.188259, train acc: 0.842484, 
Test 59, loss: 0.154621, test acc: 0.901445,
Train 60, loss: 0.182573, train acc: 0.857271, 
Test 60, loss: 0.150995, test acc: 0.921827,
Train 61, loss: 0.168647, train acc: 0.869744, 
Test 61, loss: 0.112668, test acc: 0.944530,
Train 62, loss: 0.180560, train acc: 0.859457, 
Test 62, loss: 0.183938, test acc: 0.793602,
Train 63, loss: 0.183533, train acc: 0.845699, 
Test 63, loss: 0.163985, test acc: 0.893705,
Train 64, loss: 0.168805, train acc: 0.865629, 
Test 64, loss: 0.116277, test acc: 0.934985,
Train 65, loss: 0.184152, train acc: 0.848271, 
Test 65, loss: 0.120738, test acc: 0.939112,
Train 66, loss: 0.180364, train acc: 0.848528, 
Test 66, loss: 0.111833, test acc: 0.940144,
Train 67, loss: 0.178465, train acc: 0.855086, 
Test 67, loss: 0.215040, test acc: 0.716718,
Train 68, loss: 0.180707, train acc: 0.855728, 
Test 68, loss: 0.121401, test acc: 0.933437,
Train 69, loss: 0.177235, train acc: 0.851357, 
Test 69, loss: 0.140129, test acc: 0.928019,
Train 70, loss: 0.181994, train acc: 0.845827, 
Test 70, loss: 0.217526, test acc: 0.755676,
Train 71, loss: 0.176825, train acc: 0.863315, 
Test 71, loss: 0.115319, test acc: 0.941692,
Train 72, loss: 0.166418, train acc: 0.875788, 
Test 72, loss: 0.108253, test acc: 0.944272,
Train 73, loss: 0.176249, train acc: 0.860357, 
Test 73, loss: 0.138655, test acc: 0.897059,
Train 74, loss: 0.177138, train acc: 0.863058, 
Test 74, loss: 0.171126, test acc: 0.869711,
Train 75, loss: 0.178366, train acc: 0.853028, 
Test 75, loss: 0.135869, test acc: 0.915119,
Train 76, loss: 0.171101, train acc: 0.857914, 
Test 76, loss: 0.132349, test acc: 0.909959,
Train 77, loss: 0.173695, train acc: 0.854571, 
Test 77, loss: 0.223867, test acc: 0.710010,
Train 78, loss: 0.184167, train acc: 0.851742, 
Test 78, loss: 0.166151, test acc: 0.899123,
Train 79, loss: 0.187841, train acc: 0.837598, 
Test 79, loss: 0.165640, test acc: 0.849329,
Train 80, loss: 0.170204, train acc: 0.851228, 
Test 80, loss: 0.126025, test acc: 0.921827,
Train 81, loss: 0.170314, train acc: 0.871416, 
Test 81, loss: 0.113216, test acc: 0.940402,
Train 82, loss: 0.181055, train acc: 0.858814, 
Test 82, loss: 0.139108, test acc: 0.936791,
Train 83, loss: 0.165645, train acc: 0.873344, 
Test 83, loss: 0.130718, test acc: 0.931631,
Train 84, loss: 0.168033, train acc: 0.869744, 
Test 84, loss: 0.117650, test acc: 0.934727,
Train 85, loss: 0.169804, train acc: 0.866401, 
Test 85, loss: 0.165689, test acc: 0.851909,
Train 86, loss: 0.173625, train acc: 0.858172, 
Test 86, loss: 0.183252, test acc: 0.860165,
Train 87, loss: 0.166619, train acc: 0.875016, 
Test 87, loss: 0.125019, test acc: 0.932921,
Train 88, loss: 0.165387, train acc: 0.868587, 
Test 88, loss: 0.162244, test acc: 0.876935,
Train 89, loss: 0.171777, train acc: 0.867301, 
Test 89, loss: 0.134992, test acc: 0.897575,
Train 90, loss: 0.161708, train acc: 0.876431, 
Test 90, loss: 0.122677, test acc: 0.939112,
Train 91, loss: 0.161132, train acc: 0.871287, 
Test 91, loss: 0.152007, test acc: 0.945304,
Train 92, loss: 0.164842, train acc: 0.873730, 
Test 92, loss: 0.111606, test acc: 0.937822,
Train 93, loss: 0.169720, train acc: 0.865372, 
Test 93, loss: 0.180688, test acc: 0.826367,
Train 94, loss: 0.171333, train acc: 0.864986, 
Test 94, loss: 0.209723, test acc: 0.782250,
Train 95, loss: 0.184828, train acc: 0.848271, 
Test 95, loss: 0.312475, test acc: 0.704592,
Train 96, loss: 0.181134, train acc: 0.857786, 
Test 96, loss: 0.139215, test acc: 0.924923,
Train 97, loss: 0.171888, train acc: 0.867430, 
Test 97, loss: 0.134072, test acc: 0.901187,
Train 98, loss: 0.164889, train acc: 0.873602, 
Test 98, loss: 0.170321, test acc: 0.857585,
Train 99, loss: 0.171430, train acc: 0.864472, 
Test 99, loss: 0.144337, test acc: 0.935501,
Train 100, loss: 0.174291, train acc: 0.862543, 
Test 100, loss: 0.176704, test acc: 0.846491,
Train 101, loss: 0.175251, train acc: 0.858429, 
Test 101, loss: 0.221910, test acc: 0.766512,
Train 102, loss: 0.183309, train acc: 0.851099, 
Test 102, loss: 0.141398, test acc: 0.904541,
Train 103, loss: 0.179168, train acc: 0.856757, 
Test 103, loss: 0.328182, test acc: 0.698400,
Train 104, loss: 0.171179, train acc: 0.868330, 
Test 104, loss: 0.149464, test acc: 0.899639,
Train 105, loss: 0.176198, train acc: 0.864858, 
Test 105, loss: 0.113908, test acc: 0.934727,
Train 106, loss: 0.168129, train acc: 0.868587, 
Test 106, loss: 0.148540, test acc: 0.890867,
Train 107, loss: 0.165412, train acc: 0.865629, 
Test 107, loss: 0.153425, test acc: 0.889061,
Train 108, loss: 0.176928, train acc: 0.866658, 
Test 108, loss: 0.183825, test acc: 0.819143,
Train 109, loss: 0.177623, train acc: 0.861129, 
Test 109, loss: 0.166349, test acc: 0.877451,
Train 110, loss: 0.166533, train acc: 0.865629, 
Test 110, loss: 0.136543, test acc: 0.906605,
Train 111, loss: 0.170891, train acc: 0.865758, 
Test 111, loss: 0.113899, test acc: 0.936275,
Train 112, loss: 0.183284, train acc: 0.853542, 
Test 112, loss: 0.157486, test acc: 0.916667,
Train 113, loss: 0.170305, train acc: 0.868201, 
Test 113, loss: 0.199651, test acc: 0.836687,
Train 114, loss: 0.165830, train acc: 0.871673, 
Test 114, loss: 0.257460, test acc: 0.756450,
Train 115, loss: 0.164970, train acc: 0.868973, 
Test 115, loss: 0.203578, test acc: 0.815789,
Train 116, loss: 0.172147, train acc: 0.856371, 
Test 116, loss: 0.280299, test acc: 0.755160,
Train 117, loss: 0.173363, train acc: 0.864729, 
Test 117, loss: 0.146629, test acc: 0.890609,
Train 118, loss: 0.164889, train acc: 0.875916, 
Test 118, loss: 0.224668, test acc: 0.825593,
Train 119, loss: 0.164383, train acc: 0.875788, 
Test 119, loss: 0.152179, test acc: 0.884933,
Train 120, loss: 0.169140, train acc: 0.863443, 
Test 120, loss: 0.267515, test acc: 0.708720,
Train 121, loss: 0.170147, train acc: 0.866272, 
Test 121, loss: 0.162550, test acc: 0.869711,
Train 122, loss: 0.163798, train acc: 0.877845, 
Test 122, loss: 0.107631, test acc: 0.947884,
Train 123, loss: 0.167433, train acc: 0.866915, 
Test 123, loss: 0.450268, test acc: 0.729618,
Train 124, loss: 0.169451, train acc: 0.868973, 
Test 124, loss: 0.137491, test acc: 0.904541,
Train 125, loss: 0.164002, train acc: 0.871801, 
Test 125, loss: 0.127192, test acc: 0.934727,
Train 126, loss: 0.174085, train acc: 0.860743, 
Test 126, loss: 0.119797, test acc: 0.931889,
Train 127, loss: 0.161256, train acc: 0.872959, 
Test 127, loss: 0.352022, test acc: 0.713106,
Train 128, loss: 0.171992, train acc: 0.865501, 
Test 128, loss: 0.162462, test acc: 0.883901,
Train 129, loss: 0.166776, train acc: 0.873730, 
Test 129, loss: 0.110201, test acc: 0.937307,
Train 130, loss: 0.166299, train acc: 0.877845, 
Test 130, loss: 0.167406, test acc: 0.860681,
Train 131, loss: 0.169401, train acc: 0.876302, 
Test 131, loss: 0.179259, test acc: 0.844943,
Train 132, loss: 0.171333, train acc: 0.863058, 
Test 132, loss: 0.121425, test acc: 0.936275,
Train 133, loss: 0.168189, train acc: 0.868587, 
Test 133, loss: 0.120113, test acc: 0.938854,
Train 134, loss: 0.156390, train acc: 0.880931, 
Test 134, loss: 0.104891, test acc: 0.931631,
Train 135, loss: 0.172453, train acc: 0.874502, 
Test 135, loss: 1.534794, test acc: 0.324561,
Train 136, loss: 0.164414, train acc: 0.872316, 
Test 136, loss: 4.222449, test acc: 0.324561,
Train 137, loss: 0.172371, train acc: 0.870387, 
Test 137, loss: 0.184849, test acc: 0.825335,
Train 138, loss: 0.166726, train acc: 0.874116, 
Test 138, loss: 0.132901, test acc: 0.936533,
Train 139, loss: 0.161350, train acc: 0.873344, 
Test 139, loss: 0.101950, test acc: 0.947626,
Train 140, loss: 0.163432, train acc: 0.872830, 
Test 140, loss: 7.197252, test acc: 0.324561,
Train 141, loss: 0.163816, train acc: 0.875145, 
Test 141, loss: 0.356680, test acc: 0.730134,
Train 142, loss: 0.176554, train acc: 0.860872, 
Test 142, loss: 0.123191, test acc: 0.944530,
Train 143, loss: 0.163245, train acc: 0.874759, 
Test 143, loss: 0.114757, test acc: 0.932663,
Train 144, loss: 0.157902, train acc: 0.876431, 
Test 144, loss: 0.161983, test acc: 0.894995,
Train 145, loss: 0.154834, train acc: 0.888132, 
Test 145, loss: 0.179923, test acc: 0.845459,
Train 146, loss: 0.158917, train acc: 0.878102, 
Test 146, loss: 0.144406, test acc: 0.928535,
Train 147, loss: 0.162665, train acc: 0.873987, 
Test 147, loss: 0.131410, test acc: 0.919247,
Train 148, loss: 0.162198, train acc: 0.875659, 
Test 148, loss: 0.143915, test acc: 0.898349,
Train 149, loss: 0.164610, train acc: 0.877588, 
Test 149, loss: 0.277469, test acc: 0.679825,
Train 150, loss: 0.165935, train acc: 0.869616, 
Test 150, loss: 0.131308, test acc: 0.926213,
Train 151, loss: 0.163498, train acc: 0.879002, 
Test 151, loss: 0.191622, test acc: 0.826109,
Train 152, loss: 0.162542, train acc: 0.870001, 
Test 152, loss: 0.135134, test acc: 0.920795,
Train 153, loss: 0.211756, train acc: 0.814967, 
Test 153, loss: 0.316898, test acc: 0.675439,
Train 154, loss: 0.311822, train acc: 0.687926, 
Test 154, loss: 0.291136, test acc: 0.724716,
Train 155, loss: 0.293175, train acc: 0.718272, 
Test 155, loss: 0.308787, test acc: 0.675439,
Train 156, loss: 0.287261, train acc: 0.727401, 
Test 156, loss: 0.282117, test acc: 0.704076,
Train 157, loss: 0.309540, train acc: 0.689083, 
Test 157, loss: 0.315382, test acc: 0.675439,
Train 158, loss: 0.312547, train acc: 0.683940, 
Test 158, loss: 0.312281, test acc: 0.676213,
Train 159, loss: 0.308725, train acc: 0.683554, 
Test 159, loss: 0.307355, test acc: 0.676213,
Train 160, loss: 0.286657, train acc: 0.726758, 
Test 160, loss: 0.245924, test acc: 0.813209,
Train 161, loss: 0.263560, train acc: 0.765719, 
Test 161, loss: 0.309747, test acc: 0.675955,
Train 162, loss: 0.223082, train acc: 0.815739, 
Test 162, loss: 0.223615, test acc: 0.779928,
Train 163, loss: 0.216720, train acc: 0.824354, 
Test 163, loss: 0.302879, test acc: 0.722652,
Train 164, loss: 0.237859, train acc: 0.810210, 
Test 164, loss: 0.269689, test acc: 0.753612,
Train 165, loss: 0.236578, train acc: 0.798508, 
Test 165, loss: 0.191428, test acc: 0.862229,
Train 166, loss: 0.226701, train acc: 0.812524, 
Test 166, loss: 0.239941, test acc: 0.765738,
Train 167, loss: 0.234396, train acc: 0.796837, 
Test 167, loss: 0.176935, test acc: 0.853715,
Train 168, loss: 0.223154, train acc: 0.814839, 
Test 168, loss: 0.183860, test acc: 0.853973,
Train 169, loss: 0.213223, train acc: 0.829240, 
Test 169, loss: 0.220921, test acc: 0.788700,
Train 170, loss: 0.218175, train acc: 0.822168, 
Test 170, loss: 0.277829, test acc: 0.718008,
Train 171, loss: 0.216807, train acc: 0.821396, 
Test 171, loss: 0.184153, test acc: 0.860681,
Train 172, loss: 0.209623, train acc: 0.832069, 
Test 172, loss: 0.182015, test acc: 0.843911,
Train 173, loss: 0.213308, train acc: 0.821782, 
Test 173, loss: 0.310387, test acc: 0.734778,
Train 174, loss: 0.261143, train acc: 0.759033, 
Test 174, loss: 0.180121, test acc: 0.851393,
Train 175, loss: 0.210322, train acc: 0.831040, 
Test 175, loss: 0.258429, test acc: 0.760062,
Train 176, loss: 0.222038, train acc: 0.814453, 
Test 176, loss: 0.229529, test acc: 0.779928,
Train 177, loss: 0.210949, train acc: 0.827311, 
Test 177, loss: 0.226331, test acc: 0.775026,
Train 178, loss: 0.209356, train acc: 0.824740, 
Test 178, loss: 0.201413, test acc: 0.817079,
Train 179, loss: 0.202236, train acc: 0.836827, 
Test 179, loss: 0.218903, test acc: 0.789474,
Train 180, loss: 0.208071, train acc: 0.832326, 
Test 180, loss: 0.195483, test acc: 0.834365,
Train 181, loss: 0.203822, train acc: 0.834255, 
Test 181, loss: 0.210773, test acc: 0.803406,
Train 182, loss: 0.205112, train acc: 0.837598, 
Test 182, loss: 0.208928, test acc: 0.799020,
Train 183, loss: 0.214931, train acc: 0.826025, 
Test 183, loss: 0.363285, test acc: 0.675439,
Train 184, loss: 0.213937, train acc: 0.824997, 
Test 184, loss: 0.234273, test acc: 0.767286,
Train 185, loss: 0.202525, train acc: 0.836569, 
Test 185, loss: 0.224063, test acc: 0.804438,
Train 186, loss: 0.210296, train acc: 0.830012, 
Test 186, loss: 0.376653, test acc: 0.675955,
Train 187, loss: 0.221451, train acc: 0.815096, 
Test 187, loss: 0.186798, test acc: 0.861455,
Train 188, loss: 0.215408, train acc: 0.819725, 
Test 188, loss: 0.207449, test acc: 0.818885,
Train 189, loss: 0.232328, train acc: 0.804938, 
Test 189, loss: 0.183561, test acc: 0.850877,
Train 190, loss: 0.231976, train acc: 0.811495, 
Test 190, loss: 0.212705, test acc: 0.852425,
Train 191, loss: 0.240374, train acc: 0.805323, 
Test 191, loss: 0.170327, test acc: 0.890351,
Train 192, loss: 0.218236, train acc: 0.822297, 
Test 192, loss: 0.183081, test acc: 0.871259,
Train 193, loss: 0.213582, train acc: 0.832969, 
Test 193, loss: 0.180100, test acc: 0.870743,
Train 194, loss: 0.206225, train acc: 0.840813, 
Test 194, loss: 0.156051, test acc: 0.905315,
Train 195, loss: 0.198555, train acc: 0.840041, 
Test 195, loss: 0.179336, test acc: 0.861197,
Train 196, loss: 0.196271, train acc: 0.849685, 
Test 196, loss: 0.272740, test acc: 0.741744,
Train 197, loss: 0.203838, train acc: 0.838627, 
Test 197, loss: 0.225626, test acc: 0.761352,
Train 198, loss: 0.200338, train acc: 0.840170, 
Test 198, loss: 0.158629, test acc: 0.895511,
Train 199, loss: 0.189572, train acc: 0.857014, 
Test 199, loss: 0.227740, test acc: 0.764706,
Train 200, loss: 0.214478, train acc: 0.830140, 
Test 200, loss: 0.156177, test acc: 0.891125,
Train 201, loss: 0.194328, train acc: 0.850328, 
Test 201, loss: 0.216585, test acc: 0.795924,
Train 202, loss: 0.200289, train acc: 0.846599, 
Test 202, loss: 0.160234, test acc: 0.892931,
Train 203, loss: 0.188203, train acc: 0.860743, 
Test 203, loss: 0.141271, test acc: 0.911249,
Train 204, loss: 0.197464, train acc: 0.849556, 
Test 204, loss: 0.157108, test acc: 0.886481,
Train 205, loss: 0.193832, train acc: 0.853285, 
Test 205, loss: 0.176952, test acc: 0.851651,
Train 206, loss: 0.194595, train acc: 0.848399, 
Test 206, loss: 0.241567, test acc: 0.777090,
Train 207, loss: 0.199987, train acc: 0.846728, 
Test 207, loss: 0.155119, test acc: 0.891383,
Train 208, loss: 0.186917, train acc: 0.861772, 
Test 208, loss: 0.185263, test acc: 0.841073,
Train 209, loss: 0.195472, train acc: 0.848656, 
Test 209, loss: 0.168887, test acc: 0.884675,
Train 210, loss: 0.190003, train acc: 0.854957, 
Test 210, loss: 0.148689, test acc: 0.915635,
Train 211, loss: 0.191472, train acc: 0.852642, 
Test 211, loss: 0.179866, test acc: 0.846491,
Train 212, loss: 0.195712, train acc: 0.854700, 
Test 212, loss: 0.207159, test acc: 0.816047,
Train 213, loss: 0.189113, train acc: 0.863829, 
Test 213, loss: 0.151130, test acc: 0.906863,
Train 214, loss: 0.178695, train acc: 0.867944, 
Test 214, loss: 0.204611, test acc: 0.791022,
Train 215, loss: 0.189921, train acc: 0.857786, 
Test 215, loss: 0.181224, test acc: 0.864035,
Train 216, loss: 0.194416, train acc: 0.853928, 
Test 216, loss: 0.147844, test acc: 0.914603,
Train 217, loss: 0.184131, train acc: 0.861900, 
Test 217, loss: 0.316920, test acc: 0.695820,
Train 218, loss: 0.187997, train acc: 0.861900, 
Test 218, loss: 0.194931, test acc: 0.835139,
Train 219, loss: 0.185379, train acc: 0.858686, 
Test 219, loss: 2.024177, test acc: 0.327915,
Train 220, loss: 0.179646, train acc: 0.871416, 
Test 220, loss: 0.277295, test acc: 0.728328,
Train 221, loss: 0.187799, train acc: 0.861258, 
Test 221, loss: 0.491251, test acc: 0.501548,
Train 222, loss: 0.187450, train acc: 0.861772, 
Test 222, loss: 0.205643, test acc: 0.798762,
Train 223, loss: 0.192479, train acc: 0.853671, 
Test 223, loss: 0.344519, test acc: 0.690144,
Train 224, loss: 0.181741, train acc: 0.862543, 
Test 224, loss: 0.190697, test acc: 0.831011,
Train 225, loss: 0.192280, train acc: 0.858557, 
Test 225, loss: 0.224631, test acc: 0.782508,
Train 226, loss: 0.191620, train acc: 0.851614, 
Test 226, loss: 0.189458, test acc: 0.885191,
Train 227, loss: 0.188930, train acc: 0.857143, 
Test 227, loss: 0.301967, test acc: 0.715428,
Train 228, loss: 0.187571, train acc: 0.858686, 
Test 228, loss: 0.196556, test acc: 0.838493,
Train 229, loss: 0.187647, train acc: 0.858557, 
Test 229, loss: 0.215238, test acc: 0.808566,
Train 230, loss: 0.185804, train acc: 0.857529, 
Test 230, loss: 0.219147, test acc: 0.814241,
Train 231, loss: 0.187634, train acc: 0.860872, 
Test 231, loss: 0.226764, test acc: 0.769350,
Train 232, loss: 0.185598, train acc: 0.867687, 
Test 232, loss: 0.163431, test acc: 0.889577,
Train 233, loss: 0.190015, train acc: 0.858557, 
Test 233, loss: 0.147158, test acc: 0.924923,
Train 234, loss: 0.181219, train acc: 0.866144, 
Test 234, loss: 0.159934, test acc: 0.892157,
Train 235, loss: 0.176766, train acc: 0.867430, 
Test 235, loss: 0.154630, test acc: 0.891899,
Train 236, loss: 0.179627, train acc: 0.863829, 
Test 236, loss: 0.159260, test acc: 0.876419,
Train 237, loss: 0.180694, train acc: 0.863315, 
Test 237, loss: 0.176154, test acc: 0.870485,
Train 238, loss: 0.187750, train acc: 0.857786, 
Test 238, loss: 0.183363, test acc: 0.906863,
Train 239, loss: 0.185044, train acc: 0.860615, 
Test 239, loss: 0.197572, test acc: 0.855263,
Train 240, loss: 0.184947, train acc: 0.861386, 
Test 240, loss: 0.142049, test acc: 0.928793,
Train 241, loss: 0.177897, train acc: 0.868201, 
Test 241, loss: 0.189086, test acc: 0.833075,
Train 242, loss: 0.180967, train acc: 0.864086, 
Test 242, loss: 0.389046, test acc: 0.590815,
Train 243, loss: 0.195431, train acc: 0.848528, 
Test 243, loss: 0.286090, test acc: 0.723684,
Train 244, loss: 0.182399, train acc: 0.864472, 
Test 244, loss: 0.854038, test acc: 0.324561,
Train 245, loss: 0.181465, train acc: 0.864086, 
Test 245, loss: 0.130316, test acc: 0.928019,
Train 246, loss: 0.183617, train acc: 0.868973, 
Test 246, loss: 0.182849, test acc: 0.878225,
Train 247, loss: 0.201329, train acc: 0.848785, 
Test 247, loss: 0.193793, test acc: 0.848813,
Train 248, loss: 0.200174, train acc: 0.845185, 
Test 248, loss: 0.159836, test acc: 0.901961,
Train 249, loss: 0.203590, train acc: 0.847885, 
Test 249, loss: 0.242122, test acc: 0.790764,
Train 250, loss: 0.202047, train acc: 0.843642, 
Test 250, loss: 0.381928, test acc: 0.693240,
Train 251, loss: 0.196643, train acc: 0.851742, 
Test 251, loss: 0.152819, test acc: 0.914603,
Train 252, loss: 0.195762, train acc: 0.849171, 
Test 252, loss: 0.363533, test acc: 0.674407,
Train 253, loss: 0.195358, train acc: 0.855214, 
Test 253, loss: 0.212159, test acc: 0.807276,
Train 254, loss: 0.196005, train acc: 0.850585, 
Test 254, loss: 0.294087, test acc: 0.692466,
Train 255, loss: 0.197398, train acc: 0.845185, 
Test 255, loss: 0.180730, test acc: 0.886997,
Train 256, loss: 0.197332, train acc: 0.847756, 
Test 256, loss: 0.171472, test acc: 0.883901,
Train 257, loss: 0.192986, train acc: 0.855471, 
Test 257, loss: 0.167359, test acc: 0.899897,
Train 258, loss: 0.193549, train acc: 0.849428, 
Test 258, loss: 0.156550, test acc: 0.896801,
Train 259, loss: 0.197183, train acc: 0.849556, 
Test 259, loss: 0.190660, test acc: 0.884159,
Train 260, loss: 0.210469, train acc: 0.829626, 
Test 260, loss: 0.236866, test acc: 0.849329,
Train 261, loss: 0.236675, train acc: 0.801980, 
Test 261, loss: 0.173423, test acc: 0.885707,
Train 262, loss: 0.214520, train acc: 0.828211, 
Test 262, loss: 0.177349, test acc: 0.912797,
Train 263, loss: 0.205626, train acc: 0.833226, 
Test 263, loss: 0.469195, test acc: 0.411249,
Train 264, loss: 0.193502, train acc: 0.852514, 
Test 264, loss: 0.169852, test acc: 0.882353,
Train 265, loss: 0.196010, train acc: 0.853671, 
Test 265, loss: 0.164498, test acc: 0.878741,
Train 266, loss: 0.191853, train acc: 0.855214, 
Test 266, loss: 0.177113, test acc: 0.885449,
Train 267, loss: 0.197952, train acc: 0.849299, 
Test 267, loss: 0.192587, test acc: 0.839783,
Train 268, loss: 0.192208, train acc: 0.855728, 
Test 268, loss: 0.177811, test acc: 0.901445,
Train 269, loss: 0.194012, train acc: 0.855343, 
Test 269, loss: 0.161573, test acc: 0.909701,
Train 270, loss: 0.191442, train acc: 0.857014, 
Test 270, loss: 0.151884, test acc: 0.909185,
Train 271, loss: 0.193916, train acc: 0.851999, 
Test 271, loss: 0.232413, test acc: 0.750774,
Train 272, loss: 0.187245, train acc: 0.861000, 
Test 272, loss: 0.156810, test acc: 0.922343,
Train 273, loss: 0.185970, train acc: 0.859843, 
Test 273, loss: 0.186514, test acc: 0.852683,
Train 274, loss: 0.192394, train acc: 0.855986, 
Test 274, loss: 0.159271, test acc: 0.908411,
Train 275, loss: 0.191121, train acc: 0.855986, 
Test 275, loss: 0.193196, test acc: 0.846233,
Train 276, loss: 0.193822, train acc: 0.855471, 
Test 276, loss: 0.178520, test acc: 0.861713,
Train 277, loss: 0.194873, train acc: 0.852385, 
Test 277, loss: 0.212931, test acc: 0.771414,
Train 278, loss: 0.186336, train acc: 0.858043, 
Test 278, loss: 0.148743, test acc: 0.904025,
Train 279, loss: 0.191861, train acc: 0.860486, 
Test 279, loss: 0.152335, test acc: 0.912281,
Train 280, loss: 0.192924, train acc: 0.851485, 
Test 280, loss: 0.162521, test acc: 0.894221,
Train 281, loss: 0.191701, train acc: 0.857014, 
Test 281, loss: 0.157754, test acc: 0.911249,
Train 282, loss: 0.197611, train acc: 0.845185, 
Test 282, loss: 0.157045, test acc: 0.901703,
Train 283, loss: 0.188903, train acc: 0.857014, 
Test 283, loss: 0.161844, test acc: 0.886223,
Train 284, loss: 0.189279, train acc: 0.856757, 
Test 284, loss: 0.170074, test acc: 0.888287,
Train 285, loss: 0.188275, train acc: 0.859586, 
Test 285, loss: 0.149773, test acc: 0.907379,
Train 286, loss: 0.183438, train acc: 0.863829, 
Test 286, loss: 0.162679, test acc: 0.878483,
Train 287, loss: 0.182809, train acc: 0.858172, 
Test 287, loss: 0.187795, test acc: 0.851393,
Train 288, loss: 0.188474, train acc: 0.858043, 
Test 288, loss: 0.202496, test acc: 0.840557,
Train 289, loss: 0.183628, train acc: 0.861386, 
Test 289, loss: 0.210124, test acc: 0.808308,
Train 290, loss: 0.181387, train acc: 0.862158, 
Test 290, loss: 0.233101, test acc: 0.765996,
Train 291, loss: 0.184403, train acc: 0.865115, 
Test 291, loss: 0.203142, test acc: 0.838235,
Train 292, loss: 0.200649, train acc: 0.841584, 
Test 292, loss: 0.158947, test acc: 0.902219,
Train 293, loss: 0.183590, train acc: 0.862929, 
Test 293, loss: 0.357223, test acc: 0.713106,
Train 294, loss: 0.177585, train acc: 0.868330, 
Test 294, loss: 0.162193, test acc: 0.894221,
Train 295, loss: 0.189242, train acc: 0.853414, 
Test 295, loss: 0.157023, test acc: 0.888029,
Train 296, loss: 0.181754, train acc: 0.863443, 
Test 296, loss: 0.194828, test acc: 0.828689,
Train 297, loss: 0.181878, train acc: 0.862929, 
Test 297, loss: 0.155312, test acc: 0.890351,
Train 298, loss: 0.186091, train acc: 0.862158, 
Test 298, loss: 0.157296, test acc: 0.890609,
Train 299, loss: 0.180680, train acc: 0.863186, 
Test 299, loss: 0.171705, test acc: 0.874871,
Train 300, loss: 0.184564, train acc: 0.862929, 
Test 300, loss: 0.169858, test acc: 0.885707,
Train 301, loss: 0.184648, train acc: 0.859843, 
Test 301, loss: 0.199878, test acc: 0.826883,
Train 302, loss: 0.190582, train acc: 0.856500, 
Test 302, loss: 0.162946, test acc: 0.891125,
Train 303, loss: 0.188100, train acc: 0.856500, 
Test 303, loss: 0.148417, test acc: 0.909185,
Train 304, loss: 0.188257, train acc: 0.861129, 
Test 304, loss: 0.161260, test acc: 0.887255,
Train 305, loss: 0.190455, train acc: 0.852514, 
Test 305, loss: 0.157692, test acc: 0.899897,
Train 306, loss: 0.180000, train acc: 0.861129, 
Test 306, loss: 0.169706, test acc: 0.869969,
Train 307, loss: 0.180210, train acc: 0.865244, 
Test 307, loss: 0.183185, test acc: 0.865325,
Train 308, loss: 0.181995, train acc: 0.866530, 
Test 308, loss: 0.189650, test acc: 0.826625,
Train 309, loss: 0.180397, train acc: 0.863058, 
Test 309, loss: 0.179263, test acc: 0.864551,
Train 310, loss: 0.182969, train acc: 0.862929, 
Test 310, loss: 0.741192, test acc: 0.634933,
Train 311, loss: 0.181610, train acc: 0.863315, 
Test 311, loss: 0.155169, test acc: 0.916409,
Train 312, loss: 0.180805, train acc: 0.864858, 
Test 312, loss: 0.155959, test acc: 0.902993,
Train 313, loss: 0.179196, train acc: 0.867944, 
Test 313, loss: 0.156132, test acc: 0.894737,
Train 314, loss: 0.174020, train acc: 0.874116, 
Test 314, loss: 0.164877, test acc: 0.876677,
Train 315, loss: 0.181775, train acc: 0.864601, 
Test 315, loss: 0.211797, test acc: 0.789990,
Train 316, loss: 0.174426, train acc: 0.869358, 
Test 316, loss: 0.176455, test acc: 0.876161,
Train 317, loss: 0.176409, train acc: 0.873087, 
Test 317, loss: 11.962504, test acc: 0.324561,
Train 318, loss: 0.246292, train acc: 0.782050, 
Test 318, loss: 0.258409, test acc: 0.750774,
Train 319, loss: 0.261975, train acc: 0.765462, 
Test 319, loss: 0.232076, test acc: 0.792570,
Train 320, loss: 0.232890, train acc: 0.811624, 
Test 320, loss: 0.217930, test acc: 0.832301,
Train 321, loss: 0.230106, train acc: 0.809952, 
Test 321, loss: 0.245839, test acc: 0.796182,
Train 322, loss: 0.242441, train acc: 0.792979, 
Test 322, loss: 0.241515, test acc: 0.796698,
Train 323, loss: 0.236705, train acc: 0.805195, 
Test 323, loss: 0.227954, test acc: 0.800568,
Train 324, loss: 0.225599, train acc: 0.821654, 
Test 324, loss: 0.203990, test acc: 0.853715,
Train 325, loss: 0.230246, train acc: 0.818053, 
Test 325, loss: 0.219083, test acc: 0.841589,
Train 326, loss: 0.230571, train acc: 0.822039, 
Test 326, loss: 0.197899, test acc: 0.873065,
Train 327, loss: 0.222010, train acc: 0.821396, 
Test 327, loss: 0.195709, test acc: 0.867647,
Train 328, loss: 0.219814, train acc: 0.830654, 
Test 328, loss: 0.181391, test acc: 0.873581,
Train 329, loss: 0.232647, train acc: 0.817667, 
Test 329, loss: 0.195845, test acc: 0.868421,
Train 330, loss: 0.215807, train acc: 0.831426, 
Test 330, loss: 0.204326, test acc: 0.846233,
Train 331, loss: 0.216654, train acc: 0.828983, 
Test 331, loss: 0.202382, test acc: 0.860681,
Train 332, loss: 0.213672, train acc: 0.831940, 
Test 332, loss: 0.182979, test acc: 0.876935,
Train 333, loss: 0.218446, train acc: 0.828726, 
Test 333, loss: 0.215001, test acc: 0.827399,
Train 334, loss: 0.223855, train acc: 0.822039, 
Test 334, loss: 0.220714, test acc: 0.823787,
Train 335, loss: 0.220000, train acc: 0.822682, 
Test 335, loss: 0.196934, test acc: 0.869711,
Train 336, loss: 0.228159, train acc: 0.816896, 
Test 336, loss: 0.223670, test acc: 0.831527,
Train 337, loss: 0.233128, train acc: 0.811367, 
Test 337, loss: 0.183087, test acc: 0.884933,
Train 338, loss: 0.212453, train acc: 0.834641, 
Test 338, loss: 0.194748, test acc: 0.872033,
Train 339, loss: 0.208665, train acc: 0.835155, 
Test 339, loss: 0.170541, test acc: 0.887771,
Train 340, loss: 0.205597, train acc: 0.840941, 
Test 340, loss: 0.175857, test acc: 0.890609,
Train 341, loss: 0.214033, train acc: 0.833355, 
Test 341, loss: 0.198147, test acc: 0.854747,
Train 342, loss: 0.213502, train acc: 0.832712, 
Test 342, loss: 0.186463, test acc: 0.878225,
Train 343, loss: 0.211530, train acc: 0.835669, 
Test 343, loss: 0.175421, test acc: 0.883901,
Train 344, loss: 0.203913, train acc: 0.841970, 
Test 344, loss: 0.197162, test acc: 0.847007,
Train 345, loss: 0.208558, train acc: 0.839527, 
Test 345, loss: 0.183176, test acc: 0.878225,
Train 346, loss: 0.208504, train acc: 0.836827, 
Test 346, loss: 0.180568, test acc: 0.889319,
Train 347, loss: 0.212300, train acc: 0.836312, 
Test 347, loss: 0.199697, test acc: 0.838751,
Train 348, loss: 0.211922, train acc: 0.835926, 
Test 348, loss: 0.180727, test acc: 0.883901,
Train 349, loss: 0.213522, train acc: 0.832326, 
Test 349, loss: 0.176913, test acc: 0.889577,
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.341284, train acc: 0.902807, 
Test 0, loss: 0.138292, test acc: 0.924040,
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.415396, train acc: 0.898199, 
Test 0, loss: 0.159203, test acc: 0.924040,
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.544458, train acc: 0.897780, 
Test 0, loss: 0.139077, test acc: 0.924040,
Max Acc:0.924040
Train 1, loss: 0.112093, train acc: 0.932970, 
Test 1, loss: 0.126833, test acc: 0.924040,
Max Acc:0.924040
Train 2, loss: 0.110579, train acc: 0.936322, 
Test 2, loss: 0.126593, test acc: 0.924040,
Max Acc:0.924040
Train 3, loss: 0.105053, train acc: 0.936322, 
Test 3, loss: 0.119743, test acc: 0.924040,
Max Acc:0.924040
Train 4, loss: 0.107058, train acc: 0.934646, 
Test 4, loss: 0.128592, test acc: 0.924040,
Max Acc:0.924040
Train 5, loss: 0.106102, train acc: 0.934646, 
Test 5, loss: 0.120290, test acc: 0.924040,
Max Acc:0.924040
Train 6, loss: 0.105243, train acc: 0.935484, 
Test 6, loss: 0.113356, test acc: 0.924040,
Max Acc:0.924040
Train 7, loss: 0.128487, train acc: 0.932970, 
Test 7, loss: 0.116646, test acc: 0.924040,
Max Acc:0.924040
Train 8, loss: 0.102392, train acc: 0.933808, 
Test 8, loss: 0.109650, test acc: 0.924040,
Max Acc:0.924040
Train 9, loss: 0.096771, train acc: 0.932970, 
Test 9, loss: 0.114071, test acc: 0.924040,
Max Acc:0.924040
Train 10, loss: 0.097322, train acc: 0.936741, 
Test 10, loss: 0.127957, test acc: 0.924040,
Max Acc:0.924040
Train 11, loss: 0.096409, train acc: 0.933389, 
Test 11, loss: 0.123949, test acc: 0.924040,
Max Acc:0.924040
Train 12, loss: 0.100416, train acc: 0.936322, 
Test 12, loss: 0.127748, test acc: 0.924040,
Max Acc:0.924040
Train 13, loss: 0.103687, train acc: 0.933389, 
Test 13, loss: 0.112556, test acc: 0.924040,
Max Acc:0.924040
Train 14, loss: 0.099712, train acc: 0.933808, 
Test 14, loss: 0.108667, test acc: 0.919866,
Train 15, loss: 0.098018, train acc: 0.935903, 
Test 15, loss: 0.101499, test acc: 0.924040,
Max Acc:0.924040
Train 16, loss: 0.097400, train acc: 0.935903, 
Test 16, loss: 0.102713, test acc: 0.924040,
Max Acc:0.924040
Train 17, loss: 0.090835, train acc: 0.937160, 
Test 17, loss: 0.106924, test acc: 0.926544,
Max Acc:0.926544
Train 18, loss: 0.098743, train acc: 0.935484, 
Test 18, loss: 0.093093, test acc: 0.929883,
Max Acc:0.929883
Train 19, loss: 0.094378, train acc: 0.934646, 
Test 19, loss: 0.153496, test acc: 0.924040,
Train 20, loss: 0.094457, train acc: 0.935903, 
Test 20, loss: 0.094152, test acc: 0.924040,
Train 21, loss: 0.096217, train acc: 0.936322, 
Test 21, loss: 0.102101, test acc: 0.929048,
Train 22, loss: 0.092209, train acc: 0.934646, 
Test 22, loss: 0.096056, test acc: 0.924040,
Train 23, loss: 0.097512, train acc: 0.937579, 
Test 23, loss: 0.138702, test acc: 0.929883,
Max Acc:0.929883
Train 24, loss: 0.106171, train acc: 0.934646, 
Test 24, loss: 0.105298, test acc: 0.924040,
Train 25, loss: 0.099842, train acc: 0.935065, 
Test 25, loss: 0.101363, test acc: 0.924040,
Train 26, loss: 0.098631, train acc: 0.934646, 
Test 26, loss: 0.094835, test acc: 0.924040,
Train 27, loss: 0.097290, train acc: 0.933808, 
Test 27, loss: 0.101361, test acc: 0.924040,
Train 28, loss: 0.093901, train acc: 0.934227, 
Test 28, loss: 0.090590, test acc: 0.941569,
Max Acc:0.941569
Train 29, loss: 0.096496, train acc: 0.931295, 
Test 29, loss: 0.107346, test acc: 0.924040,
Train 30, loss: 0.097562, train acc: 0.934646, 
Test 30, loss: 0.099239, test acc: 0.924040,
Train 31, loss: 0.099289, train acc: 0.934646, 
Test 31, loss: 0.102848, test acc: 0.924040,
Train 32, loss: 0.101104, train acc: 0.932132, 
Test 32, loss: 0.106358, test acc: 0.919032,
Train 33, loss: 0.095413, train acc: 0.931295, 
Test 33, loss: 0.103998, test acc: 0.914023,
Train 34, loss: 0.093062, train acc: 0.933808, 
Test 34, loss: 0.093081, test acc: 0.924040,
Train 35, loss: 0.088232, train acc: 0.936322, 
Test 35, loss: 0.103143, test acc: 0.924040,
Train 36, loss: 0.094141, train acc: 0.936741, 
Test 36, loss: 0.095085, test acc: 0.924040,
Train 37, loss: 0.093927, train acc: 0.936741, 
Test 37, loss: 0.093349, test acc: 0.924040,
Train 38, loss: 0.095487, train acc: 0.934227, 
Test 38, loss: 0.102309, test acc: 0.924040,
Train 39, loss: 0.091355, train acc: 0.930876, 
Test 39, loss: 0.100410, test acc: 0.924040,
Train 40, loss: 0.084367, train acc: 0.937160, 
Test 40, loss: 0.098403, test acc: 0.924040,
Train 41, loss: 0.089985, train acc: 0.933389, 
Test 41, loss: 0.085978, test acc: 0.942404,
Max Acc:0.942404
Train 42, loss: 0.091265, train acc: 0.936741, 
Test 42, loss: 0.092285, test acc: 0.933222,
Train 43, loss: 0.084103, train acc: 0.938835, 
Test 43, loss: 0.095655, test acc: 0.924040,
Train 44, loss: 0.086711, train acc: 0.934646, 
Test 44, loss: 0.088249, test acc: 0.924040,
Train 45, loss: 0.088204, train acc: 0.934646, 
Test 45, loss: 0.090114, test acc: 0.924040,
Train 46, loss: 0.084027, train acc: 0.935484, 
Test 46, loss: 0.094509, test acc: 0.924040,
Train 47, loss: 0.084548, train acc: 0.933808, 
Test 47, loss: 0.097640, test acc: 0.924040,
Train 48, loss: 0.090977, train acc: 0.935065, 
Test 48, loss: 0.104053, test acc: 0.924040,
Train 49, loss: 0.088432, train acc: 0.933808, 
Test 49, loss: 0.085498, test acc: 0.924040,
Train 50, loss: 0.088792, train acc: 0.936741, 
Test 50, loss: 0.083416, test acc: 0.939065,
Train 51, loss: 0.085715, train acc: 0.937160, 
Test 51, loss: 0.096194, test acc: 0.924040,
Train 52, loss: 0.088534, train acc: 0.935484, 
Test 52, loss: 0.087128, test acc: 0.945743,
Max Acc:0.945743
Train 53, loss: 0.091852, train acc: 0.937997, 
Test 53, loss: 0.085580, test acc: 0.941569,
Train 54, loss: 0.085455, train acc: 0.940092, 
Test 54, loss: 0.080143, test acc: 0.949917,
Max Acc:0.949917
Train 55, loss: 0.082640, train acc: 0.939673, 
Test 55, loss: 0.095992, test acc: 0.942404,
Train 56, loss: 0.080982, train acc: 0.940511, 
Test 56, loss: 0.080992, test acc: 0.949082,
Train 57, loss: 0.080498, train acc: 0.938835, 
Test 57, loss: 0.084945, test acc: 0.940735,
Train 58, loss: 0.088621, train acc: 0.943025, 
Test 58, loss: 0.098182, test acc: 0.914858,
Train 59, loss: 0.092797, train acc: 0.934646, 
Test 59, loss: 0.075838, test acc: 0.940735,
Train 60, loss: 0.080360, train acc: 0.937997, 
Test 60, loss: 0.100354, test acc: 0.942404,
Train 61, loss: 0.082296, train acc: 0.942187, 
Test 61, loss: 0.083712, test acc: 0.941569,
Train 62, loss: 0.084192, train acc: 0.939673, 
Test 62, loss: 0.077443, test acc: 0.941569,
Train 63, loss: 0.076701, train acc: 0.941349, 
Test 63, loss: 0.096667, test acc: 0.924040,
Train 64, loss: 0.082497, train acc: 0.941768, 
Test 64, loss: 0.101536, test acc: 0.940735,
Train 65, loss: 0.081338, train acc: 0.944282, 
Test 65, loss: 0.082856, test acc: 0.940735,
Train 66, loss: 0.078180, train acc: 0.946795, 
Test 66, loss: 0.100735, test acc: 0.932387,
Train 67, loss: 0.082933, train acc: 0.944282, 
Test 67, loss: 0.104278, test acc: 0.942404,
Train 68, loss: 0.083707, train acc: 0.940511, 
Test 68, loss: 0.078435, test acc: 0.941569,
Train 69, loss: 0.080411, train acc: 0.942606, 
Test 69, loss: 0.098882, test acc: 0.924040,
Train 70, loss: 0.080407, train acc: 0.941768, 
Test 70, loss: 0.111677, test acc: 0.940735,
Train 71, loss: 0.082910, train acc: 0.934227, 
Test 71, loss: 0.079095, test acc: 0.949082,
Train 72, loss: 0.075454, train acc: 0.939673, 
Test 72, loss: 0.090089, test acc: 0.949082,
Train 73, loss: 0.077346, train acc: 0.941349, 
Test 73, loss: 0.088366, test acc: 0.940735,
Train 74, loss: 0.085917, train acc: 0.936741, 
Test 74, loss: 0.091146, test acc: 0.924040,
Train 75, loss: 0.077952, train acc: 0.940511, 
Test 75, loss: 0.082664, test acc: 0.940735,
Train 76, loss: 0.081057, train acc: 0.945538, 
Test 76, loss: 0.096697, test acc: 0.924040,
Train 77, loss: 0.088392, train acc: 0.936741, 
Test 77, loss: 0.098285, test acc: 0.940735,
Train 78, loss: 0.078289, train acc: 0.945119, 
Test 78, loss: 0.084380, test acc: 0.950751,
Max Acc:0.950751
Train 79, loss: 0.082870, train acc: 0.942606, 
Test 79, loss: 0.081319, test acc: 0.946578,
Train 80, loss: 0.079963, train acc: 0.948890, 
Test 80, loss: 0.129900, test acc: 0.954090,
Max Acc:0.954090
Train 81, loss: 0.084996, train acc: 0.938416, 
Test 81, loss: 0.101014, test acc: 0.933222,
Train 82, loss: 0.076957, train acc: 0.943444, 
Test 82, loss: 0.102321, test acc: 0.924875,
Train 83, loss: 0.074549, train acc: 0.940511, 
Test 83, loss: 0.097626, test acc: 0.942404,
Train 84, loss: 0.065300, train acc: 0.950984, 
Test 84, loss: 0.085169, test acc: 0.940735,
Train 85, loss: 0.079515, train acc: 0.943863, 
Test 85, loss: 0.085399, test acc: 0.951586,
Train 86, loss: 0.075624, train acc: 0.950984, 
Test 86, loss: 0.083209, test acc: 0.941569,
Train 87, loss: 0.079409, train acc: 0.935065, 
Test 87, loss: 0.088985, test acc: 0.947412,
Train 88, loss: 0.067301, train acc: 0.947633, 
Test 88, loss: 0.082565, test acc: 0.959098,
Max Acc:0.959098
Train 89, loss: 0.069022, train acc: 0.953498, 
Test 89, loss: 0.113905, test acc: 0.954925,
Train 90, loss: 0.082720, train acc: 0.938416, 
Test 90, loss: 0.076725, test acc: 0.956594,
Train 91, loss: 0.073266, train acc: 0.945119, 
Test 91, loss: 0.069970, test acc: 0.964942,
Max Acc:0.964942
Train 92, loss: 0.068299, train acc: 0.953079, 
Test 92, loss: 0.080414, test acc: 0.941569,
Train 93, loss: 0.074000, train acc: 0.946795, 
Test 93, loss: 0.083833, test acc: 0.941569,
Train 94, loss: 0.069895, train acc: 0.948890, 
Test 94, loss: 0.088066, test acc: 0.948247,
Train 95, loss: 0.074783, train acc: 0.946795, 
Test 95, loss: 0.078283, test acc: 0.958264,
Train 96, loss: 0.073528, train acc: 0.948890, 
Test 96, loss: 0.075857, test acc: 0.940735,
Train 97, loss: 0.070089, train acc: 0.956431, 
Test 97, loss: 0.090435, test acc: 0.968280,
Max Acc:0.968280
Train 98, loss: 0.066659, train acc: 0.959363, 
Test 98, loss: 0.087427, test acc: 0.966611,
Train 99, loss: 0.069883, train acc: 0.944282, 
Test 99, loss: 0.084391, test acc: 0.940735,
Train 100, loss: 0.066897, train acc: 0.952241, 
Test 100, loss: 0.081521, test acc: 0.951586,
Train 101, loss: 0.066268, train acc: 0.953917, 
Test 101, loss: 0.077260, test acc: 0.949082,
Train 102, loss: 0.087156, train acc: 0.939254, 
Test 102, loss: 0.078981, test acc: 0.968280,
Max Acc:0.968280
Train 103, loss: 0.075368, train acc: 0.946795, 
Test 103, loss: 0.086467, test acc: 0.953255,
Train 104, loss: 0.074317, train acc: 0.946795, 
Test 104, loss: 0.083106, test acc: 0.934891,
Train 105, loss: 0.067428, train acc: 0.954755, 
Test 105, loss: 0.086336, test acc: 0.940735,
Train 106, loss: 0.068911, train acc: 0.956012, 
Test 106, loss: 0.112757, test acc: 0.959098,
Train 107, loss: 0.076175, train acc: 0.946795, 
Test 107, loss: 0.085199, test acc: 0.941569,
Train 108, loss: 0.072385, train acc: 0.953498, 
Test 108, loss: 0.074491, test acc: 0.953255,
Train 109, loss: 0.074610, train acc: 0.948052, 
Test 109, loss: 0.092511, test acc: 0.952421,
Train 110, loss: 0.065187, train acc: 0.948890, 
Test 110, loss: 0.078610, test acc: 0.940735,
Train 111, loss: 0.063090, train acc: 0.957269, 
Test 111, loss: 0.089385, test acc: 0.952421,
Train 112, loss: 0.061840, train acc: 0.957269, 
Test 112, loss: 0.082526, test acc: 0.941569,
Train 113, loss: 0.082630, train acc: 0.945957, 
Test 113, loss: 0.095125, test acc: 0.923205,
Train 114, loss: 0.071541, train acc: 0.947214, 
Test 114, loss: 0.076617, test acc: 0.940735,
Train 115, loss: 0.072297, train acc: 0.949309, 
Test 115, loss: 0.075190, test acc: 0.959933,
Train 116, loss: 0.071645, train acc: 0.950566, 
Test 116, loss: 0.085229, test acc: 0.952421,
Train 117, loss: 0.068960, train acc: 0.947214, 
Test 117, loss: 0.090746, test acc: 0.944908,
Train 118, loss: 0.072392, train acc: 0.954755, 
Test 118, loss: 0.082335, test acc: 0.948247,
Train 119, loss: 0.066888, train acc: 0.959782, 
Test 119, loss: 0.084905, test acc: 0.943239,
Train 120, loss: 0.068899, train acc: 0.953079, 
Test 120, loss: 0.083643, test acc: 0.953255,
Train 121, loss: 0.064228, train acc: 0.954755, 
Test 121, loss: 0.101647, test acc: 0.938230,
Train 122, loss: 0.065637, train acc: 0.952241, 
Test 122, loss: 0.084095, test acc: 0.960768,
Train 123, loss: 0.059627, train acc: 0.960201, 
Test 123, loss: 0.084821, test acc: 0.964942,
Train 124, loss: 0.073028, train acc: 0.951403, 
Test 124, loss: 0.105054, test acc: 0.925710,
Train 125, loss: 0.070713, train acc: 0.952660, 
Test 125, loss: 0.075772, test acc: 0.940735,
Train 126, loss: 0.063221, train acc: 0.958525, 
Test 126, loss: 0.081463, test acc: 0.959098,
Train 127, loss: 0.058632, train acc: 0.964390, 
Test 127, loss: 0.076128, test acc: 0.941569,
Train 128, loss: 0.063560, train acc: 0.954755, 
Test 128, loss: 0.084546, test acc: 0.933222,
Train 129, loss: 0.057198, train acc: 0.963134, 
Test 129, loss: 0.071894, test acc: 0.960768,
Train 130, loss: 0.059252, train acc: 0.964390, 
Test 130, loss: 0.088411, test acc: 0.952421,
Train 131, loss: 0.063718, train acc: 0.959782, 
Test 131, loss: 0.120064, test acc: 0.909015,
Train 132, loss: 0.060460, train acc: 0.957687, 
Test 132, loss: 0.079460, test acc: 0.959098,
Train 133, loss: 0.067252, train acc: 0.959363, 
Test 133, loss: 0.080616, test acc: 0.965776,
Train 134, loss: 0.067948, train acc: 0.951822, 
Test 134, loss: 0.070759, test acc: 0.958264,
Train 135, loss: 0.063186, train acc: 0.962296, 
Test 135, loss: 0.071773, test acc: 0.941569,
Train 136, loss: 0.063759, train acc: 0.954336, 
Test 136, loss: 0.071884, test acc: 0.940735,
Train 137, loss: 0.068274, train acc: 0.955593, 
Test 137, loss: 0.071113, test acc: 0.958264,
Train 138, loss: 0.059716, train acc: 0.963972, 
Test 138, loss: 0.081486, test acc: 0.951586,
Train 139, loss: 0.058952, train acc: 0.964809, 
Test 139, loss: 0.063206, test acc: 0.959098,
Train 140, loss: 0.076260, train acc: 0.946376, 
Test 140, loss: 0.089422, test acc: 0.941569,
Train 141, loss: 0.071943, train acc: 0.950147, 
Test 141, loss: 0.076509, test acc: 0.948247,
Train 142, loss: 0.060062, train acc: 0.956850, 
Test 142, loss: 0.071864, test acc: 0.969950,
Max Acc:0.969950
Train 143, loss: 0.061385, train acc: 0.963553, 
Test 143, loss: 0.092685, test acc: 0.953255,
Train 144, loss: 0.056410, train acc: 0.964390, 
Test 144, loss: 0.153945, test acc: 0.945743,
Train 145, loss: 0.059919, train acc: 0.962715, 
Test 145, loss: 0.076132, test acc: 0.959098,
Train 146, loss: 0.064579, train acc: 0.958944, 
Test 146, loss: 0.075671, test acc: 0.959098,
Train 147, loss: 0.063494, train acc: 0.958525, 
Test 147, loss: 0.070633, test acc: 0.967446,
Train 148, loss: 0.061804, train acc: 0.959782, 
Test 148, loss: 0.094468, test acc: 0.962437,
Train 149, loss: 0.053124, train acc: 0.968999, 
Test 149, loss: 0.070321, test acc: 0.958264,
Train 150, loss: 0.058080, train acc: 0.961039, 
Test 150, loss: 0.063765, test acc: 0.959098,
Train 151, loss: 0.055259, train acc: 0.966066, 
Test 151, loss: 0.067848, test acc: 0.966611,
Train 152, loss: 0.054974, train acc: 0.962296, 
Test 152, loss: 0.061582, test acc: 0.959098,
Train 153, loss: 0.056563, train acc: 0.963553, 
Test 153, loss: 0.091317, test acc: 0.929048,
Train 154, loss: 0.058830, train acc: 0.962296, 
Test 154, loss: 0.072095, test acc: 0.958264,
Train 155, loss: 0.056900, train acc: 0.961458, 
Test 155, loss: 0.075661, test acc: 0.941569,
Train 156, loss: 0.056856, train acc: 0.966066, 
Test 156, loss: 0.075669, test acc: 0.959098,
Train 157, loss: 0.050280, train acc: 0.971512, 
Test 157, loss: 0.080408, test acc: 0.970785,
Max Acc:0.970785
Train 158, loss: 0.054808, train acc: 0.967323, 
Test 158, loss: 0.098154, test acc: 0.963272,
Train 159, loss: 0.051739, train acc: 0.968580, 
Test 159, loss: 0.089101, test acc: 0.952421,
Train 160, loss: 0.057359, train acc: 0.965228, 
Test 160, loss: 0.074029, test acc: 0.941569,
Train 161, loss: 0.059294, train acc: 0.962715, 
Test 161, loss: 0.082267, test acc: 0.947412,
Train 162, loss: 0.054550, train acc: 0.966066, 
Test 162, loss: 0.073685, test acc: 0.941569,
Train 163, loss: 0.058726, train acc: 0.960620, 
Test 163, loss: 0.059802, test acc: 0.969950,
Train 164, loss: 0.055769, train acc: 0.966904, 
Test 164, loss: 0.057291, test acc: 0.958264,
Train 165, loss: 0.054593, train acc: 0.968580, 
Test 165, loss: 0.069972, test acc: 0.959098,
Train 166, loss: 0.063110, train acc: 0.962296, 
Test 166, loss: 0.085010, test acc: 0.952421,
Train 167, loss: 0.055551, train acc: 0.969837, 
Test 167, loss: 0.074403, test acc: 0.969115,
Train 168, loss: 0.054235, train acc: 0.967323, 
Test 168, loss: 0.076628, test acc: 0.969950,
Train 169, loss: 0.055233, train acc: 0.966485, 
Test 169, loss: 0.068696, test acc: 0.958264,
Train 170, loss: 0.054588, train acc: 0.965647, 
Test 170, loss: 0.078414, test acc: 0.949082,
Train 171, loss: 0.050731, train acc: 0.969837, 
Test 171, loss: 0.086001, test acc: 0.941569,
Train 172, loss: 0.056237, train acc: 0.963553, 
Test 172, loss: 0.069298, test acc: 0.958264,
Train 173, loss: 0.060374, train acc: 0.959363, 
Test 173, loss: 0.061495, test acc: 0.969115,
Train 174, loss: 0.060622, train acc: 0.960201, 
Test 174, loss: 0.072697, test acc: 0.934057,
Train 175, loss: 0.065419, train acc: 0.958106, 
Test 175, loss: 0.080497, test acc: 0.924040,
Train 176, loss: 0.062605, train acc: 0.958106, 
Test 176, loss: 0.074248, test acc: 0.933222,
Train 177, loss: 0.060835, train acc: 0.963134, 
Test 177, loss: 0.066525, test acc: 0.959098,
Train 178, loss: 0.055641, train acc: 0.966904, 
Test 178, loss: 0.075630, test acc: 0.959098,
Train 179, loss: 0.065732, train acc: 0.957269, 
Test 179, loss: 0.076874, test acc: 0.944073,
Train 180, loss: 0.058024, train acc: 0.956431, 
Test 180, loss: 0.071548, test acc: 0.959098,
Train 181, loss: 0.059690, train acc: 0.961458, 
Test 181, loss: 0.070992, test acc: 0.958264,
Train 182, loss: 0.060785, train acc: 0.963553, 
Test 182, loss: 0.083567, test acc: 0.959098,
Train 183, loss: 0.059967, train acc: 0.965228, 
Test 183, loss: 0.076303, test acc: 0.949082,
Train 184, loss: 0.052569, train acc: 0.969837, 
Test 184, loss: 0.075772, test acc: 0.969950,
Train 185, loss: 0.049880, train acc: 0.968580, 
Test 185, loss: 0.075229, test acc: 0.959098,
Train 186, loss: 0.057500, train acc: 0.967742, 
Test 186, loss: 0.069577, test acc: 0.952421,
Train 187, loss: 0.051492, train acc: 0.969837, 
Test 187, loss: 0.067968, test acc: 0.969950,
Train 188, loss: 0.056262, train acc: 0.963553, 
Test 188, loss: 0.067226, test acc: 0.969950,
Train 189, loss: 0.054837, train acc: 0.964390, 
Test 189, loss: 0.065595, test acc: 0.959098,
Train 190, loss: 0.049957, train acc: 0.971931, 
Test 190, loss: 0.071580, test acc: 0.969950,
Train 191, loss: 0.051431, train acc: 0.966066, 
Test 191, loss: 0.071843, test acc: 0.947412,
Train 192, loss: 0.053312, train acc: 0.961458, 
Test 192, loss: 0.078435, test acc: 0.970785,
Max Acc:0.970785
Train 193, loss: 0.047700, train acc: 0.967323, 
Test 193, loss: 0.065567, test acc: 0.963272,
Train 194, loss: 0.055027, train acc: 0.966485, 
Test 194, loss: 0.062743, test acc: 0.958264,
Train 195, loss: 0.049732, train acc: 0.968999, 
Test 195, loss: 0.079658, test acc: 0.959098,
Train 196, loss: 0.048394, train acc: 0.969837, 
Test 196, loss: 0.064875, test acc: 0.959098,
Train 197, loss: 0.049370, train acc: 0.971931, 
Test 197, loss: 0.106393, test acc: 0.962437,
Train 198, loss: 0.050536, train acc: 0.974445, 
Test 198, loss: 0.077331, test acc: 0.969115,
Train 199, loss: 0.052480, train acc: 0.967742, 
Test 199, loss: 0.083624, test acc: 0.970785,
Max Acc:0.970785
Train 200, loss: 0.040775, train acc: 0.979472, 
Test 200, loss: 0.085945, test acc: 0.969950,
Train 201, loss: 0.059881, train acc: 0.958944, 
Test 201, loss: 0.069201, test acc: 0.959098,
Train 202, loss: 0.055079, train acc: 0.961458, 
Test 202, loss: 0.091159, test acc: 0.953255,
Train 203, loss: 0.053081, train acc: 0.967323, 
Test 203, loss: 0.069959, test acc: 0.969115,
Train 204, loss: 0.052410, train acc: 0.970256, 
Test 204, loss: 0.087668, test acc: 0.959098,
Train 205, loss: 0.053633, train acc: 0.969418, 
Test 205, loss: 0.082075, test acc: 0.959098,
Train 206, loss: 0.051253, train acc: 0.969837, 
Test 206, loss: 0.083126, test acc: 0.968280,
Train 207, loss: 0.054972, train acc: 0.966904, 
Test 207, loss: 0.074703, test acc: 0.952421,
Train 208, loss: 0.049202, train acc: 0.973607, 
Test 208, loss: 0.115880, test acc: 0.939900,
Train 209, loss: 0.052326, train acc: 0.968580, 
Test 209, loss: 0.074156, test acc: 0.969950,
Train 210, loss: 0.052956, train acc: 0.970674, 
Test 210, loss: 0.069633, test acc: 0.952421,
Train 211, loss: 0.052833, train acc: 0.966904, 
Test 211, loss: 0.074521, test acc: 0.959098,
Train 212, loss: 0.048421, train acc: 0.973607, 
Test 212, loss: 0.104172, test acc: 0.914858,
Train 213, loss: 0.048084, train acc: 0.970674, 
Test 213, loss: 0.064408, test acc: 0.969950,
Train 214, loss: 0.049725, train acc: 0.970256, 
Test 214, loss: 0.069553, test acc: 0.969950,
Train 215, loss: 0.047042, train acc: 0.976121, 
Test 215, loss: 0.074792, test acc: 0.970785,
Max Acc:0.970785
Train 216, loss: 0.051143, train acc: 0.971512, 
Test 216, loss: 0.071730, test acc: 0.969950,
Train 217, loss: 0.049699, train acc: 0.969837, 
Test 217, loss: 0.063897, test acc: 0.959098,
Train 218, loss: 0.045416, train acc: 0.972769, 
Test 218, loss: 0.070056, test acc: 0.969950,
Train 219, loss: 0.047809, train acc: 0.970674, 
Test 219, loss: 0.062597, test acc: 0.958264,
Train 220, loss: 0.051434, train acc: 0.968161, 
Test 220, loss: 0.065835, test acc: 0.969950,
Train 221, loss: 0.040640, train acc: 0.976959, 
Test 221, loss: 0.064617, test acc: 0.969115,
Train 222, loss: 0.042401, train acc: 0.974445, 
Test 222, loss: 0.076619, test acc: 0.969950,
Train 223, loss: 0.045877, train acc: 0.975702, 
Test 223, loss: 0.075888, test acc: 0.970785,
Max Acc:0.970785
Train 224, loss: 0.043245, train acc: 0.974445, 
Test 224, loss: 0.092305, test acc: 0.951586,
Train 225, loss: 0.060950, train acc: 0.963553, 
Test 225, loss: 0.085828, test acc: 0.952421,
Train 226, loss: 0.045662, train acc: 0.976121, 
Test 226, loss: 0.092559, test acc: 0.970785,
Max Acc:0.970785
Train 227, loss: 0.048710, train acc: 0.970256, 
Test 227, loss: 0.083451, test acc: 0.957429,
Train 228, loss: 0.044258, train acc: 0.973188, 
Test 228, loss: 0.061878, test acc: 0.969950,
Train 229, loss: 0.047361, train acc: 0.973607, 
Test 229, loss: 0.075121, test acc: 0.969950,
Train 230, loss: 0.052544, train acc: 0.963553, 
Test 230, loss: 0.076582, test acc: 0.969115,
Train 231, loss: 0.044011, train acc: 0.973188, 
Test 231, loss: 0.076094, test acc: 0.959098,
Train 232, loss: 0.045779, train acc: 0.971931, 
Test 232, loss: 0.066622, test acc: 0.969950,
Train 233, loss: 0.045302, train acc: 0.974026, 
Test 233, loss: 0.072617, test acc: 0.969950,
Train 234, loss: 0.042025, train acc: 0.981567, 
Test 234, loss: 0.080603, test acc: 0.970785,
Max Acc:0.970785
Train 235, loss: 0.059254, train acc: 0.959363, 
Test 235, loss: 0.081972, test acc: 0.934891,
Train 236, loss: 0.057710, train acc: 0.960201, 
Test 236, loss: 0.070460, test acc: 0.966611,
Train 237, loss: 0.044047, train acc: 0.976540, 
Test 237, loss: 0.068302, test acc: 0.959098,
Train 238, loss: 0.041221, train acc: 0.976959, 
Test 238, loss: 0.081812, test acc: 0.948247,
Train 239, loss: 0.041889, train acc: 0.976121, 
Test 239, loss: 0.068536, test acc: 0.969115,
Train 240, loss: 0.038798, train acc: 0.978215, 
Test 240, loss: 0.074629, test acc: 0.959933,
Train 241, loss: 0.045356, train acc: 0.974026, 
Test 241, loss: 0.065079, test acc: 0.961603,
Train 242, loss: 0.042304, train acc: 0.973607, 
Test 242, loss: 0.069446, test acc: 0.954925,
Train 243, loss: 0.044675, train acc: 0.976959, 
Test 243, loss: 0.071336, test acc: 0.969950,
Train 244, loss: 0.038341, train acc: 0.979472, 
Test 244, loss: 0.080369, test acc: 0.956594,
Train 245, loss: 0.040874, train acc: 0.979472, 
Test 245, loss: 0.070729, test acc: 0.970785,
Max Acc:0.970785
Train 246, loss: 0.037672, train acc: 0.980310, 
Test 246, loss: 0.071514, test acc: 0.969950,
Train 247, loss: 0.046968, train acc: 0.971931, 
Test 247, loss: 0.059542, test acc: 0.958264,
Train 248, loss: 0.041228, train acc: 0.975702, 
Test 248, loss: 0.058630, test acc: 0.959933,
Train 249, loss: 0.044463, train acc: 0.974026, 
Test 249, loss: 0.067828, test acc: 0.969950,
Train 250, loss: 0.039060, train acc: 0.979053, 
Test 250, loss: 0.065476, test acc: 0.969950,
Train 251, loss: 0.038657, train acc: 0.979053, 
Test 251, loss: 0.064080, test acc: 0.958264,
Train 252, loss: 0.034305, train acc: 0.981986, 
Test 252, loss: 0.074243, test acc: 0.970785,
Max Acc:0.970785
Train 253, loss: 0.049131, train acc: 0.967742, 
Test 253, loss: 0.066638, test acc: 0.969950,
Train 254, loss: 0.035797, train acc: 0.978634, 
Test 254, loss: 0.092396, test acc: 0.969950,
Train 255, loss: 0.039726, train acc: 0.978215, 
Test 255, loss: 0.077768, test acc: 0.970785,
Max Acc:0.970785
Train 256, loss: 0.034209, train acc: 0.980729, 
Test 256, loss: 0.069766, test acc: 0.969950,
Train 257, loss: 0.040576, train acc: 0.976540, 
Test 257, loss: 0.065744, test acc: 0.969115,
Train 258, loss: 0.036531, train acc: 0.976959, 
Test 258, loss: 0.066647, test acc: 0.969950,
Train 259, loss: 0.043872, train acc: 0.977377, 
Test 259, loss: 0.067643, test acc: 0.970785,
Max Acc:0.970785
Train 260, loss: 0.042955, train acc: 0.976959, 
Test 260, loss: 0.074625, test acc: 0.970785,
Max Acc:0.970785
Train 261, loss: 0.036880, train acc: 0.976540, 
Test 261, loss: 0.071107, test acc: 0.970785,
Max Acc:0.970785
Train 262, loss: 0.041925, train acc: 0.975702, 
Test 262, loss: 0.073248, test acc: 0.970785,
Max Acc:0.970785
Train 263, loss: 0.038082, train acc: 0.979891, 
Test 263, loss: 0.077338, test acc: 0.959098,
Train 264, loss: 0.036285, train acc: 0.981148, 
Test 264, loss: 0.073088, test acc: 0.970785,
Max Acc:0.970785
Train 265, loss: 0.038360, train acc: 0.978215, 
Test 265, loss: 0.068670, test acc: 0.959098,
Train 266, loss: 0.033593, train acc: 0.981986, 
Test 266, loss: 0.077851, test acc: 0.969950,
Train 267, loss: 0.040448, train acc: 0.974864, 
Test 267, loss: 0.085965, test acc: 0.952421,
Train 268, loss: 0.046029, train acc: 0.974445, 
Test 268, loss: 0.072973, test acc: 0.969950,
Train 269, loss: 0.034912, train acc: 0.981567, 
Test 269, loss: 0.078506, test acc: 0.970785,
Max Acc:0.970785
Train 270, loss: 0.037778, train acc: 0.977377, 
Test 270, loss: 0.078113, test acc: 0.969950,
Train 271, loss: 0.038423, train acc: 0.979053, 
Test 271, loss: 0.077360, test acc: 0.969950,
Train 272, loss: 0.034651, train acc: 0.980729, 
Test 272, loss: 0.077944, test acc: 0.970785,
Max Acc:0.970785
Train 273, loss: 0.037309, train acc: 0.978215, 
Test 273, loss: 0.081209, test acc: 0.970785,
Max Acc:0.970785
Train 274, loss: 0.035156, train acc: 0.979472, 
Test 274, loss: 0.076020, test acc: 0.970785,
Max Acc:0.970785
Train 275, loss: 0.033534, train acc: 0.979472, 
Test 275, loss: 0.076724, test acc: 0.969950,
Train 276, loss: 0.034724, train acc: 0.982824, 
Test 276, loss: 0.064116, test acc: 0.959098,
Train 277, loss: 0.035489, train acc: 0.981567, 
Test 277, loss: 0.071420, test acc: 0.969950,
Train 278, loss: 0.032242, train acc: 0.981148, 
Test 278, loss: 0.071705, test acc: 0.969115,
Train 279, loss: 0.035999, train acc: 0.978634, 
Test 279, loss: 0.063223, test acc: 0.959098,
Train 280, loss: 0.043453, train acc: 0.975702, 
Test 280, loss: 0.072652, test acc: 0.969950,
Train 281, loss: 0.034515, train acc: 0.979891, 
Test 281, loss: 0.075591, test acc: 0.970785,
Max Acc:0.970785
Train 282, loss: 0.031511, train acc: 0.981148, 
Test 282, loss: 0.071130, test acc: 0.970785,
Max Acc:0.970785
Train 283, loss: 0.032242, train acc: 0.983661, 
Test 283, loss: 0.064357, test acc: 0.969950,
Train 284, loss: 0.031373, train acc: 0.980310, 
Test 284, loss: 0.071080, test acc: 0.969950,
Train 285, loss: 0.030158, train acc: 0.983661, 
Test 285, loss: 0.076059, test acc: 0.970785,
Max Acc:0.970785
Train 286, loss: 0.034362, train acc: 0.979472, 
Test 286, loss: 0.089552, test acc: 0.952421,
Train 287, loss: 0.032827, train acc: 0.980729, 
Test 287, loss: 0.081255, test acc: 0.954090,
Train 288, loss: 0.034319, train acc: 0.979891, 
Test 288, loss: 0.084582, test acc: 0.959933,
Train 289, loss: 0.030652, train acc: 0.982824, 
Test 289, loss: 0.081769, test acc: 0.969115,
Train 290, loss: 0.028838, train acc: 0.981986, 
Test 290, loss: 0.066050, test acc: 0.968280,
Train 291, loss: 0.030418, train acc: 0.982405, 
Test 291, loss: 0.076707, test acc: 0.965776,
Train 292, loss: 0.026320, train acc: 0.984080, 
Test 292, loss: 0.150008, test acc: 0.930718,
Train 293, loss: 0.030121, train acc: 0.984080, 
Test 293, loss: 0.082141, test acc: 0.970785,
Max Acc:0.970785
Train 294, loss: 0.037466, train acc: 0.979053, 
Test 294, loss: 0.072577, test acc: 0.969115,
Train 295, loss: 0.032654, train acc: 0.984080, 
Test 295, loss: 0.081230, test acc: 0.970785,
Max Acc:0.970785
Train 296, loss: 0.027913, train acc: 0.984499, 
Test 296, loss: 0.076752, test acc: 0.970785,
Max Acc:0.970785
Train 297, loss: 0.030924, train acc: 0.981567, 
Test 297, loss: 0.070824, test acc: 0.969950,
Train 298, loss: 0.028982, train acc: 0.984080, 
Test 298, loss: 0.082011, test acc: 0.971619,
Max Acc:0.971619
Train 299, loss: 0.026552, train acc: 0.983243, 
Test 299, loss: 0.089306, test acc: 0.968280,
Train 300, loss: 0.028857, train acc: 0.983243, 
Test 300, loss: 0.074310, test acc: 0.969950,
Train 301, loss: 0.030433, train acc: 0.981567, 
Test 301, loss: 0.078716, test acc: 0.968280,
Train 302, loss: 0.028223, train acc: 0.982824, 
Test 302, loss: 0.078313, test acc: 0.969115,
Train 303, loss: 0.026998, train acc: 0.984080, 
Test 303, loss: 0.079534, test acc: 0.970785,
Train 304, loss: 0.028324, train acc: 0.984080, 
Test 304, loss: 0.078086, test acc: 0.971619,
Max Acc:0.971619
Train 305, loss: 0.029091, train acc: 0.983243, 
Test 305, loss: 0.072733, test acc: 0.968280,
Train 306, loss: 0.026841, train acc: 0.983661, 
Test 306, loss: 0.069441, test acc: 0.970785,
Train 307, loss: 0.028521, train acc: 0.984499, 
Test 307, loss: 0.096977, test acc: 0.959098,
Train 308, loss: 0.030024, train acc: 0.983661, 
Test 308, loss: 0.070570, test acc: 0.969115,
Train 309, loss: 0.026464, train acc: 0.985756, 
Test 309, loss: 0.077372, test acc: 0.970785,
Train 310, loss: 0.025353, train acc: 0.984499, 
Test 310, loss: 0.080267, test acc: 0.971619,
Max Acc:0.971619
Train 311, loss: 0.022621, train acc: 0.987013, 
Test 311, loss: 0.079967, test acc: 0.970785,
Train 312, loss: 0.028493, train acc: 0.985756, 
Test 312, loss: 0.096931, test acc: 0.958264,
Train 313, loss: 0.024918, train acc: 0.984080, 
Test 313, loss: 0.104074, test acc: 0.951586,
Train 314, loss: 0.023317, train acc: 0.988270, 
Test 314, loss: 0.097568, test acc: 0.960768,
Train 315, loss: 0.030933, train acc: 0.982824, 
Test 315, loss: 0.065616, test acc: 0.970785,
Train 316, loss: 0.027344, train acc: 0.984080, 
Test 316, loss: 0.076244, test acc: 0.969115,
Train 317, loss: 0.026346, train acc: 0.983243, 
Test 317, loss: 0.085745, test acc: 0.969950,
Train 318, loss: 0.027315, train acc: 0.985756, 
Test 318, loss: 0.085502, test acc: 0.967446,
Train 319, loss: 0.022908, train acc: 0.987432, 
Test 319, loss: 0.083342, test acc: 0.971619,
Max Acc:0.971619
Train 320, loss: 0.031450, train acc: 0.983243, 
Test 320, loss: 0.078501, test acc: 0.970785,
Train 321, loss: 0.025624, train acc: 0.986594, 
Test 321, loss: 0.083182, test acc: 0.965776,
Train 322, loss: 0.024089, train acc: 0.986594, 
Test 322, loss: 0.082648, test acc: 0.970785,
Train 323, loss: 0.026053, train acc: 0.981986, 
Test 323, loss: 0.083139, test acc: 0.964107,
Train 324, loss: 0.024127, train acc: 0.985756, 
Test 324, loss: 0.084027, test acc: 0.965776,
Train 325, loss: 0.023190, train acc: 0.985337, 
Test 325, loss: 0.079783, test acc: 0.969115,
Train 326, loss: 0.028778, train acc: 0.983243, 
Test 326, loss: 0.078423, test acc: 0.969950,
Train 327, loss: 0.026082, train acc: 0.987432, 
Test 327, loss: 0.075667, test acc: 0.970785,
Train 328, loss: 0.024990, train acc: 0.987013, 
Test 328, loss: 0.071524, test acc: 0.969115,
Train 329, loss: 0.024455, train acc: 0.985756, 
Test 329, loss: 0.135320, test acc: 0.939900,
Train 330, loss: 0.023756, train acc: 0.986594, 
Test 330, loss: 0.100613, test acc: 0.954090,
Train 331, loss: 0.024104, train acc: 0.985756, 
Test 331, loss: 0.080827, test acc: 0.969115,
Train 332, loss: 0.023459, train acc: 0.987013, 
Test 332, loss: 0.092648, test acc: 0.958264,
Train 333, loss: 0.022918, train acc: 0.986594, 
Test 333, loss: 0.084234, test acc: 0.969115,
Train 334, loss: 0.025157, train acc: 0.986594, 
Test 334, loss: 0.079119, test acc: 0.970785,
Train 335, loss: 0.021703, train acc: 0.988270, 
Test 335, loss: 0.080945, test acc: 0.969115,
Train 336, loss: 0.020184, train acc: 0.988270, 
Test 336, loss: 0.088108, test acc: 0.969115,
Train 337, loss: 0.023095, train acc: 0.986175, 
Test 337, loss: 0.095687, test acc: 0.962437,
Train 338, loss: 0.019913, train acc: 0.989527, 
Test 338, loss: 0.105642, test acc: 0.961603,
Train 339, loss: 0.023040, train acc: 0.986594, 
Test 339, loss: 0.094687, test acc: 0.964107,
Train 340, loss: 0.023197, train acc: 0.986594, 
Test 340, loss: 0.098647, test acc: 0.962437,
Train 341, loss: 0.024453, train acc: 0.985756, 
Test 341, loss: 0.091850, test acc: 0.965776,
Train 342, loss: 0.026984, train acc: 0.984499, 
Test 342, loss: 0.116344, test acc: 0.949082,
Train 343, loss: 0.020342, train acc: 0.987432, 
Test 343, loss: 0.089419, test acc: 0.964107,
Train 344, loss: 0.023329, train acc: 0.986594, 
Test 344, loss: 0.125908, test acc: 0.943239,
Train 345, loss: 0.023515, train acc: 0.985756, 
Test 345, loss: 0.093564, test acc: 0.964107,
Train 346, loss: 0.022197, train acc: 0.987432, 
Test 346, loss: 0.092753, test acc: 0.964942,
Train 347, loss: 0.023383, train acc: 0.986594, 
Test 347, loss: 0.082537, test acc: 0.971619,
Max Acc:0.971619
Train 348, loss: 0.021775, train acc: 0.987013, 
Test 348, loss: 0.083376, test acc: 0.968280,
Train 349, loss: 0.023450, train acc: 0.985337, 
Test 349, loss: 0.085019, test acc: 0.968280,
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 350
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 150
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.385573, train acc: 0.744533, 
Test 0, loss: 0.306329, test acc: 0.713230,
Max Acc:0.713230
Train 1, loss: 0.270371, train acc: 0.747638, 
Test 1, loss: 0.300275, test acc: 0.713366,
Max Acc:0.713366
Train 2, loss: 0.271334, train acc: 0.744032, 
Test 2, loss: 0.306203, test acc: 0.706317,
Train 3, loss: 0.270999, train acc: 0.746937, 
Test 3, loss: 0.313991, test acc: 0.715060,
Max Acc:0.715060
Train 4, loss: 0.269736, train acc: 0.744500, 
Test 4, loss: 0.322283, test acc: 0.713434,
Train 5, loss: 0.267060, train acc: 0.747171, 
Test 5, loss: 0.322139, test acc: 0.713434,
Train 6, loss: 0.260873, train acc: 0.748005, 
Test 6, loss: 0.354688, test acc: 0.719466,
Max Acc:0.719466
Train 7, loss: 0.261078, train acc: 0.748473, 
Test 7, loss: 0.297293, test acc: 0.713434,
Train 8, loss: 0.259149, train acc: 0.748907, 
Test 8, loss: 0.321232, test acc: 0.705910,
Train 9, loss: 0.261581, train acc: 0.747137, 
Test 9, loss: 0.299839, test acc: 0.713434,
Train 10, loss: 0.264242, train acc: 0.748673, 
Test 10, loss: 0.324085, test acc: 0.713434,
Train 11, loss: 0.263461, train acc: 0.750376, 
Test 11, loss: 0.374435, test acc: 0.597668,
Train 12, loss: 0.262774, train acc: 0.748706, 
Test 12, loss: 0.300957, test acc: 0.713434,
Train 13, loss: 0.269416, train acc: 0.750576, 
Test 13, loss: 0.311822, test acc: 0.713434,
Train 14, loss: 0.269998, train acc: 0.751644, 
Test 14, loss: 0.311029, test acc: 0.713434,
Train 15, loss: 0.272006, train acc: 0.751711, 
Test 15, loss: 0.305148, test acc: 0.713434,
Train 16, loss: 0.273577, train acc: 0.751978, 
Test 16, loss: 0.304641, test acc: 0.713434,
Train 17, loss: 0.277188, train acc: 0.752178, 
Test 17, loss: 0.297398, test acc: 0.713434,
Train 18, loss: 0.274423, train acc: 0.750275, 
Test 18, loss: 0.315569, test acc: 0.663549,
Train 19, loss: 0.271895, train acc: 0.751043, 
Test 19, loss: 0.301459, test acc: 0.712146,
Train 20, loss: 0.272922, train acc: 0.750476, 
Test 20, loss: 0.295143, test acc: 0.713908,
Train 21, loss: 0.279302, train acc: 0.749942, 
Test 21, loss: 0.298806, test acc: 0.713434,
Train 22, loss: 0.278787, train acc: 0.748606, 
Test 22, loss: 0.303943, test acc: 0.713434,
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 150
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.2
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 150
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.2
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 150
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.2
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.442520, train acc: 0.747137, 
Test 0, loss: 0.333526, test acc: 0.700895,
Max Acc:0.700895
Train 1, loss: 0.289945, train acc: 0.750309, 
Test 1, loss: 0.306093, test acc: 0.717365,
Max Acc:0.717365
Train 2, loss: 0.291383, train acc: 0.750042, 
Test 2, loss: 0.310524, test acc: 0.716077,
Train 3, loss: 0.290417, train acc: 0.749741, 
Test 3, loss: 0.301229, test acc: 0.713434,
Train 4, loss: 0.292287, train acc: 0.747137, 
Test 4, loss: 0.313539, test acc: 0.713434,
Train 5, loss: 0.289149, train acc: 0.751878, 
Test 5, loss: 0.309079, test acc: 0.713434,
Train 6, loss: 0.290334, train acc: 0.750643, 
Test 6, loss: 0.302569, test acc: 0.713434,
Train 7, loss: 0.290142, train acc: 0.750509, 
Test 7, loss: 0.301482, test acc: 0.713434,
Train 8, loss: 0.288588, train acc: 0.752279, 
Test 8, loss: 0.312349, test acc: 0.713434,
Train 9, loss: 0.290572, train acc: 0.749741, 
Test 9, loss: 0.313380, test acc: 0.713434,
Train 10, loss: 0.290246, train acc: 0.749674, 
Test 10, loss: 0.302650, test acc: 0.713434,
Train 11, loss: 0.291274, train acc: 0.747571, 
Test 11, loss: 0.303001, test acc: 0.713434,
Train 12, loss: 0.289855, train acc: 0.750309, 
Test 12, loss: 0.302102, test acc: 0.713434,
Train 13, loss: 0.290490, train acc: 0.750810, 
Test 13, loss: 0.328068, test acc: 0.713434,
Train 14, loss: 0.289859, train acc: 0.751778, 
Test 14, loss: 0.304792, test acc: 0.713434,
Train 15, loss: 0.290759, train acc: 0.749508, 
Test 15, loss: 0.327637, test acc: 0.713434,
Train 16, loss: 0.290260, train acc: 0.752813, 
Test 16, loss: 0.321213, test acc: 0.713434,
Train 17, loss: 0.289781, train acc: 0.749474, 
Test 17, loss: 0.320167, test acc: 0.713434,
Train 18, loss: 0.291085, train acc: 0.748306, 
Test 18, loss: 0.301490, test acc: 0.713434,
Train 19, loss: 0.290360, train acc: 0.752112, 
Test 19, loss: 0.307365, test acc: 0.713434,
Train 20, loss: 0.288763, train acc: 0.752445, 
Test 20, loss: 0.301532, test acc: 0.713434,
Train 21, loss: 0.289676, train acc: 0.750910, 
Test 21, loss: 0.301412, test acc: 0.713434,
Train 22, loss: 0.289727, train acc: 0.751477, 
Test 22, loss: 0.330643, test acc: 0.713434,
Train 23, loss: 0.290290, train acc: 0.750109, 
Test 23, loss: 0.303463, test acc: 0.713434,
Train 24, loss: 0.290294, train acc: 0.748473, 
Test 24, loss: 0.302128, test acc: 0.713434,
Train 25, loss: 0.290363, train acc: 0.751143, 
Test 25, loss: 0.304568, test acc: 0.713434,
Train 26, loss: 0.289660, train acc: 0.750342, 
Test 26, loss: 0.301726, test acc: 0.713434,
Train 27, loss: 0.290868, train acc: 0.750543, 
Test 27, loss: 0.311489, test acc: 0.713434,
Train 28, loss: 0.289461, train acc: 0.751377, 
Test 28, loss: 0.314057, test acc: 0.713434,
Train 29, loss: 0.289207, train acc: 0.751878, 
Test 29, loss: 0.308125, test acc: 0.713434,
Train 30, loss: 0.288808, train acc: 0.749107, 
Test 30, loss: 0.304964, test acc: 0.713434,
Train 31, loss: 0.288851, train acc: 0.750676, 
Test 31, loss: 0.305203, test acc: 0.713434,
Train 32, loss: 0.290573, train acc: 0.749240, 
Test 32, loss: 0.304769, test acc: 0.713434,
Train 33, loss: 0.289350, train acc: 0.751778, 
Test 33, loss: 0.301583, test acc: 0.713434,
Train 34, loss: 0.289199, train acc: 0.752145, 
Test 34, loss: 0.330915, test acc: 0.713434,
Train 35, loss: 0.289120, train acc: 0.752312, 
Test 35, loss: 0.303060, test acc: 0.713434,
Train 36, loss: 0.290647, train acc: 0.750810, 
Test 36, loss: 0.301525, test acc: 0.713434,
Train 37, loss: 0.289099, train acc: 0.752846, 
Test 37, loss: 0.301697, test acc: 0.713434,
Train 38, loss: 0.288844, train acc: 0.752512, 
Test 38, loss: 0.301427, test acc: 0.713434,
Train 39, loss: 0.289289, train acc: 0.751143, 
Test 39, loss: 0.301937, test acc: 0.713434,
Train 40, loss: 0.290441, train acc: 0.749841, 
Test 40, loss: 0.310499, test acc: 0.713434,
Train 41, loss: 0.287825, train acc: 0.752813, 
Test 41, loss: 0.307811, test acc: 0.713434,
Train 42, loss: 0.288829, train acc: 0.751978, 
Test 42, loss: 0.303230, test acc: 0.713434,
Train 43, loss: 0.289213, train acc: 0.750876, 
Test 43, loss: 0.317760, test acc: 0.713434,
Train 44, loss: 0.289190, train acc: 0.751611, 
Test 44, loss: 0.302686, test acc: 0.713434,
Train 45, loss: 0.288473, train acc: 0.752245, 
Test 45, loss: 0.312107, test acc: 0.713434,
Train 46, loss: 0.289570, train acc: 0.750810, 
Test 46, loss: 0.311457, test acc: 0.713434,
Train 47, loss: 0.288402, train acc: 0.752078, 
Test 47, loss: 0.305293, test acc: 0.713434,
Train 48, loss: 0.288246, train acc: 0.752212, 
Test 48, loss: 0.306074, test acc: 0.713434,
Train 49, loss: 0.287673, train acc: 0.752445, 
Test 49, loss: 0.305818, test acc: 0.713434,
Train 50, loss: 0.288112, train acc: 0.751244, 
Test 50, loss: 0.301787, test acc: 0.713434,
Train 51, loss: 0.289021, train acc: 0.750876, 
Test 51, loss: 0.301598, test acc: 0.713434,
Train 52, loss: 0.288863, train acc: 0.752279, 
Test 52, loss: 0.309134, test acc: 0.713434,
Train 53, loss: 0.288396, train acc: 0.751177, 
Test 53, loss: 0.301861, test acc: 0.713434,
Train 54, loss: 0.288540, train acc: 0.752813, 
Test 54, loss: 0.302633, test acc: 0.713434,
Train 55, loss: 0.287610, train acc: 0.752846, 
Test 55, loss: 0.302999, test acc: 0.713434,
Train 56, loss: 0.287515, train acc: 0.752512, 
Test 56, loss: 0.302016, test acc: 0.713434,
Train 57, loss: 0.288150, train acc: 0.751678, 
Test 57, loss: 0.301695, test acc: 0.713434,
Train 58, loss: 0.288937, train acc: 0.752312, 
Test 58, loss: 0.302084, test acc: 0.713434,
Train 59, loss: 0.288628, train acc: 0.751878, 
Test 59, loss: 0.305792, test acc: 0.713434,
Train 60, loss: 0.287089, train acc: 0.752245, 
Test 60, loss: 0.305799, test acc: 0.713434,
Train 61, loss: 0.287539, train acc: 0.752212, 
Test 61, loss: 0.312633, test acc: 0.713434,
Train 62, loss: 0.287257, train acc: 0.752880, 
Test 62, loss: 0.301850, test acc: 0.713434,
Train 63, loss: 0.287043, train acc: 0.752813, 
Test 63, loss: 0.301701, test acc: 0.713434,
Train 64, loss: 0.287412, train acc: 0.752479, 
Test 64, loss: 0.312649, test acc: 0.713434,
Train 65, loss: 0.288468, train acc: 0.752345, 
Test 65, loss: 0.303156, test acc: 0.713434,
Train 66, loss: 0.286143, train acc: 0.752913, 
Test 66, loss: 0.307646, test acc: 0.713434,
Train 67, loss: 0.287451, train acc: 0.752813, 
Test 67, loss: 0.309535, test acc: 0.713434,
Train 68, loss: 0.286467, train acc: 0.752813, 
Test 68, loss: 0.306715, test acc: 0.713434,
Train 69, loss: 0.286776, train acc: 0.752846, 
Test 69, loss: 0.301455, test acc: 0.713434,
Train 70, loss: 0.286169, train acc: 0.752880, 
Test 70, loss: 0.301761, test acc: 0.713434,
Train 71, loss: 0.286910, train acc: 0.752813, 
Test 71, loss: 0.302150, test acc: 0.713434,
Train 72, loss: 0.287015, train acc: 0.752813, 
Test 72, loss: 0.303746, test acc: 0.713434,
Train 73, loss: 0.286570, train acc: 0.752813, 
Test 73, loss: 0.301547, test acc: 0.713434,
Train 74, loss: 0.287337, train acc: 0.752846, 
Test 74, loss: 0.302845, test acc: 0.713434,
Train 75, loss: 0.286766, train acc: 0.752880, 
Test 75, loss: 0.302424, test acc: 0.713434,
Train 76, loss: 0.286033, train acc: 0.752145, 
Test 76, loss: 0.301415, test acc: 0.713434,
Train 77, loss: 0.286387, train acc: 0.752813, 
Test 77, loss: 0.301420, test acc: 0.713434,
Train 78, loss: 0.286296, train acc: 0.752813, 
Test 78, loss: 0.304957, test acc: 0.713434,
Train 79, loss: 0.286157, train acc: 0.752813, 
Test 79, loss: 0.304069, test acc: 0.713434,
Train 80, loss: 0.285807, train acc: 0.752813, 
Test 80, loss: 0.301427, test acc: 0.713434,
Train 81, loss: 0.285827, train acc: 0.752813, 
Test 81, loss: 0.302175, test acc: 0.713434,
Train 82, loss: 0.286255, train acc: 0.752913, 
Test 82, loss: 0.309854, test acc: 0.713434,
Train 83, loss: 0.286082, train acc: 0.752846, 
Test 83, loss: 0.305876, test acc: 0.713434,
Train 84, loss: 0.286506, train acc: 0.752880, 
Test 84, loss: 0.307484, test acc: 0.713434,
Train 85, loss: 0.285813, train acc: 0.752813, 
Test 85, loss: 0.328656, test acc: 0.713434,
Train 86, loss: 0.285585, train acc: 0.752813, 
Test 86, loss: 0.308748, test acc: 0.713434,
Train 87, loss: 0.285876, train acc: 0.752846, 
Test 87, loss: 0.304250, test acc: 0.713434,
Train 88, loss: 0.285481, train acc: 0.752880, 
Test 88, loss: 0.301545, test acc: 0.713434,
Train 89, loss: 0.285652, train acc: 0.752846, 
Test 89, loss: 0.304361, test acc: 0.713434,
Train 90, loss: 0.285574, train acc: 0.752813, 
Test 90, loss: 0.304046, test acc: 0.713434,
Train 91, loss: 0.284932, train acc: 0.752813, 
Test 91, loss: 0.302736, test acc: 0.713434,
Train 92, loss: 0.285312, train acc: 0.752880, 
Test 92, loss: 0.302552, test acc: 0.713434,
Train 93, loss: 0.285296, train acc: 0.752846, 
Test 93, loss: 0.302018, test acc: 0.713434,
Train 94, loss: 0.285064, train acc: 0.752846, 
Test 94, loss: 0.301486, test acc: 0.713434,
Train 95, loss: 0.285129, train acc: 0.752813, 
Test 95, loss: 0.306419, test acc: 0.713434,
Train 96, loss: 0.284897, train acc: 0.752846, 
Test 96, loss: 0.307991, test acc: 0.713434,
Train 97, loss: 0.284879, train acc: 0.752813, 
Test 97, loss: 0.301480, test acc: 0.713434,
Train 98, loss: 0.284602, train acc: 0.752880, 
Test 98, loss: 0.303481, test acc: 0.713434,
Train 99, loss: 0.284660, train acc: 0.752813, 
Test 99, loss: 0.303403, test acc: 0.713434,
Train 100, loss: 0.284763, train acc: 0.752813, 
Test 100, loss: 0.302376, test acc: 0.713434,
Train 101, loss: 0.284609, train acc: 0.752880, 
Test 101, loss: 0.302342, test acc: 0.713434,
Train 102, loss: 0.284726, train acc: 0.752813, 
Test 102, loss: 0.305233, test acc: 0.713434,
Train 103, loss: 0.284694, train acc: 0.752813, 
Test 103, loss: 0.306537, test acc: 0.713434,
Train 104, loss: 0.284973, train acc: 0.752846, 
Test 104, loss: 0.302322, test acc: 0.713434,
Train 105, loss: 0.284057, train acc: 0.752813, 
Test 105, loss: 0.301501, test acc: 0.713434,
Train 106, loss: 0.284568, train acc: 0.752846, 
Test 106, loss: 0.301412, test acc: 0.713434,
Train 107, loss: 0.284444, train acc: 0.752813, 
Test 107, loss: 0.308437, test acc: 0.713434,
Train 108, loss: 0.284072, train acc: 0.752846, 
Test 108, loss: 0.302923, test acc: 0.713434,
Train 109, loss: 0.284176, train acc: 0.752846, 
Test 109, loss: 0.301431, test acc: 0.713434,
Train 110, loss: 0.284051, train acc: 0.752846, 
Test 110, loss: 0.302231, test acc: 0.713434,
Train 111, loss: 0.284053, train acc: 0.752813, 
Test 111, loss: 0.304696, test acc: 0.713434,
Train 112, loss: 0.283939, train acc: 0.752880, 
Test 112, loss: 0.303322, test acc: 0.713434,
Train 113, loss: 0.283927, train acc: 0.752846, 
Test 113, loss: 0.305569, test acc: 0.713434,
Train 114, loss: 0.283961, train acc: 0.752880, 
Test 114, loss: 0.306534, test acc: 0.713434,
Train 115, loss: 0.283999, train acc: 0.752846, 
Test 115, loss: 0.302661, test acc: 0.713434,
Train 116, loss: 0.283561, train acc: 0.752813, 
Test 116, loss: 0.301515, test acc: 0.713434,
Train 117, loss: 0.283792, train acc: 0.752846, 
Test 117, loss: 0.301861, test acc: 0.713434,
Train 118, loss: 0.283655, train acc: 0.752813, 
Test 118, loss: 0.302130, test acc: 0.713434,
Train 119, loss: 0.283719, train acc: 0.752813, 
Test 119, loss: 0.302208, test acc: 0.713434,
Train 120, loss: 0.283803, train acc: 0.752846, 
Test 120, loss: 0.301831, test acc: 0.713434,
Train 121, loss: 0.283727, train acc: 0.752846, 
Test 121, loss: 0.303283, test acc: 0.713434,
Train 122, loss: 0.283452, train acc: 0.752846, 
Test 122, loss: 0.304581, test acc: 0.713434,
Train 123, loss: 0.283417, train acc: 0.752813, 
Test 123, loss: 0.303338, test acc: 0.713434,
Train 124, loss: 0.283452, train acc: 0.752846, 
Test 124, loss: 0.301839, test acc: 0.713434,
Train 125, loss: 0.283487, train acc: 0.752880, 
Test 125, loss: 0.301472, test acc: 0.713434,
Train 126, loss: 0.283498, train acc: 0.752813, 
Test 126, loss: 0.302552, test acc: 0.713434,
Train 127, loss: 0.283360, train acc: 0.752846, 
Test 127, loss: 0.303825, test acc: 0.713434,
Train 128, loss: 0.283437, train acc: 0.752846, 
Test 128, loss: 0.302977, test acc: 0.713434,
Train 129, loss: 0.283173, train acc: 0.752880, 
Test 129, loss: 0.302300, test acc: 0.713434,
Train 130, loss: 0.283234, train acc: 0.752846, 
Test 130, loss: 0.301711, test acc: 0.713434,
Train 131, loss: 0.283246, train acc: 0.752880, 
Test 131, loss: 0.302472, test acc: 0.713434,
Train 132, loss: 0.283256, train acc: 0.752813, 
Test 132, loss: 0.303157, test acc: 0.713434,
Train 133, loss: 0.283168, train acc: 0.752813, 
Test 133, loss: 0.301543, test acc: 0.713434,
Train 134, loss: 0.283202, train acc: 0.752813, 
Test 134, loss: 0.301635, test acc: 0.713434,
Train 135, loss: 0.283136, train acc: 0.752846, 
Test 135, loss: 0.301550, test acc: 0.713434,
Train 136, loss: 0.283166, train acc: 0.752846, 
Test 136, loss: 0.301484, test acc: 0.713434,
Train 137, loss: 0.283148, train acc: 0.752846, 
Test 137, loss: 0.301572, test acc: 0.713434,
Train 138, loss: 0.283093, train acc: 0.752880, 
Test 138, loss: 0.301896, test acc: 0.713434,
Train 139, loss: 0.282973, train acc: 0.752846, 
Test 139, loss: 0.301419, test acc: 0.713434,
Train 140, loss: 0.283113, train acc: 0.752813, 
Test 140, loss: 0.301747, test acc: 0.713434,
Train 141, loss: 0.283047, train acc: 0.752846, 
Test 141, loss: 0.301543, test acc: 0.713434,
Train 142, loss: 0.283042, train acc: 0.752846, 
Test 142, loss: 0.301838, test acc: 0.713434,
Train 143, loss: 0.283045, train acc: 0.752813, 
Test 143, loss: 0.301721, test acc: 0.713434,
Train 144, loss: 0.283024, train acc: 0.752813, 
Test 144, loss: 0.301680, test acc: 0.713434,
Train 145, loss: 0.283030, train acc: 0.752880, 
Test 145, loss: 0.301818, test acc: 0.713434,
Train 146, loss: 0.282905, train acc: 0.752880, 
Test 146, loss: 0.301567, test acc: 0.713434,
Train 147, loss: 0.283062, train acc: 0.752813, 
Test 147, loss: 0.301615, test acc: 0.713434,
Train 148, loss: 0.283025, train acc: 0.752813, 
Test 148, loss: 0.301635, test acc: 0.713434,
Train 149, loss: 0.283018, train acc: 0.752846, 
Test 149, loss: 0.301476, test acc: 0.713434,
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 150
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.2
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.582862, train acc: 0.588452, 
Test 0, loss: 0.299908, test acc: 0.854649,
Max Acc:0.854649
Train 1, loss: 0.341189, train acc: 0.590259, 
Test 1, loss: 0.321440, test acc: 0.854649,
Max Acc:0.854649
Train 2, loss: 0.340535, train acc: 0.594595, 
Test 2, loss: 0.279721, test acc: 0.854649,
Max Acc:0.854649
Train 3, loss: 0.340151, train acc: 0.597196, 
Test 3, loss: 0.276279, test acc: 0.854649,
Max Acc:0.854649
Train 4, loss: 0.343035, train acc: 0.588813, 
Test 4, loss: 0.242229, test acc: 0.854649,
Max Acc:0.854649
Train 5, loss: 0.338712, train acc: 0.582743, 
Test 5, loss: 0.250934, test acc: 0.854649,
Max Acc:0.854649
Train 6, loss: 0.333888, train acc: 0.595534, 
Test 6, loss: 0.273008, test acc: 0.854649,
Max Acc:0.854649
Train 7, loss: 0.328012, train acc: 0.608975, 
Test 7, loss: 0.298777, test acc: 0.671165,
Train 8, loss: 0.322414, train acc: 0.618370, 
Test 8, loss: 0.307025, test acc: 0.854649,
Max Acc:0.854649
Train 9, loss: 0.327506, train acc: 0.607169, 
Test 9, loss: 0.356532, test acc: 0.393664,
Train 10, loss: 0.331964, train acc: 0.602616, 
Test 10, loss: 0.285805, test acc: 0.854649,
Max Acc:0.854649
Train 11, loss: 0.338702, train acc: 0.604784, 
Test 11, loss: 0.288103, test acc: 0.854649,
Max Acc:0.854649
Train 12, loss: 0.336524, train acc: 0.586718, 
Test 12, loss: 0.305930, test acc: 0.854649,
Max Acc:0.854649
Train 13, loss: 0.337071, train acc: 0.585778, 
Test 13, loss: 0.403994, test acc: 0.145351,
Train 14, loss: 0.339457, train acc: 0.590331, 
Test 14, loss: 0.340026, test acc: 0.854649,
Max Acc:0.854649
Train 15, loss: 0.336522, train acc: 0.591126, 
Test 15, loss: 0.266407, test acc: 0.854649,
Max Acc:0.854649
Train 16, loss: 0.341407, train acc: 0.588452, 
Test 16, loss: 0.291386, test acc: 0.854649,
Max Acc:0.854649
Train 17, loss: 0.343317, train acc: 0.587585, 
Test 17, loss: 0.228899, test acc: 0.854649,
Max Acc:0.854649
Train 18, loss: 0.342759, train acc: 0.583321, 
Test 18, loss: 0.354139, test acc: 0.145351,
Train 19, loss: 0.340742, train acc: 0.588452, 
Test 19, loss: 0.306917, test acc: 0.854649,
Max Acc:0.854649
Train 20, loss: 0.341840, train acc: 0.590042, 
Test 20, loss: 0.285965, test acc: 0.854649,
Max Acc:0.854649
Train 21, loss: 0.340457, train acc: 0.589175, 
Test 21, loss: 0.246034, test acc: 0.854649,
Max Acc:0.854649
Train 22, loss: 0.340547, train acc: 0.593655, 
Test 22, loss: 0.336379, test acc: 0.854649,
Max Acc:0.854649
Train 23, loss: 0.340853, train acc: 0.591632, 
Test 23, loss: 0.297780, test acc: 0.854649,
Max Acc:0.854649
Train 24, loss: 0.340035, train acc: 0.595751, 
Test 24, loss: 0.308972, test acc: 0.854649,
Max Acc:0.854649
Train 25, loss: 0.340164, train acc: 0.596835, 
Test 25, loss: 0.213490, test acc: 0.854649,
Max Acc:0.854649
Train 26, loss: 0.343018, train acc: 0.590620, 
Test 26, loss: 0.295145, test acc: 0.854649,
Max Acc:0.854649
Train 27, loss: 0.341985, train acc: 0.590403, 
Test 27, loss: 0.236897, test acc: 0.854649,
Max Acc:0.854649
Train 28, loss: 0.340559, train acc: 0.593655, 
Test 28, loss: 0.223137, test acc: 0.854649,
Max Acc:0.854649
Train 29, loss: 0.342139, train acc: 0.586356, 
Test 29, loss: 0.354864, test acc: 0.145351,
Train 30, loss: 0.340764, train acc: 0.596112, 
Test 30, loss: 0.331131, test acc: 0.854649,
Max Acc:0.854649
Train 31, loss: 0.340930, train acc: 0.590331, 
Test 31, loss: 0.298694, test acc: 0.854649,
Max Acc:0.854649
Train 32, loss: 0.341322, train acc: 0.586067, 
Test 32, loss: 0.326822, test acc: 0.854649,
Max Acc:0.854649
Train 33, loss: 0.341420, train acc: 0.587585, 
Test 33, loss: 0.373066, test acc: 0.145351,
Train 34, loss: 0.340874, train acc: 0.596690, 
Test 34, loss: 0.228120, test acc: 0.854649,
Max Acc:0.854649
Train 35, loss: 0.341632, train acc: 0.589897, 
Test 35, loss: 0.284866, test acc: 0.854649,
Max Acc:0.854649
Train 36, loss: 0.339036, train acc: 0.602399, 
Test 36, loss: 0.424543, test acc: 0.145351,
Train 37, loss: 0.341392, train acc: 0.588524, 
Test 37, loss: 0.261571, test acc: 0.854649,
Max Acc:0.854649
Train 38, loss: 0.341813, train acc: 0.588018, 
Test 38, loss: 0.246308, test acc: 0.854649,
Max Acc:0.854649
Train 39, loss: 0.339030, train acc: 0.600448, 
Test 39, loss: 0.280843, test acc: 0.854649,
Max Acc:0.854649
Train 40, loss: 0.339547, train acc: 0.592643, 
Test 40, loss: 0.262673, test acc: 0.854649,
Max Acc:0.854649
Train 41, loss: 0.338183, train acc: 0.604134, 
Test 41, loss: 0.266515, test acc: 0.854649,
Max Acc:0.854649
Train 42, loss: 0.339562, train acc: 0.596184, 
Test 42, loss: 0.277978, test acc: 0.854649,
Max Acc:0.854649
Train 43, loss: 0.340416, train acc: 0.592499, 
Test 43, loss: 0.311310, test acc: 0.854649,
Max Acc:0.854649
Train 44, loss: 0.339661, train acc: 0.594378, 
Test 44, loss: 0.281418, test acc: 0.854649,
Max Acc:0.854649
Train 45, loss: 0.340639, train acc: 0.593872, 
Test 45, loss: 0.254847, test acc: 0.854649,
Max Acc:0.854649
Train 46, loss: 0.339462, train acc: 0.595317, 
Test 46, loss: 0.225182, test acc: 0.854649,
Max Acc:0.854649
Train 47, loss: 0.339085, train acc: 0.600448, 
Test 47, loss: 0.280121, test acc: 0.854649,
Max Acc:0.854649
Train 48, loss: 0.338281, train acc: 0.601893, 
Test 48, loss: 0.263158, test acc: 0.854649,
Max Acc:0.854649
Train 49, loss: 0.339190, train acc: 0.600882, 
Test 49, loss: 0.254931, test acc: 0.854649,
Max Acc:0.854649
Train 50, loss: 0.340463, train acc: 0.596329, 
Test 50, loss: 0.341680, test acc: 0.854649,
Max Acc:0.854649
Train 51, loss: 0.339421, train acc: 0.598063, 
Test 51, loss: 0.315528, test acc: 0.854649,
Max Acc:0.854649
Train 52, loss: 0.339492, train acc: 0.596907, 
Test 52, loss: 0.221953, test acc: 0.854649,
Max Acc:0.854649
Train 53, loss: 0.338979, train acc: 0.595679, 
Test 53, loss: 0.292979, test acc: 0.854649,
Max Acc:0.854649
Train 54, loss: 0.338600, train acc: 0.602327, 
Test 54, loss: 0.267889, test acc: 0.854649,
Max Acc:0.854649
Train 55, loss: 0.338742, train acc: 0.600448, 
Test 55, loss: 0.290293, test acc: 0.854649,
Max Acc:0.854649
Train 56, loss: 0.338817, train acc: 0.601026, 
Test 56, loss: 0.292993, test acc: 0.854649,
Max Acc:0.854649
Train 57, loss: 0.337637, train acc: 0.603050, 
Test 57, loss: 0.284150, test acc: 0.854649,
Max Acc:0.854649
Train 58, loss: 0.338313, train acc: 0.600882, 
Test 58, loss: 0.275378, test acc: 0.854649,
Max Acc:0.854649
Train 59, loss: 0.338825, train acc: 0.596329, 
Test 59, loss: 0.344128, test acc: 0.854649,
Max Acc:0.854649
Train 60, loss: 0.338048, train acc: 0.605796, 
Test 60, loss: 0.378246, test acc: 0.145351,
Train 61, loss: 0.338229, train acc: 0.598930, 
Test 61, loss: 0.286003, test acc: 0.854649,
Max Acc:0.854649
Train 62, loss: 0.337001, train acc: 0.602616, 
Test 62, loss: 0.317745, test acc: 0.854649,
Max Acc:0.854649
Train 63, loss: 0.339077, train acc: 0.598280, 
Test 63, loss: 0.232743, test acc: 0.854649,
Max Acc:0.854649
Train 64, loss: 0.338816, train acc: 0.601532, 
Test 64, loss: 0.343008, test acc: 0.854649,
Max Acc:0.854649
Train 65, loss: 0.339109, train acc: 0.599220, 
Test 65, loss: 0.329362, test acc: 0.854649,
Max Acc:0.854649
Train 66, loss: 0.337706, train acc: 0.602905, 
Test 66, loss: 0.372694, test acc: 0.145351,
Train 67, loss: 0.337259, train acc: 0.603917, 
Test 67, loss: 0.275383, test acc: 0.854649,
Max Acc:0.854649
Train 68, loss: 0.338388, train acc: 0.604278, 
Test 68, loss: 0.252148, test acc: 0.854649,
Max Acc:0.854649
Train 69, loss: 0.337312, train acc: 0.609553, 
Test 69, loss: 0.282153, test acc: 0.854649,
Max Acc:0.854649
Train 70, loss: 0.336772, train acc: 0.612589, 
Test 70, loss: 0.315213, test acc: 0.854649,
Max Acc:0.854649
Train 71, loss: 0.338387, train acc: 0.602110, 
Test 71, loss: 0.298843, test acc: 0.854649,
Max Acc:0.854649
Train 72, loss: 0.338113, train acc: 0.603555, 
Test 72, loss: 0.270593, test acc: 0.854649,
Max Acc:0.854649
Train 73, loss: 0.337008, train acc: 0.608036, 
Test 73, loss: 0.308579, test acc: 0.854649,
Max Acc:0.854649
Train 74, loss: 0.336902, train acc: 0.609264, 
Test 74, loss: 0.278909, test acc: 0.854649,
Max Acc:0.854649
Train 75, loss: 0.337693, train acc: 0.603483, 
Test 75, loss: 0.298772, test acc: 0.854649,
Max Acc:0.854649
Train 76, loss: 0.336758, train acc: 0.610421, 
Test 76, loss: 0.264236, test acc: 0.854649,
Max Acc:0.854649
Train 77, loss: 0.337267, train acc: 0.605723, 
Test 77, loss: 0.288204, test acc: 0.854649,
Max Acc:0.854649
Train 78, loss: 0.336536, train acc: 0.607241, 
Test 78, loss: 0.318077, test acc: 0.854649,
Max Acc:0.854649
Train 79, loss: 0.336162, train acc: 0.612805, 
Test 79, loss: 0.233163, test acc: 0.854649,
Max Acc:0.854649
Train 80, loss: 0.337348, train acc: 0.606374, 
Test 80, loss: 0.254596, test acc: 0.854649,
Max Acc:0.854649
Train 81, loss: 0.337093, train acc: 0.609409, 
Test 81, loss: 0.289741, test acc: 0.854649,
Max Acc:0.854649
Train 82, loss: 0.336022, train acc: 0.615985, 
Test 82, loss: 0.258179, test acc: 0.854649,
Max Acc:0.854649
Train 83, loss: 0.335970, train acc: 0.610493, 
Test 83, loss: 0.248062, test acc: 0.854649,
Max Acc:0.854649
Train 84, loss: 0.336908, train acc: 0.610276, 
Test 84, loss: 0.287753, test acc: 0.854649,
Max Acc:0.854649
Train 85, loss: 0.335457, train acc: 0.614467, 
Test 85, loss: 0.257814, test acc: 0.854649,
Max Acc:0.854649
Train 86, loss: 0.336566, train acc: 0.610999, 
Test 86, loss: 0.353998, test acc: 0.145351,
Train 87, loss: 0.336190, train acc: 0.606446, 
Test 87, loss: 0.284131, test acc: 0.854649,
Max Acc:0.854649
Train 88, loss: 0.335005, train acc: 0.617358, 
Test 88, loss: 0.265695, test acc: 0.854649,
Max Acc:0.854649
Train 89, loss: 0.336053, train acc: 0.615118, 
Test 89, loss: 0.250869, test acc: 0.854649,
Max Acc:0.854649
Train 90, loss: 0.335509, train acc: 0.616708, 
Test 90, loss: 0.283935, test acc: 0.854649,
Max Acc:0.854649
Train 91, loss: 0.335744, train acc: 0.617575, 
Test 91, loss: 0.288055, test acc: 0.854649,
Max Acc:0.854649
Train 92, loss: 0.335809, train acc: 0.613239, 
Test 92, loss: 0.291518, test acc: 0.854649,
Max Acc:0.854649
Train 93, loss: 0.335530, train acc: 0.619381, 
Test 93, loss: 0.314433, test acc: 0.854649,
Max Acc:0.854649
Train 94, loss: 0.336072, train acc: 0.617213, 
Test 94, loss: 0.375295, test acc: 0.145351,
Train 95, loss: 0.335191, train acc: 0.614829, 
Test 95, loss: 0.271750, test acc: 0.854649,
Max Acc:0.854649
Train 96, loss: 0.334977, train acc: 0.619815, 
Test 96, loss: 0.295334, test acc: 0.854649,
Max Acc:0.854649
Train 97, loss: 0.335266, train acc: 0.614251, 
Test 97, loss: 0.266794, test acc: 0.854649,
Max Acc:0.854649
Train 98, loss: 0.334383, train acc: 0.618731, 
Test 98, loss: 0.284676, test acc: 0.854649,
Max Acc:0.854649
Train 99, loss: 0.335066, train acc: 0.615551, 
Test 99, loss: 0.292918, test acc: 0.854649,
Max Acc:0.854649
Train 100, loss: 0.334945, train acc: 0.616491, 
Test 100, loss: 0.272880, test acc: 0.854649,
Max Acc:0.854649
Train 101, loss: 0.335429, train acc: 0.616635, 
Test 101, loss: 0.303170, test acc: 0.854649,
Max Acc:0.854649
Train 102, loss: 0.335159, train acc: 0.618008, 
Test 102, loss: 0.275350, test acc: 0.854649,
Max Acc:0.854649
Train 103, loss: 0.335331, train acc: 0.616708, 
Test 103, loss: 0.288607, test acc: 0.854649,
Max Acc:0.854649
Train 104, loss: 0.334685, train acc: 0.616057, 
Test 104, loss: 0.255610, test acc: 0.854649,
Max Acc:0.854649
Train 105, loss: 0.334062, train acc: 0.617792, 
Test 105, loss: 0.299391, test acc: 0.854649,
Max Acc:0.854649
Train 106, loss: 0.334019, train acc: 0.618731, 
Test 106, loss: 0.273094, test acc: 0.854649,
Max Acc:0.854649
Train 107, loss: 0.334350, train acc: 0.618731, 
Test 107, loss: 0.264486, test acc: 0.854649,
Max Acc:0.854649
Train 108, loss: 0.334179, train acc: 0.618153, 
Test 108, loss: 0.265378, test acc: 0.854649,
Max Acc:0.854649
Train 109, loss: 0.334042, train acc: 0.619887, 
Test 109, loss: 0.316235, test acc: 0.854649,
Max Acc:0.854649
Train 110, loss: 0.334121, train acc: 0.617286, 
Test 110, loss: 0.282147, test acc: 0.854649,
Max Acc:0.854649
Train 111, loss: 0.334110, train acc: 0.619887, 
Test 111, loss: 0.291679, test acc: 0.854649,
Max Acc:0.854649
Train 112, loss: 0.334461, train acc: 0.620032, 
Test 112, loss: 0.248647, test acc: 0.854649,
Max Acc:0.854649
Train 113, loss: 0.334135, train acc: 0.620032, 
Test 113, loss: 0.283515, test acc: 0.854649,
Max Acc:0.854649
Train 114, loss: 0.334398, train acc: 0.620104, 
Test 114, loss: 0.292151, test acc: 0.854649,
Max Acc:0.854649
Train 115, loss: 0.333789, train acc: 0.619960, 
Test 115, loss: 0.291701, test acc: 0.854649,
Max Acc:0.854649
Train 116, loss: 0.333657, train acc: 0.620104, 
Test 116, loss: 0.289274, test acc: 0.854649,
Max Acc:0.854649
Train 117, loss: 0.333669, train acc: 0.620104, 
Test 117, loss: 0.297753, test acc: 0.854649,
Max Acc:0.854649
Train 118, loss: 0.333779, train acc: 0.619960, 
Test 118, loss: 0.291422, test acc: 0.854649,
Max Acc:0.854649
Train 119, loss: 0.333768, train acc: 0.620104, 
Test 119, loss: 0.298351, test acc: 0.854649,
Max Acc:0.854649
Train 120, loss: 0.333489, train acc: 0.619960, 
Test 120, loss: 0.296668, test acc: 0.854649,
Max Acc:0.854649
Train 121, loss: 0.333641, train acc: 0.620032, 
Test 121, loss: 0.285574, test acc: 0.854649,
Max Acc:0.854649
Train 122, loss: 0.333510, train acc: 0.620032, 
Test 122, loss: 0.293422, test acc: 0.854649,
Max Acc:0.854649
Train 123, loss: 0.333612, train acc: 0.620104, 
Test 123, loss: 0.313119, test acc: 0.854649,
Max Acc:0.854649
Train 124, loss: 0.333421, train acc: 0.620176, 
Test 124, loss: 0.271931, test acc: 0.854649,
Max Acc:0.854649
Train 125, loss: 0.333757, train acc: 0.619887, 
Test 125, loss: 0.289634, test acc: 0.854649,
Max Acc:0.854649
Train 126, loss: 0.333367, train acc: 0.620249, 
Test 126, loss: 0.293808, test acc: 0.854649,
Max Acc:0.854649
Train 127, loss: 0.333323, train acc: 0.619960, 
Test 127, loss: 0.264483, test acc: 0.854649,
Max Acc:0.854649
Train 128, loss: 0.333457, train acc: 0.620104, 
Test 128, loss: 0.283658, test acc: 0.854649,
Max Acc:0.854649
Train 129, loss: 0.333299, train acc: 0.620104, 
Test 129, loss: 0.276344, test acc: 0.854649,
Max Acc:0.854649
Train 130, loss: 0.333382, train acc: 0.619960, 
Test 130, loss: 0.302287, test acc: 0.854649,
Max Acc:0.854649
Train 131, loss: 0.333130, train acc: 0.620176, 
Test 131, loss: 0.299235, test acc: 0.854649,
Max Acc:0.854649
Train 132, loss: 0.333155, train acc: 0.620104, 
Test 132, loss: 0.289522, test acc: 0.854649,
Max Acc:0.854649
Train 133, loss: 0.333137, train acc: 0.619887, 
Test 133, loss: 0.274189, test acc: 0.854649,
Max Acc:0.854649
Train 134, loss: 0.333158, train acc: 0.620032, 
Test 134, loss: 0.268617, test acc: 0.854649,
Max Acc:0.854649
Train 135, loss: 0.333123, train acc: 0.620032, 
Test 135, loss: 0.279435, test acc: 0.854649,
Max Acc:0.854649
Train 136, loss: 0.332991, train acc: 0.620032, 
Test 136, loss: 0.283989, test acc: 0.854649,
Max Acc:0.854649
Train 137, loss: 0.333103, train acc: 0.620104, 
Test 137, loss: 0.285663, test acc: 0.854649,
Max Acc:0.854649
Train 138, loss: 0.333035, train acc: 0.619960, 
Test 138, loss: 0.280134, test acc: 0.854649,
Max Acc:0.854649
Train 139, loss: 0.333080, train acc: 0.620176, 
Test 139, loss: 0.276312, test acc: 0.854649,
Max Acc:0.854649
Train 140, loss: 0.333006, train acc: 0.620104, 
Test 140, loss: 0.280403, test acc: 0.854649,
Max Acc:0.854649
Train 141, loss: 0.332996, train acc: 0.619960, 
Test 141, loss: 0.283275, test acc: 0.854649,
Max Acc:0.854649
Train 142, loss: 0.332944, train acc: 0.620176, 
Test 142, loss: 0.278520, test acc: 0.854649,
Max Acc:0.854649
Train 143, loss: 0.332950, train acc: 0.620104, 
Test 143, loss: 0.281136, test acc: 0.854649,
Max Acc:0.854649
Train 144, loss: 0.332948, train acc: 0.620176, 
Test 144, loss: 0.277708, test acc: 0.854649,
Max Acc:0.854649
Train 145, loss: 0.332802, train acc: 0.620176, 
Test 145, loss: 0.289529, test acc: 0.854649,
Max Acc:0.854649
Train 146, loss: 0.332966, train acc: 0.619960, 
Test 146, loss: 0.286495, test acc: 0.854649,
Max Acc:0.854649
Train 147, loss: 0.332881, train acc: 0.620104, 
Test 147, loss: 0.289300, test acc: 0.854649,
Max Acc:0.854649
Train 148, loss: 0.332998, train acc: 0.619960, 
Test 148, loss: 0.281276, test acc: 0.854649,
Max Acc:0.854649
Train 149, loss: 0.332793, train acc: 0.620104, 
Test 149, loss: 0.284449, test acc: 0.854649,
Max Acc:0.854649
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 150
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.1
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.450545, train acc: 0.581544, 
Test 0, loss: 0.341590, test acc: 0.607456,
Max Acc:0.607456
Train 1, loss: 0.333770, train acc: 0.600389, 
Test 1, loss: 0.332877, test acc: 0.623808,
Max Acc:0.623808
Train 2, loss: 0.331999, train acc: 0.600885, 
Test 2, loss: 0.365633, test acc: 0.631636,
Max Acc:0.631636
Train 3, loss: 0.328718, train acc: 0.610384, 
Test 3, loss: 0.351817, test acc: 0.602186,
Train 4, loss: 0.328116, train acc: 0.611529, 
Test 4, loss: 0.346863, test acc: 0.604123,
Train 5, loss: 0.322719, train acc: 0.627322, 
Test 5, loss: 0.333769, test acc: 0.631094,
Train 6, loss: 0.320836, train acc: 0.635143, 
Test 6, loss: 0.341881, test acc: 0.592110,
Train 7, loss: 0.321541, train acc: 0.638843, 
Test 7, loss: 0.347997, test acc: 0.583663,
Train 8, loss: 0.321900, train acc: 0.640217, 
Test 8, loss: 0.333932, test acc: 0.622103,
Train 9, loss: 0.320306, train acc: 0.643574, 
Test 9, loss: 0.352058, test acc: 0.595598,
Train 10, loss: 0.320108, train acc: 0.641704, 
Test 10, loss: 0.373490, test acc: 0.578238,
Train 11, loss: 0.323375, train acc: 0.634113, 
Test 11, loss: 0.341773, test acc: 0.593893,
Train 12, loss: 0.323327, train acc: 0.635257, 
Test 12, loss: 0.340089, test acc: 0.589165,
Train 13, loss: 0.323851, train acc: 0.638729, 
Test 13, loss: 0.344987, test acc: 0.579090,
Train 14, loss: 0.323373, train acc: 0.638691, 
Test 14, loss: 0.356820, test acc: 0.573743,
Train 15, loss: 0.325604, train acc: 0.630260, 
Test 15, loss: 0.339058, test acc: 0.583430,
Train 16, loss: 0.326618, train acc: 0.625415, 
Test 16, loss: 0.350720, test acc: 0.585755,
Train 17, loss: 0.326197, train acc: 0.629916, 
Test 17, loss: 0.366635, test acc: 0.590483,
Train 18, loss: 0.324968, train acc: 0.634532, 
Test 18, loss: 0.334984, test acc: 0.611873,
Train 19, loss: 0.326064, train acc: 0.626521, 
Test 19, loss: 0.337835, test acc: 0.615903,
Train 20, loss: 0.325471, train acc: 0.629115, 
Test 20, loss: 0.349814, test acc: 0.622413,
Train 21, loss: 0.326075, train acc: 0.631862, 
Test 21, loss: 0.336688, test acc: 0.611176,
Train 22, loss: 0.324501, train acc: 0.631709, 
Test 22, loss: 0.339041, test acc: 0.599783,
Train 23, loss: 0.324396, train acc: 0.632358, 
Test 23, loss: 0.344582, test acc: 0.601411,
Train 24, loss: 0.324168, train acc: 0.633388, 
Test 24, loss: 0.360315, test acc: 0.557235,
Train 25, loss: 0.325763, train acc: 0.629153, 
Test 25, loss: 0.334797, test acc: 0.608541,
Train 26, loss: 0.325692, train acc: 0.631175, 
Test 26, loss: 0.341930, test acc: 0.601876,
Train 27, loss: 0.327271, train acc: 0.626292, 
Test 27, loss: 0.343610, test acc: 0.604898,
Train 28, loss: 0.328423, train acc: 0.624309, 
Test 28, loss: 0.345196, test acc: 0.604356,
Train 29, loss: 0.332519, train acc: 0.608934, 
Test 29, loss: 0.346706, test acc: 0.608231,
Train 30, loss: 0.332587, train acc: 0.605844, 
Test 30, loss: 0.337327, test acc: 0.586995,
Train 31, loss: 0.331431, train acc: 0.608477, 
Test 31, loss: 0.358672, test acc: 0.587383,
Train 32, loss: 0.331408, train acc: 0.608553, 
Test 32, loss: 0.332283, test acc: 0.623421,
Train 33, loss: 0.331158, train acc: 0.616335, 
Test 33, loss: 0.340200, test acc: 0.618228,
Train 34, loss: 0.331287, train acc: 0.610689, 
Test 34, loss: 0.341992, test acc: 0.618073,
Train 35, loss: 0.329341, train acc: 0.614924, 
Test 35, loss: 0.352088, test acc: 0.537859,
Train 36, loss: 0.329630, train acc: 0.610079, 
Test 36, loss: 0.337566, test acc: 0.614276,
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 150
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.05
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.749593, train acc: 0.615038, 
Test 0, loss: 0.685578, test acc: 0.638456,
Max Acc:0.638456
Train 1, loss: 0.679213, train acc: 0.625796, 
Test 1, loss: 0.687060, test acc: 0.613113,
Train 2, loss: 0.678831, train acc: 0.629344, 
Test 2, loss: 0.690677, test acc: 0.638456,
Max Acc:0.638456
Train 3, loss: 0.677758, train acc: 0.632625, 
Test 3, loss: 0.689902, test acc: 0.560335,
Train 4, loss: 0.676818, train acc: 0.638767, 
Test 4, loss: 0.691894, test acc: 0.596373,
Train 5, loss: 0.674567, train acc: 0.649754, 
Test 5, loss: 0.692870, test acc: 0.607068,
Train 6, loss: 0.672958, train acc: 0.652920, 
Test 6, loss: 0.704323, test acc: 0.581725,
Train 7, loss: 0.672043, train acc: 0.655019, 
Test 7, loss: 0.705393, test acc: 0.574363,
Train 8, loss: 0.670941, train acc: 0.660779, 
Test 8, loss: 0.686283, test acc: 0.619081,
Train 9, loss: 0.671469, train acc: 0.660436, 
Test 9, loss: 0.697596, test acc: 0.599318,
Train 10, loss: 0.669689, train acc: 0.664403, 
Test 10, loss: 0.704137, test acc: 0.598853,
Train 11, loss: 0.670553, train acc: 0.658147, 
Test 11, loss: 0.694373, test acc: 0.582965,
Train 12, loss: 0.669716, train acc: 0.666730, 
Test 12, loss: 0.704802, test acc: 0.573665,
Train 13, loss: 0.668511, train acc: 0.666311, 
Test 13, loss: 0.697638, test acc: 0.566612,
Train 14, loss: 0.668593, train acc: 0.666158, 
Test 14, loss: 0.706285, test acc: 0.585445,
Train 15, loss: 0.668769, train acc: 0.666158, 
Test 15, loss: 0.692319, test acc: 0.611098,
Train 16, loss: 0.667947, train acc: 0.669553, 
Test 16, loss: 0.693671, test acc: 0.599938,
Train 17, loss: 0.666987, train acc: 0.670698, 
Test 17, loss: 0.704626, test acc: 0.587460,
Train 18, loss: 0.667122, train acc: 0.668371, 
Test 18, loss: 0.698884, test acc: 0.596450,
Train 19, loss: 0.667453, train acc: 0.671384, 
Test 19, loss: 0.691056, test acc: 0.602108,
Train 20, loss: 0.667061, train acc: 0.669668, 
Test 20, loss: 0.709538, test acc: 0.582810,
Train 21, loss: 0.667422, train acc: 0.667112, 
Test 21, loss: 0.691498, test acc: 0.579633,
Train 22, loss: 0.665794, train acc: 0.672681, 
Test 22, loss: 0.689624, test acc: 0.598620,
Train 23, loss: 0.667074, train acc: 0.669401, 
Test 23, loss: 0.695790, test acc: 0.601101,
Train 24, loss: 0.666833, train acc: 0.671270, 
Test 24, loss: 0.703952, test acc: 0.548942,
Train 25, loss: 0.666471, train acc: 0.671156, 
Test 25, loss: 0.691511, test acc: 0.607533,
Train 26, loss: 0.665548, train acc: 0.672071, 
Test 26, loss: 0.699987, test acc: 0.557157,
Train 27, loss: 0.667000, train acc: 0.670164, 
Test 27, loss: 0.694347, test acc: 0.593660,
Train 28, loss: 0.667228, train acc: 0.669401, 
Test 28, loss: 0.693066, test acc: 0.602418,
Train 29, loss: 0.666670, train acc: 0.667722, 
Test 29, loss: 0.701076, test acc: 0.577385,
Train 30, loss: 0.665633, train acc: 0.674131, 
Test 30, loss: 0.695494, test acc: 0.603891,
Train 31, loss: 0.666719, train acc: 0.668828, 
Test 31, loss: 0.699352, test acc: 0.587538,
Train 32, loss: 0.664969, train acc: 0.671957, 
Test 32, loss: 0.693749, test acc: 0.591878,
Train 33, loss: 0.665840, train acc: 0.673368, 
Test 33, loss: 0.702237, test acc: 0.612416,
Train 34, loss: 0.666189, train acc: 0.672453, 
Test 34, loss: 0.701952, test acc: 0.590483,
Train 35, loss: 0.665943, train acc: 0.671613, 
Test 35, loss: 0.718429, test acc: 0.479501,
Train 36, loss: 0.666430, train acc: 0.671117, 
Test 36, loss: 0.703286, test acc: 0.551500,
Train 37, loss: 0.665830, train acc: 0.674856, 
Test 37, loss: 0.702303, test acc: 0.568550,
Train 38, loss: 0.665557, train acc: 0.674474, 
Test 38, loss: 0.689887, test acc: 0.595598,
Train 39, loss: 0.662827, train acc: 0.680693, 
Test 39, loss: 0.684497, test acc: 0.582655,
Train 40, loss: 0.659274, train acc: 0.692481, 
Test 40, loss: 0.696534, test acc: 0.577075,
Train 41, loss: 0.657506, train acc: 0.700645, 
Test 41, loss: 0.719878, test acc: 0.555917,
Train 42, loss: 0.656057, train acc: 0.703315, 
Test 42, loss: 0.698576, test acc: 0.568782,
Train 43, loss: 0.656629, train acc: 0.703926, 
Test 43, loss: 0.687758, test acc: 0.575060,
Train 44, loss: 0.655253, train acc: 0.708313, 
Test 44, loss: 0.711769, test acc: 0.569247,
Train 45, loss: 0.654409, train acc: 0.710449, 
Test 45, loss: 0.717901, test acc: 0.579943,
Train 46, loss: 0.654026, train acc: 0.709457, 
Test 46, loss: 0.726850, test acc: 0.544525,
Train 47, loss: 0.652944, train acc: 0.712318, 
Test 47, loss: 0.689110, test acc: 0.599628,
Train 48, loss: 0.652019, train acc: 0.715332, 
Test 48, loss: 0.708480, test acc: 0.542509,
Train 49, loss: 0.651530, train acc: 0.716744, 
Test 49, loss: 0.710431, test acc: 0.537007,
Train 50, loss: 0.651171, train acc: 0.720978, 
Test 50, loss: 0.680238, test acc: 0.606526,
Train 51, loss: 0.651568, train acc: 0.718689, 
Test 51, loss: 0.697400, test acc: 0.565295,
Train 52, loss: 0.651459, train acc: 0.717163, 
Test 52, loss: 0.691232, test acc: 0.584050,
Train 53, loss: 0.651290, train acc: 0.719109, 
Test 53, loss: 0.673690, test acc: 0.646129,
Max Acc:0.646129
Train 54, loss: 0.650129, train acc: 0.723687, 
Test 54, loss: 0.679442, test acc: 0.619158,
Train 55, loss: 0.651146, train acc: 0.723649, 
Test 55, loss: 0.686717, test acc: 0.594668,
Train 56, loss: 0.649485, train acc: 0.722695, 
Test 56, loss: 0.711685, test acc: 0.563202,
Train 57, loss: 0.649129, train acc: 0.724907, 
Test 57, loss: 0.678079, test acc: 0.624196,
Train 58, loss: 0.648722, train acc: 0.725785, 
Test 58, loss: 0.694407, test acc: 0.592498,
Train 59, loss: 0.649866, train acc: 0.722657, 
Test 59, loss: 0.683221, test acc: 0.618693,
Train 60, loss: 0.647772, train acc: 0.728608, 
Test 60, loss: 0.685309, test acc: 0.616058,
Train 61, loss: 0.649156, train acc: 0.726891, 
Test 61, loss: 0.697661, test acc: 0.551965,
Train 62, loss: 0.649040, train acc: 0.726319, 
Test 62, loss: 0.675301, test acc: 0.619546,
Train 63, loss: 0.647199, train acc: 0.730286, 
Test 63, loss: 0.684358, test acc: 0.611408,
Train 64, loss: 0.647809, train acc: 0.727311, 
Test 64, loss: 0.680498, test acc: 0.608386,
Train 65, loss: 0.648168, train acc: 0.727692, 
Test 65, loss: 0.701441, test acc: 0.584205,
Train 66, loss: 0.647873, train acc: 0.725442, 
Test 66, loss: 0.674335, test acc: 0.633264,
Train 67, loss: 0.647319, train acc: 0.729867, 
Test 67, loss: 0.671781, test acc: 0.635356,
Train 68, loss: 0.647049, train acc: 0.729333, 
Test 68, loss: 0.684321, test acc: 0.613346,
Train 69, loss: 0.646969, train acc: 0.729142, 
Test 69, loss: 0.678370, test acc: 0.604356,
Train 70, loss: 0.646472, train acc: 0.729104, 
Test 70, loss: 0.673633, test acc: 0.633264,
Train 71, loss: 0.646213, train acc: 0.732690, 
Test 71, loss: 0.702913, test acc: 0.584515,
Train 72, loss: 0.646722, train acc: 0.732270, 
Test 72, loss: 0.686806, test acc: 0.607533,
Train 73, loss: 0.646773, train acc: 0.733110, 
Test 73, loss: 0.714427, test acc: 0.575448,
Train 74, loss: 0.644647, train acc: 0.736505, 
Test 74, loss: 0.678078, test acc: 0.636829,
Train 75, loss: 0.645040, train acc: 0.733300, 
Test 75, loss: 0.698062, test acc: 0.579478,
Train 76, loss: 0.644225, train acc: 0.736428, 
Test 76, loss: 0.696711, test acc: 0.589940,
Train 77, loss: 0.645241, train acc: 0.734597, 
Test 77, loss: 0.683836, test acc: 0.607301,
Train 78, loss: 0.643191, train acc: 0.738717, 
Test 78, loss: 0.683483, test acc: 0.607766,
Train 79, loss: 0.643580, train acc: 0.737954, 
Test 79, loss: 0.688556, test acc: 0.585523,
Train 80, loss: 0.642912, train acc: 0.740968, 
Test 80, loss: 0.675658, test acc: 0.617608,
Train 81, loss: 0.643261, train acc: 0.738450, 
Test 81, loss: 0.679469, test acc: 0.639619,
Train 82, loss: 0.643028, train acc: 0.737764, 
Test 82, loss: 0.695518, test acc: 0.602186,
Train 83, loss: 0.641967, train acc: 0.740091, 
Test 83, loss: 0.675530, test acc: 0.623576,
Train 84, loss: 0.640584, train acc: 0.744402, 
Test 84, loss: 0.685923, test acc: 0.605286,
Train 85, loss: 0.641919, train acc: 0.744325, 
Test 85, loss: 0.664701, test acc: 0.673642,
Max Acc:0.673642
Train 86, loss: 0.641583, train acc: 0.739709, 
Test 86, loss: 0.670753, test acc: 0.638456,
Train 87, loss: 0.641594, train acc: 0.740358, 
Test 87, loss: 0.678277, test acc: 0.621018,
Train 88, loss: 0.640528, train acc: 0.746881, 
Test 88, loss: 0.689446, test acc: 0.602573,
Train 89, loss: 0.639598, train acc: 0.744554, 
Test 89, loss: 0.670691, test acc: 0.631791,
Train 90, loss: 0.639077, train acc: 0.747110, 
Test 90, loss: 0.692830, test acc: 0.596683,
Train 91, loss: 0.638001, train acc: 0.748407, 
Test 91, loss: 0.692002, test acc: 0.611408,
Train 92, loss: 0.638560, train acc: 0.749132, 
Test 92, loss: 0.693512, test acc: 0.611176,
Train 93, loss: 0.637562, train acc: 0.750658, 
Test 93, loss: 0.673644, test acc: 0.638146,
Train 94, loss: 0.637682, train acc: 0.750582, 
Test 94, loss: 0.683070, test acc: 0.635434,
Train 95, loss: 0.636825, train acc: 0.752031, 
Test 95, loss: 0.670494, test acc: 0.652639,
Train 96, loss: 0.635470, train acc: 0.753634, 
Test 96, loss: 0.680685, test acc: 0.633729,
Train 97, loss: 0.635409, train acc: 0.754626, 
Test 97, loss: 0.676314, test acc: 0.628071,
Train 98, loss: 0.633565, train acc: 0.758250, 
Test 98, loss: 0.678931, test acc: 0.626753,
Train 99, loss: 0.633620, train acc: 0.757906, 
Test 99, loss: 0.669971, test acc: 0.635434,
Train 100, loss: 0.632843, train acc: 0.760081, 
Test 100, loss: 0.669856, test acc: 0.646671,
Train 101, loss: 0.632732, train acc: 0.759661, 
Test 101, loss: 0.663051, test acc: 0.660932,
Train 102, loss: 0.633382, train acc: 0.760424, 
Test 102, loss: 0.666467, test acc: 0.649616,
Train 103, loss: 0.633104, train acc: 0.760233, 
Test 103, loss: 0.660988, test acc: 0.675889,
Max Acc:0.675889
Train 104, loss: 0.632779, train acc: 0.757449, 
Test 104, loss: 0.682005, test acc: 0.636829,
Train 105, loss: 0.630966, train acc: 0.766223, 
Test 105, loss: 0.676313, test acc: 0.641789,
Train 106, loss: 0.631147, train acc: 0.764239, 
Test 106, loss: 0.666766, test acc: 0.661087,
Train 107, loss: 0.629721, train acc: 0.766337, 
Test 107, loss: 0.668050, test acc: 0.667674,
Train 108, loss: 0.629836, train acc: 0.764506, 
Test 108, loss: 0.685144, test acc: 0.612648,
Train 109, loss: 0.629702, train acc: 0.767367, 
Test 109, loss: 0.660600, test acc: 0.672169,
Train 110, loss: 0.629096, train acc: 0.768893, 
Test 110, loss: 0.674789, test acc: 0.627528,
Train 111, loss: 0.629356, train acc: 0.769389, 
Test 111, loss: 0.670067, test acc: 0.654886,
Train 112, loss: 0.627552, train acc: 0.770343, 
Test 112, loss: 0.682315, test acc: 0.622878,
Train 113, loss: 0.627756, train acc: 0.773471, 
Test 113, loss: 0.680048, test acc: 0.631869,
Train 114, loss: 0.626004, train acc: 0.772861, 
Test 114, loss: 0.673587, test acc: 0.657367,
Train 115, loss: 0.626249, train acc: 0.775150, 
Test 115, loss: 0.675463, test acc: 0.644036,
Train 116, loss: 0.624963, train acc: 0.780185, 
Test 116, loss: 0.670056, test acc: 0.657832,
Train 117, loss: 0.625505, train acc: 0.778125, 
Test 117, loss: 0.660000, test acc: 0.682554,
Max Acc:0.682554
Train 118, loss: 0.624273, train acc: 0.778049, 
Test 118, loss: 0.666075, test acc: 0.664497,
Train 119, loss: 0.623051, train acc: 0.780147, 
Test 119, loss: 0.669849, test acc: 0.664884,
Train 120, loss: 0.623279, train acc: 0.780300, 
Test 120, loss: 0.651293, test acc: 0.694955,
Max Acc:0.694955
Train 121, loss: 0.623615, train acc: 0.780758, 
Test 121, loss: 0.666054, test acc: 0.662017,
Train 122, loss: 0.622569, train acc: 0.782780, 
Test 122, loss: 0.676538, test acc: 0.639154,
Train 123, loss: 0.621241, train acc: 0.785793, 
Test 123, loss: 0.664239, test acc: 0.670619,
Train 124, loss: 0.622146, train acc: 0.781788, 
Test 124, loss: 0.665758, test acc: 0.657987,
Train 125, loss: 0.619527, train acc: 0.788693, 
Test 125, loss: 0.673817, test acc: 0.654809,
Train 126, loss: 0.620406, train acc: 0.788693, 
Test 126, loss: 0.670395, test acc: 0.666047,
Train 127, loss: 0.618854, train acc: 0.792012, 
Test 127, loss: 0.658896, test acc: 0.691622,
Train 128, loss: 0.619627, train acc: 0.788922, 
Test 128, loss: 0.682277, test acc: 0.637604,
Train 129, loss: 0.618632, train acc: 0.789837, 
Test 129, loss: 0.668734, test acc: 0.664497,
Train 130, loss: 0.618410, train acc: 0.790753, 
Test 130, loss: 0.667303, test acc: 0.665659,
Train 131, loss: 0.618398, train acc: 0.789685, 
Test 131, loss: 0.672797, test acc: 0.655894,
Train 132, loss: 0.616974, train acc: 0.797238, 
Test 132, loss: 0.677858, test acc: 0.648686,
Train 133, loss: 0.616765, train acc: 0.794415, 
Test 133, loss: 0.670442, test acc: 0.654266,
Train 134, loss: 0.616412, train acc: 0.797848, 
Test 134, loss: 0.671534, test acc: 0.656436,
Train 135, loss: 0.615693, train acc: 0.797467, 
Test 135, loss: 0.683317, test acc: 0.637991,
Train 136, loss: 0.614246, train acc: 0.801129, 
Test 136, loss: 0.663925, test acc: 0.673022,
Train 137, loss: 0.614615, train acc: 0.799489, 
Test 137, loss: 0.666829, test acc: 0.671472,
Train 138, loss: 0.613430, train acc: 0.801549, 
Test 138, loss: 0.662334, test acc: 0.678369,
Train 139, loss: 0.613050, train acc: 0.803952, 
Test 139, loss: 0.675217, test acc: 0.658142,
Train 140, loss: 0.613514, train acc: 0.803914, 
Test 140, loss: 0.669879, test acc: 0.660234,
Train 141, loss: 0.613303, train acc: 0.801358, 
Test 141, loss: 0.679660, test acc: 0.638921,
Train 142, loss: 0.612606, train acc: 0.805287, 
Test 142, loss: 0.672300, test acc: 0.657522,
Train 143, loss: 0.613119, train acc: 0.802731, 
Test 143, loss: 0.671934, test acc: 0.662637,
Train 144, loss: 0.612165, train acc: 0.806279, 
Test 144, loss: 0.666788, test acc: 0.674494,
Train 145, loss: 0.609862, train acc: 0.809636, 
Test 145, loss: 0.669165, test acc: 0.663489,
Train 146, loss: 0.611225, train acc: 0.807805, 
Test 146, loss: 0.684150, test acc: 0.643494,
Train 147, loss: 0.610601, train acc: 0.807080, 
Test 147, loss: 0.670789, test acc: 0.660854,
Train 148, loss: 0.611033, train acc: 0.809217, 
Test 148, loss: 0.682209, test acc: 0.655351,
Train 149, loss: 0.610461, train acc: 0.809369, 
Test 149, loss: 0.665464, test acc: 0.675657,
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 150
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.04
manual_seed: 0
momentum: 0.95
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.137456, train acc: 0.478691, 
Test 0, loss: 0.075659, test acc: 0.465322,
Max Acc:0.465322
Train 1, loss: 0.074185, train acc: 0.487434, 
Test 1, loss: 0.075749, test acc: 0.349442,
Train 2, loss: 0.067831, train acc: 0.504287, 
Test 2, loss: 0.073746, test acc: 0.416567,
Train 3, loss: 0.067103, train acc: 0.501837, 
Test 3, loss: 0.073247, test acc: 0.384721,
Train 4, loss: 0.066354, train acc: 0.510919, 
Test 4, loss: 0.074278, test acc: 0.393820,
Train 5, loss: 0.066307, train acc: 0.508131, 
Test 5, loss: 0.073275, test acc: 0.390386,
Train 6, loss: 0.065904, train acc: 0.510961, 
Test 6, loss: 0.077679, test acc: 0.435880,
Train 7, loss: 0.065616, train acc: 0.520634, 
Test 7, loss: 0.075004, test acc: 0.401373,
Train 8, loss: 0.065852, train acc: 0.512355, 
Test 8, loss: 0.076081, test acc: 0.413562,
Train 9, loss: 0.065614, train acc: 0.512313, 
Test 9, loss: 0.074378, test acc: 0.384979,
Train 10, loss: 0.065611, train acc: 0.509736, 
Test 10, loss: 0.075053, test acc: 0.423262,
Train 11, loss: 0.065492, train acc: 0.513369, 
Test 11, loss: 0.074197, test acc: 0.349442,
Train 12, loss: 0.065096, train acc: 0.518184, 
Test 12, loss: 0.075194, test acc: 0.390472,
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 150
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.05
manual_seed: 0
momentum: 1.25
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: nan, train acc: 0.585428, 
Test 0, loss: nan, test acc: 0.650558,
Max Acc:0.650558
Train 1, loss: nan, train acc: 0.590201, 
Test 1, loss: nan, test acc: 0.650558,
Max Acc:0.650558
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 150
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.01
manual_seed: 0
momentum: 0.8
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.632401, train acc: 0.631679, 
Test 0, loss: 0.671793, test acc: 0.560515,
Max Acc:0.560515
Train 1, loss: 0.605499, train acc: 0.647687, 
Test 1, loss: 0.716744, test acc: 0.588069,
Max Acc:0.588069
Train 2, loss: 0.590082, train acc: 0.659725, 
Test 2, loss: 0.714308, test acc: 0.535107,
Train 3, loss: 0.590108, train acc: 0.656304, 
Test 3, loss: 0.674917, test acc: 0.556395,
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 500
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.03
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.121090, train acc: 0.502449, 
Test 0, loss: 0.083091, test acc: 0.349142,
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 500
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.03
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.115915, train acc: 0.507770, 
Test 0, loss: 0.078063, test acc: 0.377616,
Max Acc:0.377616
Train 1, loss: 0.082235, train acc: 0.509840, 
Test 1, loss: 0.078797, test acc: 0.391681,
Max Acc:0.391681
Train 2, loss: 0.071470, train acc: 0.510726, 
Test 2, loss: 0.075439, test acc: 0.384134,
Train 3, loss: 0.069887, train acc: 0.503716, 
Test 3, loss: 0.073954, test acc: 0.349142,
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 500
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.03
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.815001, train acc: 0.628463, 
Test 0, loss: 0.685185, test acc: 0.627616,
Max Acc:0.627616
Train 1, loss: 0.626438, train acc: 0.656419, 
Test 1, loss: 0.679035, test acc: 0.619468,
Train 2, loss: 0.620051, train acc: 0.662627, 
Test 2, loss: 0.696617, test acc: 0.600429,
Train 3, loss: 0.616857, train acc: 0.666512, 
Test 3, loss: 0.706517, test acc: 0.543911,
Train 4, loss: 0.614322, train acc: 0.670777, 
Test 4, loss: 0.697119, test acc: 0.603002,
Train 5, loss: 0.611436, train acc: 0.673269, 
Test 5, loss: 0.690590, test acc: 0.584734,
Train 6, loss: 0.607262, train acc: 0.679519, 
Test 6, loss: 0.726061, test acc: 0.610292,
Train 7, loss: 0.607658, train acc: 0.678843, 
Test 7, loss: 0.714336, test acc: 0.614580,
Train 8, loss: 0.605957, train acc: 0.678209, 
Test 8, loss: 0.720479, test acc: 0.625986,
Train 9, loss: 0.607044, train acc: 0.677618, 
Test 9, loss: 0.719915, test acc: 0.618010,
Train 10, loss: 0.605231, train acc: 0.679603, 
Test 10, loss: 0.749736, test acc: 0.609348,
Train 11, loss: 0.605958, train acc: 0.680870, 
Test 11, loss: 0.683666, test acc: 0.619554,
Train 12, loss: 0.602505, train acc: 0.681968, 
Test 12, loss: 0.716399, test acc: 0.612607,
Train 13, loss: 0.599317, train acc: 0.687162, 
Test 13, loss: 0.739260, test acc: 0.574786,
Train 14, loss: 0.596765, train acc: 0.685811, 
Test 14, loss: 0.703989, test acc: 0.603259,
Train 15, loss: 0.584059, train acc: 0.698480, 
Test 15, loss: 0.720212, test acc: 0.572899,
Train 16, loss: 0.569102, train acc: 0.711993, 
Test 16, loss: 0.694295, test acc: 0.581389,
Train 17, loss: 0.558873, train acc: 0.722931, 
Test 17, loss: 0.726865, test acc: 0.610120,
Train 18, loss: 0.548215, train acc: 0.732264, 
Test 18, loss: 0.662697, test acc: 0.639708,
Max Acc:0.639708
Train 19, loss: 0.539865, train acc: 0.740118, 
Test 19, loss: 0.661137, test acc: 0.621269,
Train 20, loss: 0.535758, train acc: 0.741681, 
Test 20, loss: 0.728495, test acc: 0.614923,
Train 21, loss: 0.533423, train acc: 0.743032, 
Test 21, loss: 0.684496, test acc: 0.621098,
Train 22, loss: 0.529426, train acc: 0.745397, 
Test 22, loss: 0.643579, test acc: 0.640480,
Max Acc:0.640480
Train 23, loss: 0.525791, train acc: 0.748437, 
Test 23, loss: 0.686465, test acc: 0.594768,
Train 24, loss: 0.520767, train acc: 0.753885, 
Test 24, loss: 0.733667, test acc: 0.608319,
Train 25, loss: 0.517272, train acc: 0.756968, 
Test 25, loss: 0.646748, test acc: 0.655832,
Max Acc:0.655832
Train 26, loss: 0.517486, train acc: 0.757052, 
Test 26, loss: 0.678004, test acc: 0.622899,
Train 27, loss: 0.514142, train acc: 0.755617, 
Test 27, loss: 0.704926, test acc: 0.623585,
Train 28, loss: 0.510801, train acc: 0.759797, 
Test 28, loss: 0.735357, test acc: 0.636021,
Train 29, loss: 0.509826, train acc: 0.759797, 
Test 29, loss: 0.729605, test acc: 0.610720,
Train 30, loss: 0.509090, train acc: 0.758277, 
Test 30, loss: 0.709757, test acc: 0.642453,
Train 31, loss: 0.505285, train acc: 0.766005, 
Test 31, loss: 0.768659, test acc: 0.579674,
Train 32, loss: 0.502695, train acc: 0.761402, 
Test 32, loss: 0.694329, test acc: 0.617496,
Train 33, loss: 0.501806, train acc: 0.766132, 
Test 33, loss: 0.671104, test acc: 0.649314,
Train 34, loss: 0.501275, train acc: 0.764062, 
Test 34, loss: 0.700250, test acc: 0.646655,
Train 35, loss: 0.505369, train acc: 0.763514, 
Test 35, loss: 0.767223, test acc: 0.616724,
Train 36, loss: 0.500918, train acc: 0.768708, 
Test 36, loss: 0.755036, test acc: 0.603688,
Train 37, loss: 0.498417, train acc: 0.770059, 
Test 37, loss: 0.713454, test acc: 0.615437,
Train 38, loss: 0.500649, train acc: 0.768581, 
Test 38, loss: 0.629120, test acc: 0.663036,
Max Acc:0.663036
Train 39, loss: 0.494557, train acc: 0.769341, 
Test 39, loss: 0.674506, test acc: 0.644425,
Train 40, loss: 0.497801, train acc: 0.767948, 
Test 40, loss: 0.660691, test acc: 0.661492,
Train 41, loss: 0.489170, train acc: 0.770355, 
Test 41, loss: 0.780425, test acc: 0.608233,
Train 42, loss: 0.492348, train acc: 0.769468, 
Test 42, loss: 0.641927, test acc: 0.655832,
Train 43, loss: 0.490034, train acc: 0.771453, 
Test 43, loss: 0.691546, test acc: 0.626329,
Train 44, loss: 0.488662, train acc: 0.772044, 
Test 44, loss: 0.673525, test acc: 0.639365,
Train 45, loss: 0.487754, train acc: 0.773691, 
Test 45, loss: 0.717033, test acc: 0.615266,
Train 46, loss: 0.488791, train acc: 0.771622, 
Test 46, loss: 0.808908, test acc: 0.586021,
Train 47, loss: 0.487279, train acc: 0.772677, 
Test 47, loss: 0.639396, test acc: 0.663808,
Max Acc:0.663808
Train 48, loss: 0.485723, train acc: 0.774029, 
Test 48, loss: 0.719031, test acc: 0.617839,
Train 49, loss: 0.483457, train acc: 0.776731, 
Test 49, loss: 0.751580, test acc: 0.631561,
Train 50, loss: 0.478714, train acc: 0.774029, 
Test 50, loss: 0.710134, test acc: 0.660463,
Train 51, loss: 0.479551, train acc: 0.773986, 
Test 51, loss: 0.637575, test acc: 0.666381,
Max Acc:0.666381
Train 52, loss: 0.482036, train acc: 0.774620, 
Test 52, loss: 0.748884, test acc: 0.607633,
Train 53, loss: 0.481251, train acc: 0.777323, 
Test 53, loss: 0.667661, test acc: 0.640823,
Train 54, loss: 0.476765, train acc: 0.781334, 
Test 54, loss: 0.666246, test acc: 0.677015,
Max Acc:0.677015
Train 55, loss: 0.477840, train acc: 0.779054, 
Test 55, loss: 0.669965, test acc: 0.661063,
Train 56, loss: 0.479289, train acc: 0.778970, 
Test 56, loss: 0.694746, test acc: 0.656432,
Train 57, loss: 0.478396, train acc: 0.775845, 
Test 57, loss: 0.729523, test acc: 0.635592,
Train 58, loss: 0.480132, train acc: 0.774367, 
Test 58, loss: 0.769374, test acc: 0.602487,
Train 59, loss: 0.477035, train acc: 0.776478, 
Test 59, loss: 0.729116, test acc: 0.613722,
Train 60, loss: 0.470017, train acc: 0.784079, 
Test 60, loss: 0.618648, test acc: 0.675557,
Train 61, loss: 0.472572, train acc: 0.782770, 
Test 61, loss: 0.682396, test acc: 0.660720,
Train 62, loss: 0.475406, train acc: 0.780194, 
Test 62, loss: 0.682538, test acc: 0.647599,
Train 63, loss: 0.472533, train acc: 0.782010, 
Test 63, loss: 0.653879, test acc: 0.641509,
Train 64, loss: 0.469282, train acc: 0.783657, 
Test 64, loss: 0.634746, test acc: 0.671784,
Train 65, loss: 0.467188, train acc: 0.783995, 
Test 65, loss: 0.668903, test acc: 0.658834,
Train 66, loss: 0.469560, train acc: 0.784079, 
Test 66, loss: 0.719471, test acc: 0.615952,
Train 67, loss: 0.483428, train acc: 0.773100, 
Test 67, loss: 0.658661, test acc: 0.640137,
Train 68, loss: 0.480563, train acc: 0.776647, 
Test 68, loss: 0.628833, test acc: 0.668868,
Train 69, loss: 0.466924, train acc: 0.784586, 
Test 69, loss: 0.738582, test acc: 0.633276,
Train 70, loss: 0.467039, train acc: 0.782306, 
Test 70, loss: 0.754570, test acc: 0.635420,
Train 71, loss: 0.464242, train acc: 0.786360, 
Test 71, loss: 0.660795, test acc: 0.651630,
Train 72, loss: 0.466073, train acc: 0.785346, 
Test 72, loss: 0.711683, test acc: 0.640223,
Train 73, loss: 0.465741, train acc: 0.788851, 
Test 73, loss: 0.639570, test acc: 0.670154,
Train 74, loss: 0.474112, train acc: 0.776943, 
Test 74, loss: 0.717623, test acc: 0.646827,
Train 75, loss: 0.462511, train acc: 0.785895, 
Test 75, loss: 0.727386, test acc: 0.646998,
Train 76, loss: 0.461874, train acc: 0.786106, 
Test 76, loss: 0.679674, test acc: 0.675386,
Train 77, loss: 0.467301, train acc: 0.783488, 
Test 77, loss: 0.691306, test acc: 0.651887,
Train 78, loss: 0.460595, train acc: 0.786486, 
Test 78, loss: 0.742351, test acc: 0.639880,
Train 79, loss: 0.462154, train acc: 0.785811, 
Test 79, loss: 0.678277, test acc: 0.657290,
Train 80, loss: 0.457569, train acc: 0.788429, 
Test 80, loss: 0.675044, test acc: 0.666467,
Train 81, loss: 0.459080, train acc: 0.789274, 
Test 81, loss: 0.633415, test acc: 0.680789,
Max Acc:0.680789
Train 82, loss: 0.458191, train acc: 0.789316, 
Test 82, loss: 0.695773, test acc: 0.666038,
Train 83, loss: 0.452288, train acc: 0.791470, 
Test 83, loss: 0.691827, test acc: 0.671441,
Train 84, loss: 0.459835, train acc: 0.786698, 
Test 84, loss: 0.648978, test acc: 0.663979,
Train 85, loss: 0.458032, train acc: 0.789949, 
Test 85, loss: 0.705804, test acc: 0.630617,
Train 86, loss: 0.478457, train acc: 0.776098, 
Test 86, loss: 0.699268, test acc: 0.649914,
Train 87, loss: 0.487879, train acc: 0.768074, 
Test 87, loss: 0.729697, test acc: 0.642710,
Train 88, loss: 0.480184, train acc: 0.774789, 
Test 88, loss: 0.615673, test acc: 0.672470,
Train 89, loss: 0.478057, train acc: 0.776731, 
Test 89, loss: 0.778545, test acc: 0.589708,
Train 90, loss: 0.465909, train acc: 0.784164, 
Test 90, loss: 0.668980, test acc: 0.640480,
Train 91, loss: 0.462593, train acc: 0.785515, 
Test 91, loss: 0.734646, test acc: 0.629331,
Train 92, loss: 0.462160, train acc: 0.788345, 
Test 92, loss: 0.773396, test acc: 0.604803,
Train 93, loss: 0.461293, train acc: 0.789527, 
Test 93, loss: 0.673891, test acc: 0.661063,
Train 94, loss: 0.458846, train acc: 0.789189, 
Test 94, loss: 0.590893, test acc: 0.695798,
Max Acc:0.695798
Train 95, loss: 0.451686, train acc: 0.790456, 
Test 95, loss: 0.662700, test acc: 0.671698,
Train 96, loss: 0.459794, train acc: 0.787373, 
Test 96, loss: 0.694037, test acc: 0.654202,
Train 97, loss: 0.454529, train acc: 0.790076, 
Test 97, loss: 0.678008, test acc: 0.642882,
Train 98, loss: 0.460218, train acc: 0.783319, 
Test 98, loss: 0.885071, test acc: 0.622642,
Train 99, loss: 0.453854, train acc: 0.789696, 
Test 99, loss: 0.667729, test acc: 0.648714,
Train 100, loss: 0.458242, train acc: 0.788471, 
Test 100, loss: 0.645766, test acc: 0.674099,
Train 101, loss: 0.446887, train acc: 0.795228, 
Test 101, loss: 0.678375, test acc: 0.653345,
Train 102, loss: 0.456786, train acc: 0.787247, 
Test 102, loss: 0.678408, test acc: 0.655146,
Train 103, loss: 0.460441, train acc: 0.784924, 
Test 103, loss: 0.697409, test acc: 0.647770,
Train 104, loss: 0.485609, train acc: 0.768201, 
Test 104, loss: 0.755745, test acc: 0.605575,
Train 105, loss: 0.467958, train acc: 0.779730, 
Test 105, loss: 0.730290, test acc: 0.611063,
Train 106, loss: 0.454367, train acc: 0.789316, 
Test 106, loss: 0.661018, test acc: 0.653002,
Train 107, loss: 0.446855, train acc: 0.794552, 
Test 107, loss: 0.630098, test acc: 0.681218,
Train 108, loss: 0.445972, train acc: 0.794510, 
Test 108, loss: 0.642199, test acc: 0.668696,
Train 109, loss: 0.443876, train acc: 0.794721, 
Test 109, loss: 0.682433, test acc: 0.661149,
Train 110, loss: 0.443416, train acc: 0.797424, 
Test 110, loss: 0.661784, test acc: 0.683962,
Train 111, loss: 0.450066, train acc: 0.793201, 
Test 111, loss: 0.665066, test acc: 0.649828,
Train 112, loss: 0.476436, train acc: 0.776774, 
Test 112, loss: 0.776350, test acc: 0.600772,
Train 113, loss: 0.480956, train acc: 0.775042, 
Test 113, loss: 0.647837, test acc: 0.658233,
Train 114, loss: 0.463209, train acc: 0.783657, 
Test 114, loss: 0.710005, test acc: 0.670755,
Train 115, loss: 0.465246, train acc: 0.782813, 
Test 115, loss: 0.794274, test acc: 0.585849,
Train 116, loss: 0.450785, train acc: 0.790794, 
Test 116, loss: 0.617333, test acc: 0.677959,
Train 117, loss: 0.442238, train acc: 0.794299, 
Test 117, loss: 0.679451, test acc: 0.676415,
Train 118, loss: 0.441282, train acc: 0.798564, 
Test 118, loss: 0.674649, test acc: 0.677358,
Train 119, loss: 0.441045, train acc: 0.799155, 
Test 119, loss: 0.711773, test acc: 0.663722,
Train 120, loss: 0.442031, train acc: 0.796706, 
Test 120, loss: 0.648686, test acc: 0.676758,
Train 121, loss: 0.439547, train acc: 0.800042, 
Test 121, loss: 0.616669, test acc: 0.683877,
Train 122, loss: 0.454739, train acc: 0.788640, 
Test 122, loss: 0.633760, test acc: 0.704545,
Max Acc:0.704545
Train 123, loss: 0.466045, train acc: 0.783108, 
Test 123, loss: 0.647690, test acc: 0.653774,
Train 124, loss: 0.453523, train acc: 0.789400, 
Test 124, loss: 0.639549, test acc: 0.657376,
Train 125, loss: 0.441545, train acc: 0.797677, 
Test 125, loss: 0.617892, test acc: 0.679245,
Train 126, loss: 0.473182, train acc: 0.780912, 
Test 126, loss: 0.714331, test acc: 0.657890,
Train 127, loss: 0.457005, train acc: 0.789400, 
Test 127, loss: 0.679599, test acc: 0.642281,
Train 128, loss: 0.483936, train acc: 0.767356, 
Test 128, loss: 0.642586, test acc: 0.650600,
Train 129, loss: 0.478206, train acc: 0.776985, 
Test 129, loss: 0.758293, test acc: 0.613894,
Train 130, loss: 0.460337, train acc: 0.785304, 
Test 130, loss: 0.699499, test acc: 0.619468,
Train 131, loss: 0.461148, train acc: 0.787880, 
Test 131, loss: 0.616399, test acc: 0.668954,
Train 132, loss: 0.455598, train acc: 0.790203, 
Test 132, loss: 0.642663, test acc: 0.678473,
Train 133, loss: 0.442675, train acc: 0.797677, 
Test 133, loss: 0.638660, test acc: 0.675557,
Train 134, loss: 0.435557, train acc: 0.799958, 
Test 134, loss: 0.672873, test acc: 0.684048,
Train 135, loss: 0.436851, train acc: 0.800084, 
Test 135, loss: 0.689908, test acc: 0.668525,
Train 136, loss: 0.433050, train acc: 0.801098, 
Test 136, loss: 0.712016, test acc: 0.660892,
Train 137, loss: 0.449265, train acc: 0.791512, 
Test 137, loss: 0.683483, test acc: 0.650086,
Train 138, loss: 0.443913, train acc: 0.797973, 
Test 138, loss: 0.672206, test acc: 0.670326,
Train 139, loss: 0.434043, train acc: 0.801182, 
Test 139, loss: 0.702998, test acc: 0.660377,
Train 140, loss: 0.429609, train acc: 0.802407, 
Test 140, loss: 0.678589, test acc: 0.659863,
Train 141, loss: 0.431670, train acc: 0.802449, 
Test 141, loss: 0.645243, test acc: 0.680789,
Train 142, loss: 0.437177, train acc: 0.799282, 
Test 142, loss: 0.627216, test acc: 0.690223,
Train 143, loss: 0.440658, train acc: 0.798438, 
Test 143, loss: 0.622052, test acc: 0.701115,
Train 144, loss: 0.429189, train acc: 0.802196, 
Test 144, loss: 0.585855, test acc: 0.703945,
Train 145, loss: 0.424500, train acc: 0.806672, 
Test 145, loss: 0.588214, test acc: 0.707290,
Max Acc:0.707290
Train 146, loss: 0.428842, train acc: 0.803716, 
Test 146, loss: 0.601968, test acc: 0.708576,
Max Acc:0.708576
Train 147, loss: 0.426279, train acc: 0.806334, 
Test 147, loss: 0.759989, test acc: 0.643310,
Train 148, loss: 0.430874, train acc: 0.801309, 
Test 148, loss: 0.626259, test acc: 0.684648,
Train 149, loss: 0.427355, train acc: 0.805279, 
Test 149, loss: 0.717884, test acc: 0.651715,
Train 150, loss: 0.427783, train acc: 0.807770, 
Test 150, loss: 0.627752, test acc: 0.689365,
Train 151, loss: 0.426109, train acc: 0.806419, 
Test 151, loss: 0.832012, test acc: 0.631132,
Train 152, loss: 0.418199, train acc: 0.809502, 
Test 152, loss: 0.643409, test acc: 0.695626,
Train 153, loss: 0.428859, train acc: 0.804561, 
Test 153, loss: 0.682520, test acc: 0.672213,
Train 154, loss: 0.427954, train acc: 0.803209, 
Test 154, loss: 0.739881, test acc: 0.658919,
Train 155, loss: 0.423725, train acc: 0.805870, 
Test 155, loss: 0.645227, test acc: 0.674528,
Train 156, loss: 0.434525, train acc: 0.801774, 
Test 156, loss: 0.674247, test acc: 0.677873,
Train 157, loss: 0.429659, train acc: 0.804012, 
Test 157, loss: 0.595722, test acc: 0.691767,
Train 158, loss: 0.423582, train acc: 0.807390, 
Test 158, loss: 0.662133, test acc: 0.697256,
Train 159, loss: 0.423447, train acc: 0.806883, 
Test 159, loss: 0.696150, test acc: 0.663636,
Train 160, loss: 0.424316, train acc: 0.810093, 
Test 160, loss: 0.830664, test acc: 0.649571,
Train 161, loss: 0.429892, train acc: 0.805110, 
Test 161, loss: 0.610754, test acc: 0.702144,
Train 162, loss: 0.428379, train acc: 0.804688, 
Test 162, loss: 0.622707, test acc: 0.675386,
Train 163, loss: 0.429753, train acc: 0.803547, 
Test 163, loss: 0.625925, test acc: 0.684820,
Train 164, loss: 0.422008, train acc: 0.809924, 
Test 164, loss: 0.622745, test acc: 0.711921,
Max Acc:0.711921
Train 165, loss: 0.419275, train acc: 0.811149, 
Test 165, loss: 0.671214, test acc: 0.681732,
Train 166, loss: 0.422728, train acc: 0.808826, 
Test 166, loss: 0.672692, test acc: 0.678216,
Train 167, loss: 0.429758, train acc: 0.803801, 
Test 167, loss: 0.739914, test acc: 0.624614,
Train 168, loss: 0.436868, train acc: 0.801858, 
Test 168, loss: 0.646191, test acc: 0.677444,
Train 169, loss: 0.437193, train acc: 0.797931, 
Test 169, loss: 0.674142, test acc: 0.671012,
Train 170, loss: 0.437682, train acc: 0.798564, 
Test 170, loss: 0.705625, test acc: 0.658319,
Train 171, loss: 0.456826, train acc: 0.785937, 
Test 171, loss: 0.638511, test acc: 0.675729,
Train 172, loss: 0.448234, train acc: 0.791554, 
Test 172, loss: 0.685098, test acc: 0.662007,
Train 173, loss: 0.442995, train acc: 0.796157, 
Test 173, loss: 0.652507, test acc: 0.669554,
Train 174, loss: 0.426422, train acc: 0.805574, 
Test 174, loss: 0.668143, test acc: 0.679417,
Train 175, loss: 0.424048, train acc: 0.803843, 
Test 175, loss: 0.617469, test acc: 0.673499,
Train 176, loss: 0.421160, train acc: 0.806123, 
Test 176, loss: 0.771140, test acc: 0.647170,
Train 177, loss: 0.424654, train acc: 0.806250, 
Test 177, loss: 0.648027, test acc: 0.669726,
Train 178, loss: 0.417353, train acc: 0.810980, 
Test 178, loss: 0.662483, test acc: 0.679760,
Train 179, loss: 0.414393, train acc: 0.813091, 
Test 179, loss: 0.654450, test acc: 0.672813,
Train 180, loss: 0.412543, train acc: 0.813894, 
Test 180, loss: 0.693283, test acc: 0.680360,
Train 181, loss: 0.418286, train acc: 0.808446, 
Test 181, loss: 0.640870, test acc: 0.676244,
Train 182, loss: 0.412333, train acc: 0.812838, 
Test 182, loss: 0.710895, test acc: 0.674700,
Train 183, loss: 0.415197, train acc: 0.810642, 
Test 183, loss: 0.620052, test acc: 0.687650,
Train 184, loss: 0.414032, train acc: 0.813894, 
Test 184, loss: 0.643004, test acc: 0.692882,
Train 185, loss: 0.413976, train acc: 0.810769, 
Test 185, loss: 0.646149, test acc: 0.661578,
Train 186, loss: 0.418892, train acc: 0.810346, 
Test 186, loss: 0.648103, test acc: 0.680360,
Train 187, loss: 0.414214, train acc: 0.812627, 
Test 187, loss: 0.597726, test acc: 0.689537,
Train 188, loss: 0.413201, train acc: 0.812880, 
Test 188, loss: 0.748451, test acc: 0.651372,
Train 189, loss: 0.418656, train acc: 0.807432, 
Test 189, loss: 0.731994, test acc: 0.652316,
Train 190, loss: 0.414175, train acc: 0.813598, 
Test 190, loss: 0.675477, test acc: 0.678216,
Train 191, loss: 0.415847, train acc: 0.812838, 
Test 191, loss: 0.664380, test acc: 0.671012,
Train 192, loss: 0.420346, train acc: 0.809333, 
Test 192, loss: 0.674986, test acc: 0.669983,
Train 193, loss: 0.434458, train acc: 0.800422, 
Test 193, loss: 0.579807, test acc: 0.718096,
Max Acc:0.718096
Train 194, loss: 0.421814, train acc: 0.808657, 
Test 194, loss: 0.610554, test acc: 0.697856,
Train 195, loss: 0.418688, train acc: 0.810980, 
Test 195, loss: 0.626328, test acc: 0.682075,
Train 196, loss: 0.420906, train acc: 0.807770, 
Test 196, loss: 0.623920, test acc: 0.694340,
Train 197, loss: 0.417320, train acc: 0.811149, 
Test 197, loss: 0.731608, test acc: 0.654803,
Train 198, loss: 0.410157, train acc: 0.814654, 
Test 198, loss: 0.635190, test acc: 0.695712,
Train 199, loss: 0.411807, train acc: 0.813514, 
Test 199, loss: 0.686928, test acc: 0.660120,
Train 200, loss: 0.411746, train acc: 0.815498, 
Test 200, loss: 0.648239, test acc: 0.674099,
Train 201, loss: 0.409130, train acc: 0.816807, 
Test 201, loss: 0.628291, test acc: 0.696312,
Train 202, loss: 0.404472, train acc: 0.818919, 
Test 202, loss: 0.632412, test acc: 0.673756,
Train 203, loss: 0.406273, train acc: 0.816132, 
Test 203, loss: 0.732078, test acc: 0.677358,
Train 204, loss: 0.407753, train acc: 0.816765, 
Test 204, loss: 0.718710, test acc: 0.670755,
Train 205, loss: 0.405053, train acc: 0.817948, 
Test 205, loss: 0.594668, test acc: 0.698714,
Train 206, loss: 0.408291, train acc: 0.817694, 
Test 206, loss: 0.646909, test acc: 0.683877,
Train 207, loss: 0.408896, train acc: 0.814949, 
Test 207, loss: 0.669577, test acc: 0.677187,
Train 208, loss: 0.426403, train acc: 0.805743, 
Test 208, loss: 0.609569, test acc: 0.700943,
Train 209, loss: 0.423338, train acc: 0.802956, 
Test 209, loss: 0.614477, test acc: 0.698714,
Train 210, loss: 0.415271, train acc: 0.810473, 
Test 210, loss: 0.608490, test acc: 0.695369,
Train 211, loss: 0.410397, train acc: 0.814020, 
Test 211, loss: 0.644263, test acc: 0.668611,
Train 212, loss: 0.406595, train acc: 0.814569, 
Test 212, loss: 0.789208, test acc: 0.645111,
Train 213, loss: 0.410428, train acc: 0.815034, 
Test 213, loss: 0.610229, test acc: 0.701115,
Train 214, loss: 0.407906, train acc: 0.818497, 
Test 214, loss: 0.748225, test acc: 0.655918,
Train 215, loss: 0.405663, train acc: 0.816005, 
Test 215, loss: 0.690131, test acc: 0.663551,
Train 216, loss: 0.401626, train acc: 0.817990, 
Test 216, loss: 0.725460, test acc: 0.677187,
Train 217, loss: 0.403778, train acc: 0.818792, 
Test 217, loss: 0.683082, test acc: 0.660120,
Train 218, loss: 0.404671, train acc: 0.817736, 
Test 218, loss: 0.736603, test acc: 0.671012,
Train 219, loss: 0.397686, train acc: 0.820861, 
Test 219, loss: 0.797220, test acc: 0.671698,
Train 220, loss: 0.401836, train acc: 0.820650, 
Test 220, loss: 0.697387, test acc: 0.655489,
Train 221, loss: 0.405272, train acc: 0.817061, 
Test 221, loss: 0.628442, test acc: 0.703431,
Train 222, loss: 0.403089, train acc: 0.817821, 
Test 222, loss: 0.734152, test acc: 0.639194,
Train 223, loss: 0.397332, train acc: 0.821284, 
Test 223, loss: 0.674865, test acc: 0.674185,
Train 224, loss: 0.422345, train acc: 0.807517, 
Test 224, loss: 0.695652, test acc: 0.645026,
Train 225, loss: 0.406076, train acc: 0.817948, 
Test 225, loss: 0.668618, test acc: 0.682676,
Train 226, loss: 0.403356, train acc: 0.819215, 
Test 226, loss: 0.629865, test acc: 0.701544,
Train 227, loss: 0.399897, train acc: 0.818539, 
Test 227, loss: 0.653958, test acc: 0.709177,
Train 228, loss: 0.398755, train acc: 0.820819, 
Test 228, loss: 0.635753, test acc: 0.698971,
Train 229, loss: 0.399771, train acc: 0.818623, 
Test 229, loss: 0.775751, test acc: 0.641852,
Train 230, loss: 0.400336, train acc: 0.821748, 
Test 230, loss: 0.642506, test acc: 0.673671,
Train 231, loss: 0.401367, train acc: 0.819088, 
Test 231, loss: 0.632509, test acc: 0.688165,
Train 232, loss: 0.397218, train acc: 0.824071, 
Test 232, loss: 0.604135, test acc: 0.703602,
Train 233, loss: 0.399058, train acc: 0.820735, 
Test 233, loss: 0.634874, test acc: 0.671784,
Train 234, loss: 0.396873, train acc: 0.822340, 
Test 234, loss: 0.642608, test acc: 0.684477,
Train 235, loss: 0.398015, train acc: 0.821368, 
Test 235, loss: 0.653163, test acc: 0.688851,
Train 236, loss: 0.394492, train acc: 0.822720, 
Test 236, loss: 0.680100, test acc: 0.667410,
Train 237, loss: 0.397163, train acc: 0.823100, 
Test 237, loss: 0.687909, test acc: 0.671098,
Train 238, loss: 0.395905, train acc: 0.822002, 
Test 238, loss: 0.648089, test acc: 0.675986,
Train 239, loss: 0.395143, train acc: 0.822002, 
Test 239, loss: 0.608932, test acc: 0.714580,
Train 240, loss: 0.397021, train acc: 0.818285, 
Test 240, loss: 0.673150, test acc: 0.679503,
Train 241, loss: 0.395742, train acc: 0.822635, 
Test 241, loss: 0.652289, test acc: 0.683019,
Train 242, loss: 0.435384, train acc: 0.798226, 
Test 242, loss: 0.641296, test acc: 0.662350,
Train 243, loss: 0.424303, train acc: 0.807137, 
Test 243, loss: 0.756521, test acc: 0.639966,
Train 244, loss: 0.411072, train acc: 0.813345, 
Test 244, loss: 0.698154, test acc: 0.661835,
Train 245, loss: 0.403725, train acc: 0.818623, 
Test 245, loss: 0.704102, test acc: 0.659605,
Train 246, loss: 0.406357, train acc: 0.814231, 
Test 246, loss: 0.674229, test acc: 0.672470,
Train 247, loss: 0.406670, train acc: 0.817905, 
Test 247, loss: 0.643390, test acc: 0.687479,
Train 248, loss: 0.402583, train acc: 0.818877, 
Test 248, loss: 0.697575, test acc: 0.683533,
Train 249, loss: 0.397422, train acc: 0.820946, 
Test 249, loss: 0.660137, test acc: 0.697513,
Train 250, loss: 0.396166, train acc: 0.821706, 
Test 250, loss: 0.723287, test acc: 0.647341,
Train 251, loss: 0.394721, train acc: 0.824113, 
Test 251, loss: 0.672311, test acc: 0.680274,
Train 252, loss: 0.391571, train acc: 0.823057, 
Test 252, loss: 0.668591, test acc: 0.659777,
Train 253, loss: 0.394425, train acc: 0.822551, 
Test 253, loss: 0.660212, test acc: 0.685163,
Train 254, loss: 0.393666, train acc: 0.821368, 
Test 254, loss: 0.691929, test acc: 0.673242,
Train 255, loss: 0.396629, train acc: 0.823057, 
Test 255, loss: 0.683440, test acc: 0.669039,
Train 256, loss: 0.393600, train acc: 0.821833, 
Test 256, loss: 0.652434, test acc: 0.701372,
Train 257, loss: 0.393286, train acc: 0.823986, 
Test 257, loss: 0.701046, test acc: 0.670497,
Train 258, loss: 0.393350, train acc: 0.821579, 
Test 258, loss: 0.759632, test acc: 0.643482,
Train 259, loss: 0.390222, train acc: 0.825971, 
Test 259, loss: 0.755763, test acc: 0.666123,
Train 260, loss: 0.392514, train acc: 0.823564, 
Test 260, loss: 0.627501, test acc: 0.697513,
Train 261, loss: 0.390804, train acc: 0.824240, 
Test 261, loss: 0.688182, test acc: 0.689537,
Train 262, loss: 0.387995, train acc: 0.825380, 
Test 262, loss: 0.660162, test acc: 0.676072,
Train 263, loss: 0.389778, train acc: 0.825338, 
Test 263, loss: 0.664699, test acc: 0.674528,
Train 264, loss: 0.386895, train acc: 0.826309, 
Test 264, loss: 0.778786, test acc: 0.668182,
Train 265, loss: 0.387626, train acc: 0.825802, 
Test 265, loss: 0.649443, test acc: 0.684134,
Train 266, loss: 0.386668, train acc: 0.828336, 
Test 266, loss: 0.698893, test acc: 0.673156,
Train 267, loss: 0.386163, train acc: 0.827069, 
Test 267, loss: 0.700490, test acc: 0.688250,
Train 268, loss: 0.389029, train acc: 0.826816, 
Test 268, loss: 0.616308, test acc: 0.712436,
Train 269, loss: 0.385176, train acc: 0.825971, 
Test 269, loss: 0.653581, test acc: 0.687822,
Train 270, loss: 0.386469, train acc: 0.827027, 
Test 270, loss: 0.664574, test acc: 0.678559,
Train 271, loss: 0.382583, train acc: 0.829814, 
Test 271, loss: 0.696336, test acc: 0.679931,
Train 272, loss: 0.395427, train acc: 0.825000, 
Test 272, loss: 0.668615, test acc: 0.680017,
Train 273, loss: 0.385711, train acc: 0.829856, 
Test 273, loss: 0.686575, test acc: 0.674357,
Train 274, loss: 0.385715, train acc: 0.828547, 
Test 274, loss: 0.686189, test acc: 0.687050,
Train 275, loss: 0.384916, train acc: 0.826309, 
Test 275, loss: 0.753617, test acc: 0.659262,
Train 276, loss: 0.379806, train acc: 0.830279, 
Test 276, loss: 0.703569, test acc: 0.657290,
Train 277, loss: 0.380481, train acc: 0.827407, 
Test 277, loss: 0.678286, test acc: 0.677101,
Train 278, loss: 0.382048, train acc: 0.831377, 
Test 278, loss: 0.672712, test acc: 0.676415,
Train 279, loss: 0.379334, train acc: 0.830574, 
Test 279, loss: 0.644860, test acc: 0.682933,
Train 280, loss: 0.376494, train acc: 0.832517, 
Test 280, loss: 0.700388, test acc: 0.682075,
Train 281, loss: 0.381147, train acc: 0.830870, 
Test 281, loss: 0.742308, test acc: 0.681990,
Train 282, loss: 0.384721, train acc: 0.827787, 
Test 282, loss: 0.637637, test acc: 0.700000,
Train 283, loss: 0.386229, train acc: 0.827914, 
Test 283, loss: 0.711659, test acc: 0.657547,
Train 284, loss: 0.379272, train acc: 0.829983, 
Test 284, loss: 0.643463, test acc: 0.701201,
Train 285, loss: 0.379330, train acc: 0.830448, 
Test 285, loss: 0.737666, test acc: 0.659949,
Train 286, loss: 0.381514, train acc: 0.830448, 
Test 286, loss: 0.613939, test acc: 0.720840,
Max Acc:0.720840
Train 287, loss: 0.374150, train acc: 0.834544, 
Test 287, loss: 0.671020, test acc: 0.684563,
Train 288, loss: 0.379075, train acc: 0.830870, 
Test 288, loss: 0.729574, test acc: 0.680189,
Train 289, loss: 0.378753, train acc: 0.831208, 
Test 289, loss: 0.649048, test acc: 0.697599,
Train 290, loss: 0.374270, train acc: 0.831546, 
Test 290, loss: 0.675606, test acc: 0.678302,
Train 291, loss: 0.379158, train acc: 0.830954, 
Test 291, loss: 0.645746, test acc: 0.685763,
Train 292, loss: 0.374869, train acc: 0.836191, 
Test 292, loss: 0.607388, test acc: 0.718268,
Train 293, loss: 0.375907, train acc: 0.830785, 
Test 293, loss: 0.640273, test acc: 0.698027,
Train 294, loss: 0.375145, train acc: 0.831039, 
Test 294, loss: 0.723570, test acc: 0.658405,
Train 295, loss: 0.375524, train acc: 0.832264, 
Test 295, loss: 0.614880, test acc: 0.709949,
Train 296, loss: 0.376911, train acc: 0.829856, 
Test 296, loss: 0.644322, test acc: 0.708662,
Train 297, loss: 0.375970, train acc: 0.832770, 
Test 297, loss: 0.762135, test acc: 0.639794,
Train 298, loss: 0.374228, train acc: 0.832939, 
Test 298, loss: 0.680321, test acc: 0.667925,
Train 299, loss: 0.371140, train acc: 0.836782, 
Test 299, loss: 0.768532, test acc: 0.644168,
Train 300, loss: 0.375749, train acc: 0.833235, 
Test 300, loss: 0.734586, test acc: 0.656346,
Train 301, loss: 0.369775, train acc: 0.833066, 
Test 301, loss: 0.778844, test acc: 0.671098,
Train 302, loss: 0.378684, train acc: 0.833024, 
Test 302, loss: 0.630955, test acc: 0.697513,
Train 303, loss: 0.373625, train acc: 0.834755, 
Test 303, loss: 0.668799, test acc: 0.685678,
Train 304, loss: 0.374048, train acc: 0.834713, 
Test 304, loss: 0.628684, test acc: 0.697513,
Train 305, loss: 0.366188, train acc: 0.837669, 
Test 305, loss: 0.642839, test acc: 0.693225,
Train 306, loss: 0.368253, train acc: 0.835980, 
Test 306, loss: 0.665270, test acc: 0.693225,
Train 307, loss: 0.368494, train acc: 0.835431, 
Test 307, loss: 0.611066, test acc: 0.710806,
Train 308, loss: 0.374071, train acc: 0.834206, 
Test 308, loss: 0.598775, test acc: 0.711407,
Train 309, loss: 0.371548, train acc: 0.834333, 
Test 309, loss: 0.631339, test acc: 0.698456,
Train 310, loss: 0.368273, train acc: 0.835346, 
Test 310, loss: 0.623758, test acc: 0.714494,
Train 311, loss: 0.367120, train acc: 0.841427, 
Test 311, loss: 0.604765, test acc: 0.708319,
Train 312, loss: 0.366194, train acc: 0.836486, 
Test 312, loss: 0.608697, test acc: 0.713722,
Train 313, loss: 0.371019, train acc: 0.833193, 
Test 313, loss: 0.662201, test acc: 0.691424,
Train 314, loss: 0.365433, train acc: 0.837035, 
Test 314, loss: 0.792400, test acc: 0.663808,
Train 315, loss: 0.363302, train acc: 0.840034, 
Test 315, loss: 0.694625, test acc: 0.701372,
Train 316, loss: 0.364303, train acc: 0.840498, 
Test 316, loss: 0.658287, test acc: 0.698971,
Train 317, loss: 0.363621, train acc: 0.839696, 
Test 317, loss: 0.659783, test acc: 0.694168,
Train 318, loss: 0.364249, train acc: 0.836064, 
Test 318, loss: 0.714063, test acc: 0.680274,
Train 319, loss: 0.364365, train acc: 0.840836, 
Test 319, loss: 0.670377, test acc: 0.668353,
Train 320, loss: 0.363037, train acc: 0.839823, 
Test 320, loss: 0.698841, test acc: 0.681818,
Train 321, loss: 0.360661, train acc: 0.841639, 
Test 321, loss: 0.709727, test acc: 0.675986,
Train 322, loss: 0.371376, train acc: 0.833150, 
Test 322, loss: 0.751024, test acc: 0.651887,
Train 323, loss: 0.373833, train acc: 0.833108, 
Test 323, loss: 0.732134, test acc: 0.690051,
Train 324, loss: 0.367287, train acc: 0.835431, 
Test 324, loss: 0.680154, test acc: 0.685420,
Train 325, loss: 0.363183, train acc: 0.839105, 
Test 325, loss: 0.680385, test acc: 0.690309,
Train 326, loss: 0.361244, train acc: 0.840372, 
Test 326, loss: 0.611386, test acc: 0.717238,
Train 327, loss: 0.360309, train acc: 0.841765, 
Test 327, loss: 0.639890, test acc: 0.704889,
Train 328, loss: 0.358121, train acc: 0.841554, 
Test 328, loss: 0.734961, test acc: 0.693053,
Train 329, loss: 0.355198, train acc: 0.839992, 
Test 329, loss: 0.656589, test acc: 0.694425,
Train 330, loss: 0.357820, train acc: 0.842821, 
Test 330, loss: 0.629379, test acc: 0.698971,
Train 331, loss: 0.355594, train acc: 0.842019, 
Test 331, loss: 0.641280, test acc: 0.697684,
Train 332, loss: 0.355053, train acc: 0.841639, 
Test 332, loss: 0.748814, test acc: 0.658748,
Train 333, loss: 0.354418, train acc: 0.843539, 
Test 333, loss: 0.750287, test acc: 0.684477,
Train 334, loss: 0.357647, train acc: 0.844130, 
Test 334, loss: 0.704061, test acc: 0.700429,
Train 335, loss: 0.355563, train acc: 0.841639, 
Test 335, loss: 0.715070, test acc: 0.685506,
Train 336, loss: 0.357156, train acc: 0.843032, 
Test 336, loss: 0.788048, test acc: 0.681904,
Train 337, loss: 0.349029, train acc: 0.844890, 
Test 337, loss: 0.678467, test acc: 0.690566,
Train 338, loss: 0.351351, train acc: 0.846537, 
Test 338, loss: 0.657624, test acc: 0.685763,
Train 339, loss: 0.345002, train acc: 0.846791, 
Test 339, loss: 0.726059, test acc: 0.672127,
Train 340, loss: 0.350816, train acc: 0.845144, 
Test 340, loss: 0.759837, test acc: 0.669811,
Train 341, loss: 0.346817, train acc: 0.847382, 
Test 341, loss: 0.713517, test acc: 0.691767,
Train 342, loss: 0.346581, train acc: 0.847213, 
Test 342, loss: 0.671693, test acc: 0.692024,
Train 343, loss: 0.353302, train acc: 0.844848, 
Test 343, loss: 0.660359, test acc: 0.683619,
Train 344, loss: 0.351213, train acc: 0.844215, 
Test 344, loss: 0.759275, test acc: 0.669297,
Train 345, loss: 0.355553, train acc: 0.841385, 
Test 345, loss: 0.695760, test acc: 0.675043,
Train 346, loss: 0.345154, train acc: 0.850000, 
Test 346, loss: 0.724832, test acc: 0.669983,
Train 347, loss: 0.346346, train acc: 0.848902, 
Test 347, loss: 0.712038, test acc: 0.692196,
Train 348, loss: 0.348940, train acc: 0.844975, 
Test 348, loss: 0.682453, test acc: 0.702916,
Train 349, loss: 0.345538, train acc: 0.847551, 
Test 349, loss: 0.662664, test acc: 0.706604,
Train 350, loss: 0.346023, train acc: 0.849704, 
Test 350, loss: 0.699490, test acc: 0.710720,
Train 351, loss: 0.342271, train acc: 0.851267, 
Test 351, loss: 0.670971, test acc: 0.688937,
Train 352, loss: 0.345347, train acc: 0.846622, 
Test 352, loss: 0.623818, test acc: 0.707976,
Train 353, loss: 0.352946, train acc: 0.846748, 
Test 353, loss: 0.705310, test acc: 0.675043,
Train 354, loss: 0.344975, train acc: 0.849916, 
Test 354, loss: 0.610294, test acc: 0.715094,
Train 355, loss: 0.339578, train acc: 0.850549, 
Test 355, loss: 0.732701, test acc: 0.684220,
Train 356, loss: 0.345652, train acc: 0.848860, 
Test 356, loss: 0.737451, test acc: 0.666552,
Train 357, loss: 0.341297, train acc: 0.850507, 
Test 357, loss: 0.839117, test acc: 0.638851,
Train 358, loss: 0.339561, train acc: 0.851943, 
Test 358, loss: 0.697029, test acc: 0.692281,
Train 359, loss: 0.340687, train acc: 0.852998, 
Test 359, loss: 0.702473, test acc: 0.686449,
Train 360, loss: 0.338236, train acc: 0.851182, 
Test 360, loss: 0.702852, test acc: 0.693482,
Train 361, loss: 0.334261, train acc: 0.854054, 
Test 361, loss: 0.789621, test acc: 0.681818,
Train 362, loss: 0.339953, train acc: 0.851056, 
Test 362, loss: 0.708530, test acc: 0.693053,
Train 363, loss: 0.337718, train acc: 0.852872, 
Test 363, loss: 0.693977, test acc: 0.695197,
Train 364, loss: 0.334441, train acc: 0.854645, 
Test 364, loss: 0.841552, test acc: 0.640909,
Train 365, loss: 0.338043, train acc: 0.852787, 
Test 365, loss: 0.698700, test acc: 0.692796,
Train 366, loss: 0.336495, train acc: 0.852956, 
Test 366, loss: 0.719030, test acc: 0.693310,
Train 367, loss: 0.334304, train acc: 0.854603, 
Test 367, loss: 0.698511, test acc: 0.700858,
Train 368, loss: 0.333034, train acc: 0.855997, 
Test 368, loss: 0.710349, test acc: 0.700515,
Train 369, loss: 0.334712, train acc: 0.854434, 
Test 369, loss: 0.810990, test acc: 0.678473,
Train 370, loss: 0.331919, train acc: 0.854392, 
Test 370, loss: 0.733436, test acc: 0.677015,
Train 371, loss: 0.332514, train acc: 0.856250, 
Test 371, loss: 0.714559, test acc: 0.670069,
Train 372, loss: 0.327439, train acc: 0.857179, 
Test 372, loss: 0.843375, test acc: 0.673842,
Train 373, loss: 0.330743, train acc: 0.854561, 
Test 373, loss: 0.782954, test acc: 0.671955,
Train 374, loss: 0.327999, train acc: 0.857728, 
Test 374, loss: 0.656641, test acc: 0.705232,
Train 375, loss: 0.325021, train acc: 0.856630, 
Test 375, loss: 0.705103, test acc: 0.675472,
Train 376, loss: 0.328035, train acc: 0.856968, 
Test 376, loss: 0.637709, test acc: 0.700858,
Train 377, loss: 0.329039, train acc: 0.856503, 
Test 377, loss: 0.690127, test acc: 0.693396,
Train 378, loss: 0.325982, train acc: 0.858910, 
Test 378, loss: 0.683672, test acc: 0.691767,
Train 379, loss: 0.323454, train acc: 0.859544, 
Test 379, loss: 0.739784, test acc: 0.685678,
Train 380, loss: 0.326870, train acc: 0.859544, 
Test 380, loss: 0.732853, test acc: 0.695455,
Train 381, loss: 0.323805, train acc: 0.859333, 
Test 381, loss: 0.766306, test acc: 0.693997,
Train 382, loss: 0.322172, train acc: 0.859459, 
Test 382, loss: 0.670526, test acc: 0.697770,
Train 383, loss: 0.323052, train acc: 0.860431, 
Test 383, loss: 0.710458, test acc: 0.704631,
Train 384, loss: 0.321157, train acc: 0.862416, 
Test 384, loss: 0.662229, test acc: 0.707376,
Train 385, loss: 0.317844, train acc: 0.863007, 
Test 385, loss: 0.754710, test acc: 0.685935,
Train 386, loss: 0.317551, train acc: 0.863260, 
Test 386, loss: 0.777102, test acc: 0.676501,
Train 387, loss: 0.315259, train acc: 0.863598, 
Test 387, loss: 0.821290, test acc: 0.669039,
Train 388, loss: 0.316170, train acc: 0.864062, 
Test 388, loss: 0.756358, test acc: 0.678045,
Train 389, loss: 0.313947, train acc: 0.863007, 
Test 389, loss: 0.781353, test acc: 0.667839,
Train 390, loss: 0.315147, train acc: 0.860811, 
Test 390, loss: 0.747275, test acc: 0.682676,
Train 391, loss: 0.314467, train acc: 0.864485, 
Test 391, loss: 0.734226, test acc: 0.690652,
Train 392, loss: 0.320374, train acc: 0.861444, 
Test 392, loss: 0.768867, test acc: 0.684305,
Train 393, loss: 0.318032, train acc: 0.863429, 
Test 393, loss: 0.724781, test acc: 0.690995,
Train 394, loss: 0.310476, train acc: 0.865921, 
Test 394, loss: 0.730679, test acc: 0.691509,
Train 395, loss: 0.311401, train acc: 0.864949, 
Test 395, loss: 0.810949, test acc: 0.682075,
Train 396, loss: 0.313497, train acc: 0.863809, 
Test 396, loss: 0.714534, test acc: 0.696827,
Train 397, loss: 0.308004, train acc: 0.867272, 
Test 397, loss: 0.778471, test acc: 0.683105,
Train 398, loss: 0.307718, train acc: 0.866850, 
Test 398, loss: 0.715979, test acc: 0.699400,
Train 399, loss: 0.309912, train acc: 0.865456, 
Test 399, loss: 0.774418, test acc: 0.687221,
Train 400, loss: 0.311361, train acc: 0.866090, 
Test 400, loss: 0.721827, test acc: 0.692710,
Train 401, loss: 0.305978, train acc: 0.866258, 
Test 401, loss: 0.741987, test acc: 0.694940,
Train 402, loss: 0.305874, train acc: 0.866554, 
Test 402, loss: 0.733113, test acc: 0.671612,
Train 403, loss: 0.305367, train acc: 0.868243, 
Test 403, loss: 0.776170, test acc: 0.684134,
Train 404, loss: 0.300927, train acc: 0.869215, 
Test 404, loss: 0.752856, test acc: 0.679931,
Train 405, loss: 0.305843, train acc: 0.869426, 
Test 405, loss: 0.732772, test acc: 0.683362,
Train 406, loss: 0.302619, train acc: 0.871833, 
Test 406, loss: 0.794590, test acc: 0.679931,
Train 407, loss: 0.304529, train acc: 0.868454, 
Test 407, loss: 0.820365, test acc: 0.683877,
Train 408, loss: 0.302599, train acc: 0.869848, 
Test 408, loss: 0.751368, test acc: 0.665266,
Train 409, loss: 0.304859, train acc: 0.869932, 
Test 409, loss: 0.717159, test acc: 0.704288,
Train 410, loss: 0.296536, train acc: 0.871664, 
Test 410, loss: 0.794948, test acc: 0.692539,
Train 411, loss: 0.297984, train acc: 0.872128, 
Test 411, loss: 0.739193, test acc: 0.695712,
Train 412, loss: 0.304878, train acc: 0.870988, 
Test 412, loss: 0.750505, test acc: 0.692967,
Train 413, loss: 0.300879, train acc: 0.872255, 
Test 413, loss: 0.667981, test acc: 0.706261,
Train 414, loss: 0.298689, train acc: 0.872889, 
Test 414, loss: 0.737309, test acc: 0.692967,
Train 415, loss: 0.297484, train acc: 0.873437, 
Test 415, loss: 0.813776, test acc: 0.666038,
Train 416, loss: 0.297711, train acc: 0.873649, 
Test 416, loss: 0.726013, test acc: 0.699314,
Train 417, loss: 0.297518, train acc: 0.872466, 
Test 417, loss: 0.716778, test acc: 0.695798,
Train 418, loss: 0.291680, train acc: 0.875084, 
Test 418, loss: 0.869807, test acc: 0.675815,
Train 419, loss: 0.293267, train acc: 0.873100, 
Test 419, loss: 0.827214, test acc: 0.688336,
Train 420, loss: 0.292731, train acc: 0.874958, 
Test 420, loss: 0.728382, test acc: 0.701544,
Train 421, loss: 0.286621, train acc: 0.877998, 
Test 421, loss: 0.781563, test acc: 0.681132,
Train 422, loss: 0.291661, train acc: 0.876014, 
Test 422, loss: 0.775022, test acc: 0.679160,
Train 423, loss: 0.285737, train acc: 0.879096, 
Test 423, loss: 0.776842, test acc: 0.694425,
Train 424, loss: 0.286931, train acc: 0.877323, 
Test 424, loss: 0.751284, test acc: 0.700943,
Train 425, loss: 0.287667, train acc: 0.876140, 
Test 425, loss: 0.791438, test acc: 0.670755,
Train 426, loss: 0.282740, train acc: 0.881081, 
Test 426, loss: 0.787812, test acc: 0.689451,
Train 427, loss: 0.286640, train acc: 0.878083, 
Test 427, loss: 0.756033, test acc: 0.684820,
Train 428, loss: 0.287937, train acc: 0.878083, 
Test 428, loss: 0.922153, test acc: 0.662693,
Train 429, loss: 0.279435, train acc: 0.880701, 
Test 429, loss: 0.821868, test acc: 0.684048,
Train 430, loss: 0.280218, train acc: 0.878547, 
Test 430, loss: 0.813003, test acc: 0.666381,
Train 431, loss: 0.281267, train acc: 0.880532, 
Test 431, loss: 0.796809, test acc: 0.692539,
Train 432, loss: 0.283572, train acc: 0.880405, 
Test 432, loss: 0.781322, test acc: 0.684305,
Train 433, loss: 0.277704, train acc: 0.879265, 
Test 433, loss: 0.739316, test acc: 0.696998,
Train 434, loss: 0.280246, train acc: 0.881757, 
Test 434, loss: 0.919082, test acc: 0.659348,
Train 435, loss: 0.278924, train acc: 0.881208, 
Test 435, loss: 0.753234, test acc: 0.686535,
Train 436, loss: 0.276654, train acc: 0.882939, 
Test 436, loss: 0.798083, test acc: 0.682590,
Train 437, loss: 0.274481, train acc: 0.882855, 
Test 437, loss: 0.870828, test acc: 0.665094,
Train 438, loss: 0.276676, train acc: 0.884797, 
Test 438, loss: 0.817748, test acc: 0.676587,
Train 439, loss: 0.277204, train acc: 0.885726, 
Test 439, loss: 0.842511, test acc: 0.670669,
Train 440, loss: 0.275101, train acc: 0.886613, 
Test 440, loss: 0.783343, test acc: 0.679588,
Train 441, loss: 0.268545, train acc: 0.887035, 
Test 441, loss: 0.809294, test acc: 0.684305,
Train 442, loss: 0.268634, train acc: 0.886529, 
Test 442, loss: 0.755104, test acc: 0.681904,
Train 443, loss: 0.271326, train acc: 0.884037, 
Test 443, loss: 0.870877, test acc: 0.677358,
Train 444, loss: 0.269406, train acc: 0.885051, 
Test 444, loss: 0.835881, test acc: 0.686707,
Train 445, loss: 0.266261, train acc: 0.889654, 
Test 445, loss: 0.861003, test acc: 0.683619,
Train 446, loss: 0.265671, train acc: 0.889062, 
Test 446, loss: 0.781970, test acc: 0.693310,
Train 447, loss: 0.266848, train acc: 0.887711, 
Test 447, loss: 0.840895, test acc: 0.673070,
Train 448, loss: 0.264723, train acc: 0.887247, 
Test 448, loss: 0.835492, test acc: 0.689108,
Train 449, loss: 0.267029, train acc: 0.887289, 
Test 449, loss: 0.807125, test acc: 0.694940,
Train 450, loss: 0.265833, train acc: 0.886867, 
Test 450, loss: 0.841674, test acc: 0.686021,
Train 451, loss: 0.264569, train acc: 0.886909, 
Test 451, loss: 0.793272, test acc: 0.694768,
Train 452, loss: 0.258548, train acc: 0.889949, 
Test 452, loss: 0.821709, test acc: 0.685678,
Train 453, loss: 0.260867, train acc: 0.890878, 
Test 453, loss: 0.818594, test acc: 0.686535,
Train 454, loss: 0.259280, train acc: 0.889738, 
Test 454, loss: 0.823213, test acc: 0.696226,
Train 455, loss: 0.259334, train acc: 0.890625, 
Test 455, loss: 0.873650, test acc: 0.687307,
Train 456, loss: 0.253593, train acc: 0.894299, 
Test 456, loss: 0.800585, test acc: 0.692453,
Train 457, loss: 0.255329, train acc: 0.893032, 
Test 457, loss: 0.913870, test acc: 0.670583,
Train 458, loss: 0.253717, train acc: 0.893666, 
Test 458, loss: 0.891208, test acc: 0.683019,
Train 459, loss: 0.255156, train acc: 0.893243, 
Test 459, loss: 0.868954, test acc: 0.683791,
Train 460, loss: 0.253871, train acc: 0.894003, 
Test 460, loss: 0.919229, test acc: 0.689280,
Train 461, loss: 0.251218, train acc: 0.893834, 
Test 461, loss: 0.903966, test acc: 0.681990,
Train 462, loss: 0.250086, train acc: 0.896537, 
Test 462, loss: 0.873713, test acc: 0.683877,
Train 463, loss: 0.251882, train acc: 0.894215, 
Test 463, loss: 0.848023, test acc: 0.686192,
Train 464, loss: 0.248627, train acc: 0.894215, 
Test 464, loss: 0.933017, test acc: 0.681389,
Train 465, loss: 0.251888, train acc: 0.894679, 
Test 465, loss: 0.868690, test acc: 0.688679,
Train 466, loss: 0.246493, train acc: 0.896579, 
Test 466, loss: 0.949895, test acc: 0.669897,
Train 467, loss: 0.241547, train acc: 0.898775, 
Test 467, loss: 0.873962, test acc: 0.686792,
Train 468, loss: 0.247730, train acc: 0.897635, 
Test 468, loss: 0.827038, test acc: 0.700343,
Train 469, loss: 0.244499, train acc: 0.898775, 
Test 469, loss: 0.899149, test acc: 0.686192,
Train 470, loss: 0.243479, train acc: 0.897002, 
Test 470, loss: 0.870202, test acc: 0.685163,
Train 471, loss: 0.243705, train acc: 0.898395, 
Test 471, loss: 0.915641, test acc: 0.679674,
Train 472, loss: 0.242929, train acc: 0.898395, 
Test 472, loss: 0.992419, test acc: 0.677873,
Train 473, loss: 0.245677, train acc: 0.897002, 
Test 473, loss: 0.822006, test acc: 0.688937,
Train 474, loss: 0.239041, train acc: 0.900211, 
Test 474, loss: 0.909825, test acc: 0.685334,
Train 475, loss: 0.242107, train acc: 0.898311, 
Test 475, loss: 0.901561, test acc: 0.679417,
Train 476, loss: 0.240107, train acc: 0.899071, 
Test 476, loss: 0.917268, test acc: 0.680875,
Train 477, loss: 0.240082, train acc: 0.898818, 
Test 477, loss: 0.879527, test acc: 0.681990,
Train 478, loss: 0.242561, train acc: 0.898691, 
Test 478, loss: 0.895473, test acc: 0.680961,
Train 479, loss: 0.238498, train acc: 0.900507, 
Test 479, loss: 0.957383, test acc: 0.674357,
Train 480, loss: 0.230377, train acc: 0.904350, 
Test 480, loss: 0.855474, test acc: 0.693654,
Train 481, loss: 0.239576, train acc: 0.899916, 
Test 481, loss: 0.871139, test acc: 0.677187,
Train 482, loss: 0.237001, train acc: 0.902280, 
Test 482, loss: 0.935492, test acc: 0.678816,
Train 483, loss: 0.231500, train acc: 0.903125, 
Test 483, loss: 0.917425, test acc: 0.683791,
Train 484, loss: 0.230863, train acc: 0.903547, 
Test 484, loss: 0.953852, test acc: 0.667496,
Train 485, loss: 0.233156, train acc: 0.901605, 
Test 485, loss: 0.918026, test acc: 0.676072,
Train 486, loss: 0.232294, train acc: 0.902323, 
Test 486, loss: 0.876387, test acc: 0.685763,
Train 487, loss: 0.229841, train acc: 0.901985, 
Test 487, loss: 0.901444, test acc: 0.669383,
Train 488, loss: 0.232973, train acc: 0.902914, 
Test 488, loss: 0.872110, test acc: 0.690480,
Train 489, loss: 0.231801, train acc: 0.901900, 
Test 489, loss: 0.879175, test acc: 0.671955,
Train 490, loss: 0.234385, train acc: 0.903463, 
Test 490, loss: 0.877715, test acc: 0.688336,
Train 491, loss: 0.232426, train acc: 0.903421, 
Test 491, loss: 0.965463, test acc: 0.675815,
Train 492, loss: 0.229312, train acc: 0.904561, 
Test 492, loss: 0.944628, test acc: 0.676158,
Train 493, loss: 0.230286, train acc: 0.904307, 
Test 493, loss: 0.942622, test acc: 0.681904,
Train 494, loss: 0.233243, train acc: 0.901816, 
Test 494, loss: 0.941073, test acc: 0.687822,
Train 495, loss: 0.232319, train acc: 0.903209, 
Test 495, loss: 0.946114, test acc: 0.682419,
Train 496, loss: 0.229413, train acc: 0.904603, 
Test 496, loss: 0.914915, test acc: 0.683962,
Train 497, loss: 0.230799, train acc: 0.902069, 
Test 497, loss: 1.042368, test acc: 0.678645,
Train 498, loss: 0.228412, train acc: 0.907348, 
Test 498, loss: 0.977336, test acc: 0.674957,
Train 499, loss: 0.230462, train acc: 0.903885, 
Test 499, loss: 0.928001, test acc: 0.683791,
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 300
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.04
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.333175, train acc: 0.897754, 
Test 0, loss: 0.453804, test acc: 0.845355,
Max Acc:0.845355
Train 1, loss: 0.267490, train acc: 0.906156, 
Test 1, loss: 0.619802, test acc: 0.832432,
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 300
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.04
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 300
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.04
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 1.313126, train acc: 0.647755, 
Test 0, loss: 1.041272, test acc: 0.441322,
Max Acc:0.441322
Train 1, loss: 0.589345, train acc: 0.718571, 
Test 1, loss: 1.449202, test acc: 0.335537,
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 300
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.04
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 300
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.04
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.975047, train acc: 0.655335, 
Test 0, loss: 0.540251, test acc: 0.752778,
Max Acc:0.752778
Train 1, loss: 0.600539, train acc: 0.697519, 
Test 1, loss: 0.507456, test acc: 0.760859,
Max Acc:0.760859
Train 2, loss: 0.570061, train acc: 0.723573, 
Test 2, loss: 0.518073, test acc: 0.751263,
Train 3, loss: 0.573943, train acc: 0.711911, 
Test 3, loss: 0.502422, test acc: 0.764141,
Max Acc:0.764141
Train 4, loss: 0.564797, train acc: 0.726923, 
Test 4, loss: 0.494594, test acc: 0.742929,
Train 5, loss: 0.561216, train acc: 0.726799, 
Test 5, loss: 0.514610, test acc: 0.754293,
Train 6, loss: 0.560588, train acc: 0.729156, 
Test 6, loss: 0.499821, test acc: 0.776010,
Max Acc:0.776010
Train 7, loss: 0.546315, train acc: 0.735980, 
Test 7, loss: 0.490464, test acc: 0.766162,
Train 8, loss: 0.563961, train acc: 0.723449, 
Test 8, loss: 0.488246, test acc: 0.761616,
Train 9, loss: 0.552270, train acc: 0.730893, 
Test 9, loss: 0.501543, test acc: 0.771717,
Train 10, loss: 0.548519, train acc: 0.729032, 
Test 10, loss: 0.508801, test acc: 0.758333,
Train 11, loss: 0.549236, train acc: 0.733002, 
Test 11, loss: 0.491275, test acc: 0.769949,
Train 12, loss: 0.535994, train acc: 0.739826, 
Test 12, loss: 0.482545, test acc: 0.762879,
Train 13, loss: 0.549231, train acc: 0.731266, 
Test 13, loss: 0.512680, test acc: 0.757323,
Train 14, loss: 0.549918, train acc: 0.727171, 
Test 14, loss: 0.490641, test acc: 0.765657,
Train 15, loss: 0.538892, train acc: 0.731017, 
Test 15, loss: 0.479474, test acc: 0.763636,
Train 16, loss: 0.537661, train acc: 0.732754, 
Test 16, loss: 0.497217, test acc: 0.771970,
Train 17, loss: 0.544146, train acc: 0.732506, 
Test 17, loss: 0.489075, test acc: 0.760101,
Train 18, loss: 0.538128, train acc: 0.731886, 
Test 18, loss: 0.482133, test acc: 0.768182,
Train 19, loss: 0.538414, train acc: 0.736849, 
Test 19, loss: 0.486232, test acc: 0.761111,
Train 20, loss: 0.526850, train acc: 0.745161, 
Test 20, loss: 0.490284, test acc: 0.769697,
Train 21, loss: 0.532040, train acc: 0.740819, 
Test 21, loss: 0.466180, test acc: 0.768434,
Train 22, loss: 0.528222, train acc: 0.739578, 
Test 22, loss: 0.465447, test acc: 0.772980,
Train 23, loss: 0.530856, train acc: 0.743424, 
Test 23, loss: 0.499482, test acc: 0.763384,
Train 24, loss: 0.526192, train acc: 0.745409, 
Test 24, loss: 0.480506, test acc: 0.776010,
Max Acc:0.776010
Train 25, loss: 0.529139, train acc: 0.743548, 
Test 25, loss: 0.478953, test acc: 0.783333,
Max Acc:0.783333
Train 26, loss: 0.516113, train acc: 0.750124, 
Test 26, loss: 0.496124, test acc: 0.754545,
Train 27, loss: 0.522184, train acc: 0.747395, 
Test 27, loss: 0.485123, test acc: 0.755303,
Train 28, loss: 0.520029, train acc: 0.746650, 
Test 28, loss: 0.476669, test acc: 0.750758,
Train 29, loss: 0.506344, train acc: 0.753474, 
Test 29, loss: 0.467016, test acc: 0.766162,
Train 30, loss: 0.517143, train acc: 0.754963, 
Test 30, loss: 0.458696, test acc: 0.767424,
Train 31, loss: 0.516565, train acc: 0.748635, 
Test 31, loss: 0.492440, test acc: 0.754293,
Train 32, loss: 0.502190, train acc: 0.753722, 
Test 32, loss: 0.444805, test acc: 0.786364,
Max Acc:0.786364
Train 33, loss: 0.509000, train acc: 0.747767, 
Test 33, loss: 0.466978, test acc: 0.768434,
Train 34, loss: 0.495731, train acc: 0.763027, 
Test 34, loss: 0.499964, test acc: 0.733586,
Train 35, loss: 0.499535, train acc: 0.762655, 
Test 35, loss: 0.533296, test acc: 0.719697,
Train 36, loss: 0.498740, train acc: 0.759677, 
Test 36, loss: 0.476513, test acc: 0.750000,
Train 37, loss: 0.488983, train acc: 0.767618, 
Test 37, loss: 0.428051, test acc: 0.802273,
Max Acc:0.802273
Train 38, loss: 0.499164, train acc: 0.759677, 
Test 38, loss: 0.507119, test acc: 0.750253,
Train 39, loss: 0.489875, train acc: 0.765881, 
Test 39, loss: 0.449916, test acc: 0.797475,
Train 40, loss: 0.480725, train acc: 0.771340, 
Test 40, loss: 0.856852, test acc: 0.678535,
Train 41, loss: 0.484456, train acc: 0.770596, 
Test 41, loss: 0.410327, test acc: 0.814646,
Max Acc:0.814646
Train 42, loss: 0.479514, train acc: 0.774318, 
Test 42, loss: 0.405117, test acc: 0.817677,
Max Acc:0.817677
Train 43, loss: 0.475726, train acc: 0.776675, 
Test 43, loss: 0.411535, test acc: 0.809596,
Train 44, loss: 0.471169, train acc: 0.779404, 
Test 44, loss: 0.448449, test acc: 0.787374,
Train 45, loss: 0.466584, train acc: 0.779404, 
Test 45, loss: 0.477530, test acc: 0.753535,
Train 46, loss: 0.455827, train acc: 0.788834, 
Test 46, loss: 0.386474, test acc: 0.830303,
Max Acc:0.830303
Train 47, loss: 0.461743, train acc: 0.790074, 
Test 47, loss: 0.483131, test acc: 0.768939,
Train 48, loss: 0.457854, train acc: 0.783623, 
Test 48, loss: 0.411327, test acc: 0.810859,
Train 49, loss: 0.452045, train acc: 0.791439, 
Test 49, loss: 0.450500, test acc: 0.767424,
Train 50, loss: 0.451087, train acc: 0.787965, 
Test 50, loss: 0.449561, test acc: 0.806566,
Train 51, loss: 0.453706, train acc: 0.783375, 
Test 51, loss: 0.393546, test acc: 0.825505,
Train 52, loss: 0.452073, train acc: 0.785856, 
Test 52, loss: 0.490685, test acc: 0.749747,
Train 53, loss: 0.448722, train acc: 0.790199, 
Test 53, loss: 0.382317, test acc: 0.827525,
Train 54, loss: 0.449092, train acc: 0.793300, 
Test 54, loss: 0.428602, test acc: 0.791919,
Train 55, loss: 0.442280, train acc: 0.792184, 
Test 55, loss: 0.389361, test acc: 0.831061,
Max Acc:0.831061
Train 56, loss: 0.439571, train acc: 0.794665, 
Test 56, loss: 0.372830, test acc: 0.848485,
Max Acc:0.848485
Train 57, loss: 0.429045, train acc: 0.798139, 
Test 57, loss: 0.407390, test acc: 0.815152,
Train 58, loss: 0.430418, train acc: 0.804094, 
Test 58, loss: 0.409878, test acc: 0.803030,
Train 59, loss: 0.431779, train acc: 0.795533, 
Test 59, loss: 0.390765, test acc: 0.814646,
Train 60, loss: 0.429423, train acc: 0.800248, 
Test 60, loss: 0.382876, test acc: 0.832323,
Train 61, loss: 0.429533, train acc: 0.804591, 
Test 61, loss: 0.374796, test acc: 0.828283,
Train 62, loss: 0.420279, train acc: 0.806948, 
Test 62, loss: 0.367151, test acc: 0.837879,
Train 63, loss: 0.416403, train acc: 0.808561, 
Test 63, loss: 0.399353, test acc: 0.820455,
Train 64, loss: 0.428629, train acc: 0.803846, 
Test 64, loss: 0.364350, test acc: 0.837121,
Train 65, loss: 0.421205, train acc: 0.807320, 
Test 65, loss: 0.345546, test acc: 0.849242,
Max Acc:0.849242
Train 66, loss: 0.407858, train acc: 0.816873, 
Test 66, loss: 0.371684, test acc: 0.824747,
Train 67, loss: 0.410690, train acc: 0.810918, 
Test 67, loss: 0.374727, test acc: 0.843939,
Train 68, loss: 0.407830, train acc: 0.815509, 
Test 68, loss: 0.410237, test acc: 0.822727,
Train 69, loss: 0.402739, train acc: 0.818610, 
Test 69, loss: 0.330633, test acc: 0.865404,
Max Acc:0.865404
Train 70, loss: 0.410132, train acc: 0.812407, 
Test 70, loss: 0.449400, test acc: 0.766919,
Train 71, loss: 0.406559, train acc: 0.819727, 
Test 71, loss: 0.435242, test acc: 0.770202,
Train 72, loss: 0.401313, train acc: 0.818238, 
Test 72, loss: 0.403016, test acc: 0.803788,
Train 73, loss: 0.397248, train acc: 0.819603, 
Test 73, loss: 0.384223, test acc: 0.818182,
Train 74, loss: 0.398655, train acc: 0.821216, 
Test 74, loss: 0.385689, test acc: 0.830303,
Train 75, loss: 0.402922, train acc: 0.819107, 
Test 75, loss: 0.349180, test acc: 0.844697,
Train 76, loss: 0.401750, train acc: 0.822457, 
Test 76, loss: 0.376430, test acc: 0.840152,
Train 77, loss: 0.398203, train acc: 0.823325, 
Test 77, loss: 0.332503, test acc: 0.847475,
Train 78, loss: 0.397572, train acc: 0.824566, 
Test 78, loss: 0.347203, test acc: 0.853030,
Train 79, loss: 0.385034, train acc: 0.831886, 
Test 79, loss: 0.331560, test acc: 0.860606,
Train 80, loss: 0.384993, train acc: 0.830645, 
Test 80, loss: 0.415557, test acc: 0.822475,
Train 81, loss: 0.392508, train acc: 0.825931, 
Test 81, loss: 0.332444, test acc: 0.859596,
Train 82, loss: 0.386027, train acc: 0.828164, 
Test 82, loss: 0.409149, test acc: 0.813384,
Train 83, loss: 0.382998, train acc: 0.828288, 
Test 83, loss: 0.345895, test acc: 0.855051,
Train 84, loss: 0.382439, train acc: 0.834119, 
Test 84, loss: 0.384191, test acc: 0.829545,
Train 85, loss: 0.371274, train acc: 0.837717, 
Test 85, loss: 0.346563, test acc: 0.844192,
Train 86, loss: 0.386221, train acc: 0.828536, 
Test 86, loss: 0.305095, test acc: 0.872727,
Max Acc:0.872727
Train 87, loss: 0.377387, train acc: 0.834243, 
Test 87, loss: 0.339077, test acc: 0.857071,
Train 88, loss: 0.378150, train acc: 0.830397, 
Test 88, loss: 0.336037, test acc: 0.864899,
Train 89, loss: 0.377543, train acc: 0.831017, 
Test 89, loss: 0.400359, test acc: 0.817929,
Train 90, loss: 0.367714, train acc: 0.841935, 
Test 90, loss: 0.337303, test acc: 0.864394,
Train 91, loss: 0.358508, train acc: 0.847146, 
Test 91, loss: 0.317159, test acc: 0.874495,
Max Acc:0.874495
Train 92, loss: 0.367802, train acc: 0.841315, 
Test 92, loss: 0.308831, test acc: 0.875505,
Max Acc:0.875505
Train 93, loss: 0.352323, train acc: 0.848883, 
Test 93, loss: 0.301873, test acc: 0.867929,
Train 94, loss: 0.356833, train acc: 0.847270, 
Test 94, loss: 0.331824, test acc: 0.849747,
Train 95, loss: 0.349313, train acc: 0.847395, 
Test 95, loss: 0.312682, test acc: 0.865152,
Train 96, loss: 0.344981, train acc: 0.850124, 
Test 96, loss: 0.298798, test acc: 0.877525,
Max Acc:0.877525
Train 97, loss: 0.343862, train acc: 0.851985, 
Test 97, loss: 0.307841, test acc: 0.865909,
Train 98, loss: 0.341379, train acc: 0.852978, 
Test 98, loss: 0.326818, test acc: 0.856818,
Train 99, loss: 0.357573, train acc: 0.847146, 
Test 99, loss: 0.287986, test acc: 0.877020,
Train 100, loss: 0.342828, train acc: 0.850124, 
Test 100, loss: 0.286045, test acc: 0.880556,
Max Acc:0.880556
Train 101, loss: 0.341484, train acc: 0.850868, 
Test 101, loss: 0.325042, test acc: 0.858838,
Train 102, loss: 0.332892, train acc: 0.857568, 
Test 102, loss: 0.296407, test acc: 0.876768,
Train 103, loss: 0.339809, train acc: 0.857692, 
Test 103, loss: 0.327924, test acc: 0.869192,
Train 104, loss: 0.339129, train acc: 0.854094, 
Test 104, loss: 0.294660, test acc: 0.883333,
Max Acc:0.883333
Train 105, loss: 0.336038, train acc: 0.852233, 
Test 105, loss: 0.359631, test acc: 0.840657,
Train 106, loss: 0.335023, train acc: 0.855087, 
Test 106, loss: 0.295767, test acc: 0.885606,
Max Acc:0.885606
Train 107, loss: 0.339723, train acc: 0.855211, 
Test 107, loss: 0.297810, test acc: 0.879293,
Train 108, loss: 0.333528, train acc: 0.859305, 
Test 108, loss: 0.270959, test acc: 0.891667,
Max Acc:0.891667
Train 109, loss: 0.322645, train acc: 0.862035, 
Test 109, loss: 0.325443, test acc: 0.863131,
Train 110, loss: 0.317124, train acc: 0.863151, 
Test 110, loss: 0.327591, test acc: 0.849747,
Train 111, loss: 0.328172, train acc: 0.853598, 
Test 111, loss: 0.298707, test acc: 0.880556,
Train 112, loss: 0.324860, train acc: 0.861166, 
Test 112, loss: 0.309862, test acc: 0.867172,
Train 113, loss: 0.333141, train acc: 0.854094, 
Test 113, loss: 0.325456, test acc: 0.859343,
Train 114, loss: 0.322399, train acc: 0.863648, 
Test 114, loss: 0.341680, test acc: 0.845202,
Train 115, loss: 0.319475, train acc: 0.860174, 
Test 115, loss: 0.299216, test acc: 0.876263,
Train 116, loss: 0.328674, train acc: 0.859926, 
Test 116, loss: 0.272201, test acc: 0.887121,
Train 117, loss: 0.327303, train acc: 0.857444, 
Test 117, loss: 0.299272, test acc: 0.876768,
Train 118, loss: 0.335158, train acc: 0.855335, 
Test 118, loss: 0.287894, test acc: 0.876263,
Train 119, loss: 0.325834, train acc: 0.860174, 
Test 119, loss: 0.261358, test acc: 0.890152,
Train 120, loss: 0.312270, train acc: 0.866129, 
Test 120, loss: 0.297154, test acc: 0.883081,
Train 121, loss: 0.310187, train acc: 0.869603, 
Test 121, loss: 0.300895, test acc: 0.862626,
Train 122, loss: 0.313712, train acc: 0.867618, 
Test 122, loss: 0.301199, test acc: 0.871465,
Train 123, loss: 0.321715, train acc: 0.863648, 
Test 123, loss: 0.316644, test acc: 0.866919,
Train 124, loss: 0.329641, train acc: 0.857692, 
Test 124, loss: 0.270456, test acc: 0.893939,
Max Acc:0.893939
Train 125, loss: 0.340416, train acc: 0.850000, 
Test 125, loss: 0.285931, test acc: 0.882828,
Train 126, loss: 0.316402, train acc: 0.862159, 
Test 126, loss: 0.283096, test acc: 0.883838,
Train 127, loss: 0.316119, train acc: 0.866129, 
Test 127, loss: 0.264839, test acc: 0.892424,
Train 128, loss: 0.323605, train acc: 0.862407, 
Test 128, loss: 0.283867, test acc: 0.886616,
Train 129, loss: 0.309926, train acc: 0.867990, 
Test 129, loss: 0.264795, test acc: 0.893182,
Train 130, loss: 0.318945, train acc: 0.862407, 
Test 130, loss: 0.286111, test acc: 0.880303,
Train 131, loss: 0.311142, train acc: 0.862903, 
Test 131, loss: 0.334388, test acc: 0.847727,
Train 132, loss: 0.311433, train acc: 0.869107, 
Test 132, loss: 0.315719, test acc: 0.855556,
Train 133, loss: 0.304047, train acc: 0.869231, 
Test 133, loss: 0.295356, test acc: 0.875758,
Train 134, loss: 0.316892, train acc: 0.866749, 
Test 134, loss: 0.305994, test acc: 0.866162,
Train 135, loss: 0.309401, train acc: 0.869355, 
Test 135, loss: 0.357051, test acc: 0.835101,
Train 136, loss: 0.296368, train acc: 0.876179, 
Test 136, loss: 0.268853, test acc: 0.896970,
Max Acc:0.896970
Train 137, loss: 0.296824, train acc: 0.874069, 
Test 137, loss: 0.292986, test acc: 0.871717,
Train 138, loss: 0.307494, train acc: 0.865757, 
Test 138, loss: 0.263541, test acc: 0.897222,
Max Acc:0.897222
Train 139, loss: 0.300670, train acc: 0.870099, 
Test 139, loss: 0.269959, test acc: 0.883333,
Train 140, loss: 0.301885, train acc: 0.871092, 
Test 140, loss: 0.256851, test acc: 0.899242,
Max Acc:0.899242
Train 141, loss: 0.302397, train acc: 0.871216, 
Test 141, loss: 0.320793, test acc: 0.861616,
Train 142, loss: 0.294230, train acc: 0.876427, 
Test 142, loss: 0.242114, test acc: 0.904040,
Max Acc:0.904040
Train 143, loss: 0.287157, train acc: 0.878164, 
Test 143, loss: 0.256811, test acc: 0.895455,
Train 144, loss: 0.299750, train acc: 0.874690, 
Test 144, loss: 0.248914, test acc: 0.903030,
Train 145, loss: 0.290835, train acc: 0.877792, 
Test 145, loss: 0.248791, test acc: 0.892929,
Train 146, loss: 0.282124, train acc: 0.880521, 
Test 146, loss: 0.237089, test acc: 0.905051,
Max Acc:0.905051
Train 147, loss: 0.293325, train acc: 0.875434, 
Test 147, loss: 0.250725, test acc: 0.899747,
Train 148, loss: 0.289931, train acc: 0.875558, 
Test 148, loss: 0.288422, test acc: 0.875000,
Train 149, loss: 0.296719, train acc: 0.871836, 
Test 149, loss: 0.235533, test acc: 0.901010,
Train 150, loss: 0.281779, train acc: 0.881266, 
Test 150, loss: 0.254115, test acc: 0.895455,
Train 151, loss: 0.284579, train acc: 0.880893, 
Test 151, loss: 0.254542, test acc: 0.895707,
Train 152, loss: 0.283339, train acc: 0.877295, 
Test 152, loss: 0.254801, test acc: 0.895960,
Train 153, loss: 0.286173, train acc: 0.878288, 
Test 153, loss: 0.273887, test acc: 0.882828,
Train 154, loss: 0.282504, train acc: 0.880893, 
Test 154, loss: 0.325384, test acc: 0.857576,
Train 155, loss: 0.273726, train acc: 0.885608, 
Test 155, loss: 0.271440, test acc: 0.886111,
Train 156, loss: 0.282147, train acc: 0.879901, 
Test 156, loss: 0.264971, test acc: 0.900000,
Train 157, loss: 0.275887, train acc: 0.881141, 
Test 157, loss: 0.247438, test acc: 0.902273,
Train 158, loss: 0.271192, train acc: 0.885732, 
Test 158, loss: 0.256865, test acc: 0.895202,
Train 159, loss: 0.288151, train acc: 0.880521, 
Test 159, loss: 0.232763, test acc: 0.907071,
Max Acc:0.907071
Train 160, loss: 0.272300, train acc: 0.885112, 
Test 160, loss: 0.337025, test acc: 0.848232,
Train 161, loss: 0.264896, train acc: 0.889578, 
Test 161, loss: 0.252207, test acc: 0.902273,
Train 162, loss: 0.285774, train acc: 0.878164, 
Test 162, loss: 0.260813, test acc: 0.899495,
Train 163, loss: 0.279965, train acc: 0.884491, 
Test 163, loss: 0.303369, test acc: 0.864646,
Train 164, loss: 0.271481, train acc: 0.885236, 
Test 164, loss: 0.248787, test acc: 0.897980,
Train 165, loss: 0.267770, train acc: 0.886104, 
Test 165, loss: 0.280993, test acc: 0.885859,
Train 166, loss: 0.273113, train acc: 0.881390, 
Test 166, loss: 0.250484, test acc: 0.898485,
Train 167, loss: 0.264116, train acc: 0.888834, 
Test 167, loss: 0.237643, test acc: 0.896970,
Train 168, loss: 0.264135, train acc: 0.886352, 
Test 168, loss: 0.254917, test acc: 0.893939,
Train 169, loss: 0.259851, train acc: 0.891811, 
Test 169, loss: 0.249754, test acc: 0.899242,
Train 170, loss: 0.269228, train acc: 0.886600, 
Test 170, loss: 0.253505, test acc: 0.895455,
Train 171, loss: 0.265994, train acc: 0.887097, 
Test 171, loss: 0.231516, test acc: 0.904545,
Train 172, loss: 0.266953, train acc: 0.889330, 
Test 172, loss: 0.252929, test acc: 0.890404,
Train 173, loss: 0.258688, train acc: 0.889330, 
Test 173, loss: 0.250185, test acc: 0.898737,
Train 174, loss: 0.256646, train acc: 0.895906, 
Test 174, loss: 0.235653, test acc: 0.905303,
Train 175, loss: 0.257341, train acc: 0.892928, 
Test 175, loss: 0.232250, test acc: 0.908838,
Max Acc:0.908838
Train 176, loss: 0.259870, train acc: 0.888089, 
Test 176, loss: 0.246788, test acc: 0.899495,
Train 177, loss: 0.248770, train acc: 0.895037, 
Test 177, loss: 0.248522, test acc: 0.894444,
Train 178, loss: 0.257061, train acc: 0.894665, 
Test 178, loss: 0.240724, test acc: 0.907071,
Train 179, loss: 0.251816, train acc: 0.893548, 
Test 179, loss: 0.250427, test acc: 0.895960,
Train 180, loss: 0.251831, train acc: 0.892556, 
Test 180, loss: 0.226389, test acc: 0.910606,
Max Acc:0.910606
Train 181, loss: 0.254133, train acc: 0.891563, 
Test 181, loss: 0.259070, test acc: 0.893434,
Train 182, loss: 0.267808, train acc: 0.888337, 
Test 182, loss: 0.226812, test acc: 0.910101,
Train 183, loss: 0.247951, train acc: 0.897891, 
Test 183, loss: 0.227047, test acc: 0.907828,
Train 184, loss: 0.251473, train acc: 0.893176, 
Test 184, loss: 0.280075, test acc: 0.886111,
Train 185, loss: 0.250556, train acc: 0.893052, 
Test 185, loss: 0.263412, test acc: 0.894949,
Train 186, loss: 0.249008, train acc: 0.895533, 
Test 186, loss: 0.263140, test acc: 0.889141,
Train 187, loss: 0.251676, train acc: 0.893176, 
Test 187, loss: 0.244817, test acc: 0.901768,
Train 188, loss: 0.254855, train acc: 0.892556, 
Test 188, loss: 0.232696, test acc: 0.908838,
Train 189, loss: 0.242752, train acc: 0.899628, 
Test 189, loss: 0.228578, test acc: 0.908081,
Train 190, loss: 0.245789, train acc: 0.896030, 
Test 190, loss: 0.249784, test acc: 0.905051,
Train 191, loss: 0.244766, train acc: 0.895906, 
Test 191, loss: 0.259301, test acc: 0.894192,
Train 192, loss: 0.241541, train acc: 0.898759, 
Test 192, loss: 0.246945, test acc: 0.901010,
Train 193, loss: 0.239261, train acc: 0.899132, 
Test 193, loss: 0.223388, test acc: 0.908838,
Train 194, loss: 0.238780, train acc: 0.901241, 
Test 194, loss: 0.239202, test acc: 0.896212,
Train 195, loss: 0.237247, train acc: 0.900372, 
Test 195, loss: 0.262081, test acc: 0.888889,
Train 196, loss: 0.240431, train acc: 0.902233, 
Test 196, loss: 0.236962, test acc: 0.898990,
Train 197, loss: 0.233340, train acc: 0.904094, 
Test 197, loss: 0.225319, test acc: 0.908586,
Train 198, loss: 0.243739, train acc: 0.898635, 
Test 198, loss: 0.227415, test acc: 0.907323,
Train 199, loss: 0.224881, train acc: 0.908685, 
Test 199, loss: 0.234953, test acc: 0.904798,
Train 200, loss: 0.231371, train acc: 0.905955, 
Test 200, loss: 0.251918, test acc: 0.901263,
Train 201, loss: 0.230153, train acc: 0.907444, 
Test 201, loss: 0.222569, test acc: 0.917677,
Max Acc:0.917677
Train 202, loss: 0.231376, train acc: 0.901985, 
Test 202, loss: 0.254265, test acc: 0.898232,
Train 203, loss: 0.232382, train acc: 0.907568, 
Test 203, loss: 0.231256, test acc: 0.906818,
Train 204, loss: 0.233394, train acc: 0.904591, 
Test 204, loss: 0.214920, test acc: 0.914646,
Train 205, loss: 0.227155, train acc: 0.905707, 
Test 205, loss: 0.214933, test acc: 0.916162,
Train 206, loss: 0.225568, train acc: 0.908561, 
Test 206, loss: 0.213322, test acc: 0.914394,
Train 207, loss: 0.231301, train acc: 0.903598, 
Test 207, loss: 0.210573, test acc: 0.911616,
Train 208, loss: 0.223123, train acc: 0.908065, 
Test 208, loss: 0.214780, test acc: 0.914646,
Train 209, loss: 0.219086, train acc: 0.907072, 
Test 209, loss: 0.229526, test acc: 0.906818,
Train 210, loss: 0.215928, train acc: 0.909429, 
Test 210, loss: 0.227055, test acc: 0.910606,
Train 211, loss: 0.215560, train acc: 0.907940, 
Test 211, loss: 0.214415, test acc: 0.918687,
Max Acc:0.918687
Train 212, loss: 0.230900, train acc: 0.905335, 
Test 212, loss: 0.217449, test acc: 0.914899,
Train 213, loss: 0.213522, train acc: 0.910794, 
Test 213, loss: 0.209149, test acc: 0.918687,
Max Acc:0.918687
Train 214, loss: 0.217051, train acc: 0.910298, 
Test 214, loss: 0.211313, test acc: 0.915909,
Train 215, loss: 0.216321, train acc: 0.909429, 
Test 215, loss: 0.220576, test acc: 0.916162,
Train 216, loss: 0.210214, train acc: 0.912903, 
Test 216, loss: 0.211525, test acc: 0.919192,
Max Acc:0.919192
Train 217, loss: 0.210423, train acc: 0.910174, 
Test 217, loss: 0.217115, test acc: 0.913636,
Train 218, loss: 0.211521, train acc: 0.913400, 
Test 218, loss: 0.256519, test acc: 0.891919,
Train 219, loss: 0.208761, train acc: 0.913151, 
Test 219, loss: 0.208670, test acc: 0.922222,
Max Acc:0.922222
Train 220, loss: 0.207994, train acc: 0.913772, 
Test 220, loss: 0.202646, test acc: 0.922980,
Max Acc:0.922980
Train 221, loss: 0.204201, train acc: 0.912655, 
Test 221, loss: 0.203433, test acc: 0.917172,
Train 222, loss: 0.211443, train acc: 0.909553, 
Test 222, loss: 0.206056, test acc: 0.923990,
Max Acc:0.923990
Train 223, loss: 0.201033, train acc: 0.918362, 
Test 223, loss: 0.216670, test acc: 0.920202,
Train 224, loss: 0.203463, train acc: 0.918486, 
Test 224, loss: 0.213821, test acc: 0.918182,
arch: dgcnn
batch_size: 1
calc_scores: softmax
dropout: 0.5
epochs: 300
eval: False
exp_name: dgcnn_paconv_train
k_neighbors: 20
lr: 0.04
manual_seed: 0
momentum: 0.9
no_cuda: False
num_matrices: [8, 8, 8, 8]
num_points: 100
pt_norm: False
test_batch_size: 1
workers: 6
Using GPU
PAConv(
  (scorenet1): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet2): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet3): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (scorenet4): ScoreNet(
    (mlp_convs_hidden): ModuleList(
      (0): Conv2d(6, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))
    )
    (mlp_bns_hidden): ModuleList(
      (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn4): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv5): Sequential(
    (0): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)
    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (linear1): Linear(in_features=2048, out_features=512, bias=False)
  (bn11): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp1): Dropout(p=0.5, inplace=False)
  (linear2): Linear(in_features=512, out_features=256, bias=False)
  (bn22): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (dp2): Dropout(p=0.5, inplace=False)
  (linear3): Linear(in_features=256, out_features=2, bias=True)
)
Train 0, loss: 0.953088, train acc: 0.657692, 
Test 0, loss: 0.572919, test acc: 0.716162,
Max Acc:0.716162
Train 1, loss: 0.599217, train acc: 0.687965, 
Test 1, loss: 0.526009, test acc: 0.750758,
Max Acc:0.750758
Train 2, loss: 0.574010, train acc: 0.720347, 
Test 2, loss: 0.544045, test acc: 0.758333,
Max Acc:0.758333
Train 3, loss: 0.575675, train acc: 0.706328, 
Test 3, loss: 0.494191, test acc: 0.750505,
Train 4, loss: 0.567030, train acc: 0.713772, 
Test 4, loss: 0.504917, test acc: 0.746970,
Train 5, loss: 0.562471, train acc: 0.719355, 
Test 5, loss: 0.510908, test acc: 0.746717,
Train 6, loss: 0.564553, train acc: 0.723077, 
Test 6, loss: 0.504388, test acc: 0.739646,
Train 7, loss: 0.547673, train acc: 0.735112, 
Test 7, loss: 0.489125, test acc: 0.763384,
Max Acc:0.763384
Train 8, loss: 0.561174, train acc: 0.720099, 
Test 8, loss: 0.491570, test acc: 0.761364,
Train 9, loss: 0.550878, train acc: 0.730521, 
Test 9, loss: 0.490872, test acc: 0.764899,
Max Acc:0.764899
Train 10, loss: 0.548245, train acc: 0.722333, 
Test 10, loss: 0.504254, test acc: 0.753030,
Train 11, loss: 0.547348, train acc: 0.727543, 
Test 11, loss: 0.497813, test acc: 0.756313,
Train 12, loss: 0.540355, train acc: 0.738710, 
Test 12, loss: 0.490240, test acc: 0.761364,
Train 13, loss: 0.545223, train acc: 0.730893, 
Test 13, loss: 0.512590, test acc: 0.753030,
Train 14, loss: 0.546251, train acc: 0.733623, 
Test 14, loss: 0.483692, test acc: 0.753030,
Train 15, loss: 0.534465, train acc: 0.729404, 
Test 15, loss: 0.506090, test acc: 0.740909,
Train 16, loss: 0.538774, train acc: 0.735112, 
Test 16, loss: 0.498571, test acc: 0.761364,
Train 17, loss: 0.539405, train acc: 0.731886, 
Test 17, loss: 0.485739, test acc: 0.764899,
Max Acc:0.764899
Train 18, loss: 0.540090, train acc: 0.729280, 
Test 18, loss: 0.488064, test acc: 0.747475,
Train 19, loss: 0.537383, train acc: 0.734739, 
Test 19, loss: 0.477651, test acc: 0.775000,
Max Acc:0.775000
Train 20, loss: 0.521104, train acc: 0.746898, 
Test 20, loss: 0.505156, test acc: 0.759848,
Train 21, loss: 0.529842, train acc: 0.739950, 
Test 21, loss: 0.468878, test acc: 0.771465,
Train 22, loss: 0.531130, train acc: 0.737593, 
Test 22, loss: 0.474540, test acc: 0.771212,
Train 23, loss: 0.530202, train acc: 0.743300, 
Test 23, loss: 0.480014, test acc: 0.771212,
Train 24, loss: 0.530368, train acc: 0.739826, 
Test 24, loss: 0.486547, test acc: 0.768434,
Train 25, loss: 0.526339, train acc: 0.741439, 
Test 25, loss: 0.495875, test acc: 0.767172,
Train 26, loss: 0.519592, train acc: 0.741935, 
Test 26, loss: 0.527166, test acc: 0.718434,
Train 27, loss: 0.520737, train acc: 0.745409, 
Test 27, loss: 0.473067, test acc: 0.771717,
Train 28, loss: 0.525847, train acc: 0.745037, 
Test 28, loss: 0.462616, test acc: 0.787374,
Max Acc:0.787374
Train 29, loss: 0.513505, train acc: 0.743176, 
Test 29, loss: 0.482466, test acc: 0.736111,
Train 30, loss: 0.514288, train acc: 0.753226, 
Test 30, loss: 0.491687, test acc: 0.733333,
Train 31, loss: 0.528465, train acc: 0.742680, 
Test 31, loss: 0.472554, test acc: 0.782576,
Train 32, loss: 0.509853, train acc: 0.751241, 
Test 32, loss: 0.447955, test acc: 0.787879,
Max Acc:0.787879
Train 33, loss: 0.515109, train acc: 0.750993, 
Test 33, loss: 0.476551, test acc: 0.771212,
Train 34, loss: 0.502591, train acc: 0.762531, 
Test 34, loss: 0.504891, test acc: 0.744192,
Train 35, loss: 0.500659, train acc: 0.754591, 
Test 35, loss: 0.460185, test acc: 0.785606,
Train 36, loss: 0.502060, train acc: 0.764268, 
Test 36, loss: 0.449604, test acc: 0.790404,
Max Acc:0.790404
Train 37, loss: 0.498874, train acc: 0.758437, 
Test 37, loss: 0.465501, test acc: 0.772727,
Train 38, loss: 0.501285, train acc: 0.759926, 
Test 38, loss: 0.432364, test acc: 0.789899,
Train 39, loss: 0.485461, train acc: 0.765633, 
Test 39, loss: 0.495506, test acc: 0.749495,
Train 40, loss: 0.484995, train acc: 0.768486, 
Test 40, loss: 0.491738, test acc: 0.735101,
Train 41, loss: 0.488991, train acc: 0.762283, 
Test 41, loss: 0.422643, test acc: 0.801768,
Max Acc:0.801768
Train 42, loss: 0.486662, train acc: 0.764888, 
Test 42, loss: 0.619452, test acc: 0.630808,
Train 43, loss: 0.488650, train acc: 0.760050, 
Test 43, loss: 0.567550, test acc: 0.702525,
Train 44, loss: 0.483731, train acc: 0.770099, 
Test 44, loss: 0.443951, test acc: 0.781313,
Train 45, loss: 0.472241, train acc: 0.772829, 
Test 45, loss: 0.467938, test acc: 0.756818,
Train 46, loss: 0.470024, train acc: 0.777047, 
Test 46, loss: 0.415991, test acc: 0.797980,
Train 47, loss: 0.464362, train acc: 0.783623, 
Test 47, loss: 0.447807, test acc: 0.790657,
Train 48, loss: 0.468483, train acc: 0.777295, 
Test 48, loss: 0.450296, test acc: 0.785101,
Train 49, loss: 0.460043, train acc: 0.785360, 
Test 49, loss: 0.456170, test acc: 0.771465,
Train 50, loss: 0.457538, train acc: 0.788462, 
Test 50, loss: 0.491986, test acc: 0.743939,
Train 51, loss: 0.457404, train acc: 0.786849, 
Test 51, loss: 0.400130, test acc: 0.818434,
Max Acc:0.818434
Train 52, loss: 0.455048, train acc: 0.786352, 
Test 52, loss: 0.393004, test acc: 0.810606,
Train 53, loss: 0.450686, train acc: 0.791935, 
Test 53, loss: 0.478028, test acc: 0.772222,
Train 54, loss: 0.457099, train acc: 0.782258, 
Test 54, loss: 0.409275, test acc: 0.836111,
Max Acc:0.836111
Train 55, loss: 0.449177, train acc: 0.786352, 
Test 55, loss: 0.418633, test acc: 0.810859,
Train 56, loss: 0.448162, train acc: 0.794417, 
Test 56, loss: 0.392900, test acc: 0.829545,
Train 57, loss: 0.440920, train acc: 0.789578, 
Test 57, loss: 0.399889, test acc: 0.812626,
Train 58, loss: 0.440252, train acc: 0.792184, 
Test 58, loss: 0.456272, test acc: 0.757323,
Train 59, loss: 0.436117, train acc: 0.797270, 
Test 59, loss: 0.430771, test acc: 0.781566,
Train 60, loss: 0.438590, train acc: 0.797395, 
Test 60, loss: 0.397794, test acc: 0.807828,
Train 61, loss: 0.431923, train acc: 0.803722, 
Test 61, loss: 0.380480, test acc: 0.837626,
Max Acc:0.837626
Train 62, loss: 0.428084, train acc: 0.808189, 
Test 62, loss: 0.399514, test acc: 0.816414,
Train 63, loss: 0.427816, train acc: 0.805707, 
Test 63, loss: 0.393718, test acc: 0.827020,
Train 64, loss: 0.430162, train acc: 0.796278, 
Test 64, loss: 0.407389, test acc: 0.805051,
Train 65, loss: 0.435515, train acc: 0.794045, 
Test 65, loss: 0.385749, test acc: 0.817929,
Train 66, loss: 0.420777, train acc: 0.806700, 
Test 66, loss: 0.431824, test acc: 0.781818,
Train 67, loss: 0.425077, train acc: 0.797519, 
Test 67, loss: 0.362626, test acc: 0.857828,
Max Acc:0.857828
Train 68, loss: 0.423801, train acc: 0.804963, 
Test 68, loss: 0.432986, test acc: 0.792929,
Train 69, loss: 0.416904, train acc: 0.810670, 
Test 69, loss: 0.345994, test acc: 0.850505,
Train 70, loss: 0.416918, train acc: 0.804963, 
Test 70, loss: 0.377526, test acc: 0.841667,
Train 71, loss: 0.409976, train acc: 0.815261, 
Test 71, loss: 0.383621, test acc: 0.814899,
Train 72, loss: 0.414510, train acc: 0.807816, 
Test 72, loss: 0.358993, test acc: 0.840404,
Train 73, loss: 0.409980, train acc: 0.814640, 
Test 73, loss: 0.560593, test acc: 0.721212,
Train 74, loss: 0.414358, train acc: 0.807320, 
Test 74, loss: 0.354967, test acc: 0.844697,
Train 75, loss: 0.406687, train acc: 0.814764, 
Test 75, loss: 0.391161, test acc: 0.818182,
Train 76, loss: 0.405055, train acc: 0.816873, 
Test 76, loss: 0.370549, test acc: 0.824495,
Train 77, loss: 0.413413, train acc: 0.805831, 
Test 77, loss: 0.424681, test acc: 0.792424,
Train 78, loss: 0.405613, train acc: 0.814020, 
Test 78, loss: 0.423099, test acc: 0.815404,
Train 79, loss: 0.403233, train acc: 0.821464, 
Test 79, loss: 0.341638, test acc: 0.863384,
Max Acc:0.863384
Train 80, loss: 0.390435, train acc: 0.820720, 
Test 80, loss: 0.378948, test acc: 0.839394,
Train 81, loss: 0.395643, train acc: 0.822953, 
Test 81, loss: 0.349058, test acc: 0.853788,
Train 82, loss: 0.389307, train acc: 0.823821, 
Test 82, loss: 0.339123, test acc: 0.846717,
Train 83, loss: 0.386579, train acc: 0.826923, 
Test 83, loss: 0.333491, test acc: 0.856818,
Train 84, loss: 0.394206, train acc: 0.824318, 
Test 84, loss: 0.343720, test acc: 0.860859,
Train 85, loss: 0.384368, train acc: 0.830645, 
Test 85, loss: 0.356251, test acc: 0.833586,
Train 86, loss: 0.387329, train acc: 0.828040, 
Test 86, loss: 0.323707, test acc: 0.861111,
Train 87, loss: 0.390589, train acc: 0.824442, 
Test 87, loss: 0.342621, test acc: 0.855303,
Train 88, loss: 0.382477, train acc: 0.830645, 
Test 88, loss: 0.336596, test acc: 0.862121,
Train 89, loss: 0.381791, train acc: 0.831266, 
Test 89, loss: 0.347076, test acc: 0.844444,
Train 90, loss: 0.380573, train acc: 0.829404, 
Test 90, loss: 0.331792, test acc: 0.858838,
Train 91, loss: 0.375278, train acc: 0.837841, 
Test 91, loss: 0.396679, test acc: 0.825505,
Train 92, loss: 0.383140, train acc: 0.830149, 
Test 92, loss: 0.327539, test acc: 0.860859,
Train 93, loss: 0.373767, train acc: 0.837593, 
Test 93, loss: 0.367816, test acc: 0.835606,
Train 94, loss: 0.371182, train acc: 0.841315, 
Test 94, loss: 0.335497, test acc: 0.851515,
Train 95, loss: 0.363371, train acc: 0.838089, 
Test 95, loss: 0.313451, test acc: 0.859848,
Train 96, loss: 0.356738, train acc: 0.845533, 
Test 96, loss: 0.316738, test acc: 0.876768,
Max Acc:0.876768
Train 97, loss: 0.364661, train acc: 0.841067, 
Test 97, loss: 0.334711, test acc: 0.854798,
Train 98, loss: 0.360598, train acc: 0.842680, 
Test 98, loss: 0.330042, test acc: 0.855808,
Train 99, loss: 0.366333, train acc: 0.843052, 
Test 99, loss: 0.352527, test acc: 0.842424,
Train 100, loss: 0.358903, train acc: 0.845533, 
Test 100, loss: 0.329456, test acc: 0.851515,
Train 101, loss: 0.359940, train acc: 0.842680, 
Test 101, loss: 0.314800, test acc: 0.871717,
Train 102, loss: 0.348695, train acc: 0.853722, 
Test 102, loss: 0.315762, test acc: 0.866919,
Train 103, loss: 0.353127, train acc: 0.850248, 
Test 103, loss: 0.289394, test acc: 0.884848,
Max Acc:0.884848
Train 104, loss: 0.348190, train acc: 0.849752, 
Test 104, loss: 0.335182, test acc: 0.851768,
Train 105, loss: 0.347943, train acc: 0.847643, 
Test 105, loss: 0.302933, test acc: 0.869697,
Train 106, loss: 0.344277, train acc: 0.851613, 
Test 106, loss: 0.322568, test acc: 0.872222,
Train 107, loss: 0.354295, train acc: 0.844417, 
Test 107, loss: 0.314137, test acc: 0.863384,
Train 108, loss: 0.350916, train acc: 0.849752, 
Test 108, loss: 0.304705, test acc: 0.872980,
Train 109, loss: 0.331028, train acc: 0.860174, 
Test 109, loss: 0.350419, test acc: 0.841919,
Train 110, loss: 0.328233, train acc: 0.858685, 
Test 110, loss: 0.314396, test acc: 0.866162,
Train 111, loss: 0.335776, train acc: 0.857692, 
Test 111, loss: 0.294913, test acc: 0.880303,
Train 112, loss: 0.336059, train acc: 0.859057, 
Test 112, loss: 0.286459, test acc: 0.881818,
Train 113, loss: 0.345622, train acc: 0.850000, 
Test 113, loss: 0.308871, test acc: 0.865404,
Train 114, loss: 0.333582, train acc: 0.854963, 
Test 114, loss: 0.423082, test acc: 0.819192,
Train 115, loss: 0.330431, train acc: 0.856203, 
Test 115, loss: 0.381758, test acc: 0.829545,
Train 116, loss: 0.343286, train acc: 0.853722, 
Test 116, loss: 0.271006, test acc: 0.889646,
Max Acc:0.889646
Train 117, loss: 0.334706, train acc: 0.854963, 
Test 117, loss: 0.273683, test acc: 0.891919,
Max Acc:0.891919
Train 118, loss: 0.337748, train acc: 0.851861, 
Test 118, loss: 0.295894, test acc: 0.867172,
Train 119, loss: 0.331979, train acc: 0.859181, 
Test 119, loss: 0.309296, test acc: 0.859091,
Train 120, loss: 0.324027, train acc: 0.860918, 
Test 120, loss: 0.274193, test acc: 0.880556,
Train 121, loss: 0.324154, train acc: 0.862655, 
Test 121, loss: 0.278162, test acc: 0.882071,
Train 122, loss: 0.319780, train acc: 0.867618, 
Test 122, loss: 0.283095, test acc: 0.880051,
Train 123, loss: 0.331845, train acc: 0.856824, 
Test 123, loss: 0.297644, test acc: 0.876010,
Train 124, loss: 0.324216, train acc: 0.862531, 
Test 124, loss: 0.283005, test acc: 0.891919,
Max Acc:0.891919
Train 125, loss: 0.318842, train acc: 0.863151, 
Test 125, loss: 0.330981, test acc: 0.853535,
Train 126, loss: 0.319991, train acc: 0.863400, 
Test 126, loss: 0.272541, test acc: 0.890152,
Train 127, loss: 0.326808, train acc: 0.857568, 
Test 127, loss: 0.286374, test acc: 0.875758,
Train 128, loss: 0.320960, train acc: 0.860918, 
Test 128, loss: 0.292970, test acc: 0.885354,
Train 129, loss: 0.311439, train acc: 0.866625, 
Test 129, loss: 0.251644, test acc: 0.897980,
Max Acc:0.897980
Train 130, loss: 0.324888, train acc: 0.862531, 
Test 130, loss: 0.267608, test acc: 0.894192,
Train 131, loss: 0.306370, train acc: 0.871092, 
Test 131, loss: 0.275946, test acc: 0.894949,
Train 132, loss: 0.307050, train acc: 0.867618, 
Test 132, loss: 0.284544, test acc: 0.877020,
Train 133, loss: 0.314511, train acc: 0.866129, 
Test 133, loss: 0.325429, test acc: 0.851010,
Train 134, loss: 0.307222, train acc: 0.871836, 
Test 134, loss: 0.333125, test acc: 0.843939,
Train 135, loss: 0.299369, train acc: 0.876923, 
Test 135, loss: 0.295025, test acc: 0.880556,
Train 136, loss: 0.294693, train acc: 0.879653, 
Test 136, loss: 0.251953, test acc: 0.901768,
Max Acc:0.901768
Train 137, loss: 0.292126, train acc: 0.880645, 
Test 137, loss: 0.316833, test acc: 0.858333,
Train 138, loss: 0.306081, train acc: 0.869975, 
Test 138, loss: 0.273020, test acc: 0.891162,
Train 139, loss: 0.300418, train acc: 0.874566, 
Test 139, loss: 0.257907, test acc: 0.901515,
Train 140, loss: 0.303958, train acc: 0.872084, 
Test 140, loss: 0.351725, test acc: 0.840152,
Train 141, loss: 0.302904, train acc: 0.871588, 
Test 141, loss: 0.334867, test acc: 0.854798,
Train 142, loss: 0.293194, train acc: 0.874938, 
Test 142, loss: 0.239767, test acc: 0.905556,
Max Acc:0.905556
Train 143, loss: 0.295208, train acc: 0.873573, 
Test 143, loss: 0.268011, test acc: 0.895707,
Train 144, loss: 0.301698, train acc: 0.873945, 
Test 144, loss: 0.281771, test acc: 0.872727,
Train 145, loss: 0.296812, train acc: 0.875806, 
Test 145, loss: 0.264961, test acc: 0.885101,
Train 146, loss: 0.285849, train acc: 0.882754, 
Test 146, loss: 0.251974, test acc: 0.897475,
Train 147, loss: 0.299914, train acc: 0.873325, 
Test 147, loss: 0.376199, test acc: 0.833586,
Train 148, loss: 0.295572, train acc: 0.873077, 
Test 148, loss: 0.280030, test acc: 0.883586,
Train 149, loss: 0.298957, train acc: 0.874938, 
Test 149, loss: 0.256995, test acc: 0.895455,
Train 150, loss: 0.290279, train acc: 0.875682, 
Test 150, loss: 0.255134, test acc: 0.906566,
Max Acc:0.906566
Train 151, loss: 0.285962, train acc: 0.875682, 
Test 151, loss: 0.261611, test acc: 0.894697,
Train 152, loss: 0.290084, train acc: 0.873201, 
Test 152, loss: 0.246996, test acc: 0.899495,
Train 153, loss: 0.285158, train acc: 0.880397, 
Test 153, loss: 0.250820, test acc: 0.894949,
Train 154, loss: 0.295859, train acc: 0.871092, 
Test 154, loss: 0.288965, test acc: 0.876768,
Train 155, loss: 0.282868, train acc: 0.882630, 
Test 155, loss: 0.241510, test acc: 0.906566,
Max Acc:0.906566
Train 156, loss: 0.282528, train acc: 0.880521, 
Test 156, loss: 0.301003, test acc: 0.867677,
Train 157, loss: 0.280128, train acc: 0.881390, 
Test 157, loss: 0.251720, test acc: 0.897980,
Train 158, loss: 0.274459, train acc: 0.885360, 
Test 158, loss: 0.296503, test acc: 0.876010,
Train 159, loss: 0.282492, train acc: 0.882382, 
Test 159, loss: 0.248619, test acc: 0.902778,
Train 160, loss: 0.276040, train acc: 0.883871, 
Test 160, loss: 0.294372, test acc: 0.874747,
Train 161, loss: 0.275355, train acc: 0.885236, 
Test 161, loss: 0.257094, test acc: 0.895455,
Train 162, loss: 0.278044, train acc: 0.882630, 
Test 162, loss: 0.253474, test acc: 0.898485,
Train 163, loss: 0.273857, train acc: 0.883871, 
Test 163, loss: 0.285269, test acc: 0.880051,
Train 164, loss: 0.278264, train acc: 0.880645, 
Test 164, loss: 0.234250, test acc: 0.913889,
Max Acc:0.913889
Train 165, loss: 0.267563, train acc: 0.883871, 
Test 165, loss: 0.232882, test acc: 0.910354,
Train 166, loss: 0.271910, train acc: 0.885360, 
Test 166, loss: 0.226260, test acc: 0.918939,
Max Acc:0.918939
Train 167, loss: 0.270857, train acc: 0.890943, 
Test 167, loss: 0.246333, test acc: 0.897222,
Train 168, loss: 0.266402, train acc: 0.887593, 
Test 168, loss: 0.236979, test acc: 0.906061,
Train 169, loss: 0.257039, train acc: 0.887717, 
Test 169, loss: 0.243807, test acc: 0.902525,
Train 170, loss: 0.273561, train acc: 0.888958, 
Test 170, loss: 0.254134, test acc: 0.896717,
Train 171, loss: 0.265371, train acc: 0.885360, 
Test 171, loss: 0.256220, test acc: 0.893434,
Train 172, loss: 0.274801, train acc: 0.884988, 
Test 172, loss: 0.230937, test acc: 0.907828,
Train 173, loss: 0.263670, train acc: 0.887221, 
Test 173, loss: 0.226805, test acc: 0.908586,
Train 174, loss: 0.253924, train acc: 0.898387, 
Test 174, loss: 0.240844, test acc: 0.899747,
Train 175, loss: 0.261448, train acc: 0.890447, 
Test 175, loss: 0.228895, test acc: 0.911111,
Train 176, loss: 0.266903, train acc: 0.888834, 
Test 176, loss: 0.234947, test acc: 0.904293,
Train 177, loss: 0.256781, train acc: 0.892928, 
Test 177, loss: 0.234864, test acc: 0.904545,
Train 178, loss: 0.263841, train acc: 0.892432, 
Test 178, loss: 0.287590, test acc: 0.876515,
Train 179, loss: 0.250586, train acc: 0.895409, 
Test 179, loss: 0.247549, test acc: 0.893182,
Train 180, loss: 0.256158, train acc: 0.896402, 
Test 180, loss: 0.229139, test acc: 0.907576,
Train 181, loss: 0.259521, train acc: 0.891067, 
Test 181, loss: 0.318905, test acc: 0.859596,
Train 182, loss: 0.262750, train acc: 0.888462, 
Test 182, loss: 0.247819, test acc: 0.900505,
Train 183, loss: 0.252690, train acc: 0.894169, 
Test 183, loss: 0.231590, test acc: 0.911111,
Train 184, loss: 0.249663, train acc: 0.895782, 
Test 184, loss: 0.283051, test acc: 0.881818,
Train 185, loss: 0.250702, train acc: 0.895906, 
Test 185, loss: 0.229551, test acc: 0.907323,
Train 186, loss: 0.253064, train acc: 0.898263, 
Test 186, loss: 0.242137, test acc: 0.901263,
Train 187, loss: 0.256859, train acc: 0.890323, 
Test 187, loss: 0.267347, test acc: 0.887374,
Train 188, loss: 0.258413, train acc: 0.891067, 
Test 188, loss: 0.232965, test acc: 0.913384,
Train 189, loss: 0.237216, train acc: 0.900868, 
Test 189, loss: 0.213578, test acc: 0.918939,
Max Acc:0.918939
Train 190, loss: 0.249195, train acc: 0.898263, 
Test 190, loss: 0.223110, test acc: 0.915909,
Train 191, loss: 0.237239, train acc: 0.901985, 
Test 191, loss: 0.292159, test acc: 0.878283,
Train 192, loss: 0.237891, train acc: 0.903722, 
Test 192, loss: 0.239722, test acc: 0.906061,
Train 193, loss: 0.240044, train acc: 0.901737, 
Test 193, loss: 0.229312, test acc: 0.908838,
Train 194, loss: 0.239472, train acc: 0.901613, 
Test 194, loss: 0.225668, test acc: 0.910859,
Train 195, loss: 0.237912, train acc: 0.900744, 
Test 195, loss: 0.210748, test acc: 0.916667,
Train 196, loss: 0.243360, train acc: 0.900744, 
Test 196, loss: 0.223147, test acc: 0.905556,
Train 197, loss: 0.234165, train acc: 0.903102, 
Test 197, loss: 0.237223, test acc: 0.903788,
Train 198, loss: 0.240568, train acc: 0.902357, 
Test 198, loss: 0.215928, test acc: 0.916414,
Train 199, loss: 0.227855, train acc: 0.906452, 
Test 199, loss: 0.246283, test acc: 0.900253,
Train 200, loss: 0.228356, train acc: 0.908189, 
Test 200, loss: 0.223539, test acc: 0.912374,
Train 201, loss: 0.223678, train acc: 0.906948, 
Test 201, loss: 0.216627, test acc: 0.913636,
Train 202, loss: 0.235297, train acc: 0.902357, 
Test 202, loss: 0.230304, test acc: 0.907071,
Train 203, loss: 0.235144, train acc: 0.900993, 
Test 203, loss: 0.229634, test acc: 0.911869,
Train 204, loss: 0.236023, train acc: 0.903722, 
Test 204, loss: 0.233136, test acc: 0.909343,
Train 205, loss: 0.227486, train acc: 0.903350, 
Test 205, loss: 0.209611, test acc: 0.922727,
Max Acc:0.922727
Train 206, loss: 0.226738, train acc: 0.907568, 
Test 206, loss: 0.223128, test acc: 0.912374,
Train 207, loss: 0.231469, train acc: 0.903722, 
Test 207, loss: 0.215507, test acc: 0.917172,
Train 208, loss: 0.216375, train acc: 0.912779, 
Test 208, loss: 0.205838, test acc: 0.919697,
Train 209, loss: 0.216835, train acc: 0.911290, 
Test 209, loss: 0.232571, test acc: 0.908333,
Train 210, loss: 0.219896, train acc: 0.912655, 
Test 210, loss: 0.209637, test acc: 0.925253,
Max Acc:0.925253
Train 211, loss: 0.215568, train acc: 0.913896, 
Test 211, loss: 0.209725, test acc: 0.915909,
Train 212, loss: 0.232392, train acc: 0.903226, 
Test 212, loss: 0.213245, test acc: 0.915657,
Train 213, loss: 0.215752, train acc: 0.911414, 
Test 213, loss: 0.215346, test acc: 0.918434,
Train 214, loss: 0.218637, train acc: 0.910670, 
Test 214, loss: 0.209051, test acc: 0.918939,
Train 215, loss: 0.217978, train acc: 0.911414, 
Test 215, loss: 0.234164, test acc: 0.905556,
Train 216, loss: 0.209958, train acc: 0.914268, 
Test 216, loss: 0.204048, test acc: 0.920455,
Train 217, loss: 0.213968, train acc: 0.912035, 
Test 217, loss: 0.220306, test acc: 0.912626,
Train 218, loss: 0.207878, train acc: 0.911787, 
Test 218, loss: 0.222722, test acc: 0.910101,
Train 219, loss: 0.216758, train acc: 0.916129, 
Test 219, loss: 0.213657, test acc: 0.914899,
Train 220, loss: 0.201158, train acc: 0.917246, 
Test 220, loss: 0.214697, test acc: 0.919697,
Train 221, loss: 0.204901, train acc: 0.916005, 
Test 221, loss: 0.200156, test acc: 0.917172,
Train 222, loss: 0.209820, train acc: 0.911538, 
Test 222, loss: 0.217627, test acc: 0.917929,
Train 223, loss: 0.207557, train acc: 0.916501, 
Test 223, loss: 0.216890, test acc: 0.915909,
Train 224, loss: 0.204279, train acc: 0.918610, 
Test 224, loss: 0.199082, test acc: 0.923485,
Train 225, loss: 0.192869, train acc: 0.919603, 
Test 225, loss: 0.207604, test acc: 0.924747,
Train 226, loss: 0.204561, train acc: 0.914640, 
Test 226, loss: 0.214528, test acc: 0.918939,
Train 227, loss: 0.201700, train acc: 0.916253, 
Test 227, loss: 0.216907, test acc: 0.913384,
Train 228, loss: 0.197732, train acc: 0.918238, 
Test 228, loss: 0.213422, test acc: 0.920960,
Train 229, loss: 0.195309, train acc: 0.918983, 
Test 229, loss: 0.197184, test acc: 0.928283,
Max Acc:0.928283
Train 230, loss: 0.198051, train acc: 0.916749, 
Test 230, loss: 0.217449, test acc: 0.916919,
Train 231, loss: 0.198371, train acc: 0.917742, 
Test 231, loss: 0.199341, test acc: 0.924747,
Train 232, loss: 0.196994, train acc: 0.917122, 
Test 232, loss: 0.203408, test acc: 0.922475,
Train 233, loss: 0.189909, train acc: 0.923077, 
Test 233, loss: 0.222301, test acc: 0.911364,
Train 234, loss: 0.197545, train acc: 0.921464, 
Test 234, loss: 0.199970, test acc: 0.927273,
Train 235, loss: 0.185696, train acc: 0.925434, 
Test 235, loss: 0.204349, test acc: 0.924747,
Train 236, loss: 0.199876, train acc: 0.922581, 
Test 236, loss: 0.200886, test acc: 0.921970,
Train 237, loss: 0.187804, train acc: 0.923573, 
Test 237, loss: 0.202381, test acc: 0.925000,
Train 238, loss: 0.192733, train acc: 0.923697, 
Test 238, loss: 0.199830, test acc: 0.921970,
Train 239, loss: 0.194103, train acc: 0.923449, 
Test 239, loss: 0.198598, test acc: 0.927273,
Train 240, loss: 0.187047, train acc: 0.925806, 
Test 240, loss: 0.201113, test acc: 0.920707,
Train 241, loss: 0.188984, train acc: 0.926055, 
Test 241, loss: 0.211188, test acc: 0.918434,
Train 242, loss: 0.187118, train acc: 0.923697, 
Test 242, loss: 0.198734, test acc: 0.927020,
Train 243, loss: 0.185669, train acc: 0.925682, 
Test 243, loss: 0.207332, test acc: 0.924747,
Train 244, loss: 0.176443, train acc: 0.926799, 
Test 244, loss: 0.196752, test acc: 0.927273,
Train 245, loss: 0.176822, train acc: 0.925310, 
Test 245, loss: 0.199116, test acc: 0.922475,
Train 246, loss: 0.180366, train acc: 0.928784, 
Test 246, loss: 0.216233, test acc: 0.920455,
Train 247, loss: 0.168609, train acc: 0.931141, 
Test 247, loss: 0.192834, test acc: 0.927273,
Train 248, loss: 0.178758, train acc: 0.927543, 
Test 248, loss: 0.188191, test acc: 0.928535,
Max Acc:0.928535
Train 249, loss: 0.173794, train acc: 0.926179, 
Test 249, loss: 0.208834, test acc: 0.922980,
Train 250, loss: 0.173416, train acc: 0.931266, 
Test 250, loss: 0.197234, test acc: 0.927778,
Train 251, loss: 0.173943, train acc: 0.931017, 
Test 251, loss: 0.209405, test acc: 0.927778,
Train 252, loss: 0.178612, train acc: 0.929032, 
Test 252, loss: 0.197322, test acc: 0.922980,
Train 253, loss: 0.163453, train acc: 0.933871, 
Test 253, loss: 0.201919, test acc: 0.928788,
Max Acc:0.928788
Train 254, loss: 0.163979, train acc: 0.931638, 
Test 254, loss: 0.190821, test acc: 0.926768,
Train 255, loss: 0.167808, train acc: 0.934864, 
Test 255, loss: 0.196837, test acc: 0.926515,
Train 256, loss: 0.166355, train acc: 0.931638, 
Test 256, loss: 0.207670, test acc: 0.928030,
Train 257, loss: 0.166089, train acc: 0.933251, 
Test 257, loss: 0.204745, test acc: 0.926010,
Train 258, loss: 0.164722, train acc: 0.931141, 
Test 258, loss: 0.195490, test acc: 0.930808,
Max Acc:0.930808
Train 259, loss: 0.153362, train acc: 0.936973, 
Test 259, loss: 0.193281, test acc: 0.929798,
Train 260, loss: 0.159254, train acc: 0.932878, 
Test 260, loss: 0.200594, test acc: 0.928788,
Train 261, loss: 0.159308, train acc: 0.935856, 
Test 261, loss: 0.190199, test acc: 0.934848,
Max Acc:0.934848
Train 262, loss: 0.155156, train acc: 0.934988, 
Test 262, loss: 0.202118, test acc: 0.928535,
Train 263, loss: 0.154349, train acc: 0.936725, 
Test 263, loss: 0.199187, test acc: 0.930303,
Train 264, loss: 0.157900, train acc: 0.934119, 
Test 264, loss: 0.191167, test acc: 0.933838,
Train 265, loss: 0.160402, train acc: 0.934615, 
Test 265, loss: 0.189360, test acc: 0.930808,
Train 266, loss: 0.147243, train acc: 0.940199, 
Test 266, loss: 0.189705, test acc: 0.933333,
Train 267, loss: 0.152823, train acc: 0.938213, 
Test 267, loss: 0.185699, test acc: 0.937121,
Max Acc:0.937121
Train 268, loss: 0.153728, train acc: 0.936228, 
Test 268, loss: 0.189329, test acc: 0.935101,
Train 269, loss: 0.153771, train acc: 0.938213, 
Test 269, loss: 0.192052, test acc: 0.930556,
Train 270, loss: 0.155143, train acc: 0.939578, 
Test 270, loss: 0.182151, test acc: 0.937626,
Max Acc:0.937626
Train 271, loss: 0.154551, train acc: 0.938958, 
Test 271, loss: 0.190914, test acc: 0.935354,
Train 272, loss: 0.153032, train acc: 0.936352, 
Test 272, loss: 0.192526, test acc: 0.932576,
Train 273, loss: 0.152376, train acc: 0.939206, 
Test 273, loss: 0.189332, test acc: 0.932071,
Train 274, loss: 0.139965, train acc: 0.943921, 
Test 274, loss: 0.188282, test acc: 0.937121,
Train 275, loss: 0.139430, train acc: 0.944665, 
Test 275, loss: 0.187066, test acc: 0.938384,
Max Acc:0.938384
Train 276, loss: 0.137597, train acc: 0.942184, 
Test 276, loss: 0.197712, test acc: 0.932576,
Train 277, loss: 0.142575, train acc: 0.941563, 
Test 277, loss: 0.192522, test acc: 0.931566,
Train 278, loss: 0.138717, train acc: 0.942556, 
Test 278, loss: 0.187022, test acc: 0.934848,
Train 279, loss: 0.145995, train acc: 0.940695, 
Test 279, loss: 0.191353, test acc: 0.934848,
Train 280, loss: 0.143950, train acc: 0.940943, 
Test 280, loss: 0.194618, test acc: 0.936111,
Train 281, loss: 0.145273, train acc: 0.941067, 
Test 281, loss: 0.192292, test acc: 0.935606,
Train 282, loss: 0.135221, train acc: 0.944665, 
Test 282, loss: 0.185719, test acc: 0.937879,
Train 283, loss: 0.139824, train acc: 0.942556, 
Test 283, loss: 0.192150, test acc: 0.935859,
Train 284, loss: 0.139558, train acc: 0.942184, 
Test 284, loss: 0.196426, test acc: 0.936111,
Train 285, loss: 0.134748, train acc: 0.947022, 
Test 285, loss: 0.185441, test acc: 0.937626,
Train 286, loss: 0.143782, train acc: 0.942432, 
Test 286, loss: 0.195846, test acc: 0.933586,
Train 287, loss: 0.142375, train acc: 0.942184, 
Test 287, loss: 0.184051, test acc: 0.935354,
Train 288, loss: 0.133454, train acc: 0.947270, 
Test 288, loss: 0.186978, test acc: 0.937879,
Train 289, loss: 0.130917, train acc: 0.946278, 
Test 289, loss: 0.192025, test acc: 0.937626,
Train 290, loss: 0.138571, train acc: 0.945161, 
Test 290, loss: 0.189626, test acc: 0.933333,
Train 291, loss: 0.142778, train acc: 0.939578, 
Test 291, loss: 0.199028, test acc: 0.931818,
Train 292, loss: 0.135188, train acc: 0.944665, 
Test 292, loss: 0.187571, test acc: 0.935354,
Train 293, loss: 0.141007, train acc: 0.943176, 
Test 293, loss: 0.191263, test acc: 0.937879,
Train 294, loss: 0.136889, train acc: 0.945409, 
Test 294, loss: 0.192937, test acc: 0.934848,
Train 295, loss: 0.128647, train acc: 0.945906, 
Test 295, loss: 0.193573, test acc: 0.937374,
Train 296, loss: 0.132134, train acc: 0.947767, 
Test 296, loss: 0.191854, test acc: 0.936364,
Train 297, loss: 0.138494, train acc: 0.945533, 
Test 297, loss: 0.183346, test acc: 0.935354,
Train 298, loss: 0.131442, train acc: 0.948511, 
Test 298, loss: 0.189226, test acc: 0.936616,
Train 299, loss: 0.129293, train acc: 0.948635, 
Test 299, loss: 0.188552, test acc: 0.938889,
Max Acc:0.938889
